\documentclass[reprint, floatfix]{revtex4-1}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}


\hypersetup{
  colorlinks,
  linkcolor={red!30!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{64,0,42}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}
\newcommand{\answer}[1]{{\color{DarkBlue}\footnotesize \textsc{Answer.} #1}}
\newcommand{\summary}[1]{{\color{DarkPurple}\footnotesize \textsc{Summary.} #1}}


\newcommand{\Err}{E}
\newcommand{\ii}{\mathrm{i}}



\begin{document}



\title{Additional notes: Optimal updating factor in Wang-Landau and metadynamics simulations}



\section{Homogeneous updating schemes}
\subsubsection{\label{sec:nnscheme}
Triple-bin updating scheme}



If $\mu_2 = \dots = \mu_b = 0$,
the matrix, $\mathbf w$, is tridiagonal,
%%
%\begin{equation}
%\arraycolsep=3.6pt\def\arraystretch{1.4}
%\mathbf w
%=
%\left(
%  \begin{array}{cccccccc}
%    1 - \mu_1   & \mu_1 & 0 & \dots & 0 \\
%    \mu_1 & 1 - 2 \, \mu_1  & \mu_1 & \dots & 0 \\
%    \vdots & &  & & \vdots \\
%    0 & \dots & \mu_1 & 1 - 2 \, \mu_1  & \mu_1 \\
%    0 & \dots & 0 & \mu_1 & 1 - \mu_1
%  \end{array}
%\right).
%\label{eq:wnn}
%\end{equation}
%%
%We have
and the eigenvalues are
\begin{equation}
  \lambda_k
  =
  1 -
  4 \, \mu_1 \sin^2
  \frac{ (k - 1) \, \pi }
       {       g \, n   }
  .
\notag
%\label{eq:wnn_eigenvalue}
\end{equation}
%
So this updating scheme is stable if
$\min \lambda_k \ge 0$,
or
\begin{equation}
  \mu_1 \le
  \begin{dcases}
    \frac 1 4
    & \mathrm{periodic \; and \;} n \mathrm{ \; even,}
    \\
    \frac 1 4
    \sec^2
    \left( \frac{  \pi   }
                { 2 \, n }
    \right)
    & \mathrm{otherwise}.
  \end{dcases}
\notag
%\label{eq:nn_stable}
\end{equation}

\note{
If the variable is periodic ($g = 1$) and $n$ is even,
the minimum is achieved at $k = (n/2) + 1$.

If the variable is periodic ($g = 1$) but $n$ is odd,
the minimum is achieved at $k = (n+1)/2$.
$$
\sin^2\left(
  \frac{n-1}{2 \, n} \pi
\right)
=
\cos^2\left(
  \frac{\pi}{2 \, n}
\right)
.
$$

If the variable is nonperiodic ($g = 2$)
the minimum is achieved at $k = n$ (the maximal index).
}


\subsection{\label{sec:invt_nn}
Triple-bin updating scheme}




For the triple-bin updating scheme,
%introduced in Sec. \ref{sec:nnscheme},
we will consider the large-$n$ limit.
%
If $2 \, \nu_k > 1$ for every $k$, we have
%$\sum_i \to \frac{2 \, n}{\pi} \int_0^{\pi/2} dp$, and
%
\begin{align}
  \Err_A(T)
  =
  \frac{ 2 \, n  }
       { \pi \, T }
  \int_0^{\pi/2}
    \frac{ \Gamma(p) \, \left(1 - 4 \, \mu_1 \, \sin^2 p \right)^2    }
         {   \lambda \, \left(2 - 8 \, \mu_1 \, \sin^2 p - \lambda \right) }
  \, dp
  .
\notag
%\label{eq:error_nn_asym_int}
\end{align}
%
For perfect sampling with $\mathbf p = \pmb \rho$,
we have Eq. \eqref{eq:Gamma_perfect},
and
$$
\begin{aligned}
  \Err_A(T)
  &=
  \frac{n}{4 \, T}
  \left(
    \frac{2 - 4 \, \mu_1 + \lambda}{ \lambda }
    +
    \frac{ \lambda }
    { \sqrt{ (2 - \lambda) (2 - 8 \, \mu_1 - \lambda) } }
  \right)
,
\end{aligned}
$$
%
which is minimal at
%
\begin{equation}
  \lambda
  =
  \frac{ 1 - 4 \, \mu_1 }
       { 1 - 2 \, \mu_1 }
  ,
%\notag
\label{eq:lambda_nn_perfect}
\end{equation}
%
and at this point,
%
\begin{equation}
  \Err_A(T)
  =
  \frac{n}{T}
  \left(
    1+ \frac{2 \, \mu_1^2}{1-4 \, \mu_1}
  \right)
  .
  \label{eq:error_nn_perfect}
\end{equation}
%
This shows that the original inverse-time formula,
Eq. \eqref{eq:alpha_invt1},
is not optimal unless $\mu_1 = 0$,
the single-bin case,
and the latter minimizes the asymptotic error,
Eq. \eqref{eq:error_nn_perfect}.
%
Note that the value given by
Eq. \eqref{eq:lambda_nn_perfect}
was less than the observed optimal value
from Fig. \ref{fig:errinvt},
and the latter would decrease
with the simulation length.
%




For the one-step sampling process,
we find from Eq. \eqref{eq:Gamma_onestep}
that $\Gamma(p) = \cot^2 p$
peaks around $p \approx 0$,
%
Then
$$
  \Err_A(T)
  \propto
  \frac{   2 \, n }
       { \pi \, T }
  \frac{             1             }
       {  \lambda \, (2 - \lambda) }
  .
$$
%
The optimal $\lambda$ is, therefore,
$\lambda \approx 1$.
%

\note{
However, the above result %Eq. \eqref{eq:lambda_nn_onestep}
is only good
for sufficiently short simulations,
since we have taken the limit of $n \to \infty$
for a fixed $T$.
%
For very long simulations,
the optimal $\lambda$
will ultimately drift to
the smallest eigenvalue,
$\min \lambda_k = 1 - 4 \, \mu_1$.
}







\section{\label{sec:equilerr}
Error of the ideal equilibrium FDS simulation}


\note{Original Appendix F.}

%As discussed in Sec. \ref{sec:FDS},
%the asymptotic limit of an adaptive FDS method
%is the corresponding equilibrium FDS method
%with the bias potential fixed to the exact value.
%%
%Such a simulation delivers the maximal efficiency.

In late stages of an adaptive FDS simulation,
the bias potential is often sufficiently precise.
Some practitioners would then fix
the bias potential and switch
to the equilibrium method
to prevent possible future efficiency loss
due to the adaptive updates.
%
Below, we will show that this practice is unnecessary
if the simulation uses the single-bin or a bandpass updating scheme
with the optimal schedule, Eq. \eqref{eq:alpha_invt1}.
%
That is, the bias potential obtained from
an optimal single-bin scheme adaptive simulation
will be similar in precision to that from
the ideal equilibrium simulation
for long times.
%
%Thus, our demonstration below serves as an alternative proof
%of the optimality of the single-bin scheme.

The error of the equilibrium FDS simulation
depends on the precision of the histogram.
%
If all $v_i$'s are exact,
the error is minimized, and can be computed
from the second term
on the right-hand side of Eq. \eqref{eq:vcorr_equil} as
%
\begin{align}
  E
  &=
  \left\langle
    \sum_{ i = 1 }^ n
      \rho_i \,
      \left(
        \ln \frac { H_i }
                  { p_i }
        -
        \sum_{r = 1}^n \rho_r
        \ln \frac { H_r }
                  { p_r }
      \right)^2
  \right\rangle
  \notag
  \\
  &
  \approx
  \sum_{ i = 1 }^ n
    \rho_i \,
    \left\langle
      \left(
        \frac { H_i }
              { p_i }
        -
        \sum_{r = 1}^n \rho_r
            \frac { H_r }
                  { p_r }
      \right)^2
    \right\rangle,
\notag
\end{align}
where
we have defined a time-averaged histogram
$H_i = \frac{1}{T} \sum_{t = 1}^T h_i(t)$
with the instantaneous histogram
$h_i(t)$ defined by Eq. \eqref{eq:h_def}.

We can rewrite the error as
a sum of the eigenmodes as
%
\begin{align}
  E
  =
  \sum_{ k = 2 }^n
    \left\langle
      \Theta_k^2
    \right\rangle
  ,
\notag
%\label{eq:error_sbin_equil}
\end{align}
%
where
%
$
  \Theta_k
  =
  \frac{ 1 } { T }
  \sum_{ t = 1 } ^ T
    \theta_k( t )
  ,
$
%
with $\theta_k(t)$ defined by Eq. \eqref{eq:theta_def}.
%and $\phi_{k i}$ by Eq. \eqref{eq:eig_orthonormal_cols}.
%\begin{align}
%\theta_k( t )
%=
%\sum_{ i = 1 }^ n
%\phi_{k i} \left[
%             \frac{ h_i(t) }
%                  { p_i    }
%             - 1
%           \right],
%\end{align}
%and
%\begin{align}
%\sum_{ k = 1 }^n
%  \phi_{k i} \, \phi_{k j}
%  = p_i \, \delta_{i j}.
%\end{align}
%
\note{Because
%
\begin{align*}
  \sum_{ k = 1 }^n
    \Theta_k^2
  &=
  \frac{ 1 } { T^2 }
  \sum_{ t, t' = 1 }^T
    \sum_{ k = 1 }^n
      \theta_k( t ) \, \theta_k( t' )
  \\
  &=
  \frac{ 1 } { T^2 }
  \sum_{ t, t' = 1 }^T
    \sum_{ i, j = 1 }^n
      \xi_i(t) \, \xi_j(t')
      %\left[
      %  \frac{ h_i(t) }
      %       { p_i    }
      %  - 1
      %\right]
      %\left[
      %  \frac{ h_j(t') }
      %       { p_j     }
      %  - 1
      %\right]
      \sum_{ k = 1 }^n
        \phi_{k \, i} \, \phi_{k \, j}
  \\
  &=
  \frac{ 1 } { T^2 }
  \sum_{ t, t' = 1 }^T
    \sum_{ i, j = 1 }^n
      \xi_i(t) \, \xi_j(t') \,
      \rho_i \, \delta_{i, j}
  \\
  &=
  \sum_{ i = 1 }^n
  \rho_i \,
  \frac{ 1 } { T }
  \sum_{ t = 1 }^T \xi_i(t)
  \;
  \frac{ 1 } { T }
  \sum_{ t' = 1 }^T \xi_i(t')
  \\
  &=
  \sum_{ i = 1 }^n
    \rho_i \,
      \left(
        \frac{ H_i }
             { p_i }
        -
        \sum_{r = 1}^n \rho_r
        \frac{ H_r }
             { p_r }
      \right)^2
  ,
\end{align*}
where we have used
$$
  \frac{1}{T}
  \sum_{t = 1}^T \xi_i(t)
  =
  \frac{ H_i }
       { p_i }
  -
  \sum_{r = 1}^n \rho_r
  \frac{ H_r }
       { p_r }
  .
$$
}%
The sum starts from $k = 2$, for
by Eq. \eqref{eq:eigenmode1}, % we have
$$
\theta_1(t)
=
\sum_{ i = 1 }^n
  \phi_{1i} \, \left(
    \frac{ h_i(t) } { p_i }
    -
    \sum_{r = 1}^n
    \rho_r
    \frac{ h_r(t) } { p_r }
  \right)
= 0.
$$

In the long-time limit,
\begin{align*}
  \left\langle
    \Theta_k^2
  \right\rangle
  &=
  \frac{1}{T^2}
  \sum_{ t, t' = 1 }^T
  \langle \theta_k(t) \, \theta_k(t') \rangle
  %\\
  %&\approx
  %\frac{1}{T}
  %\sum_{ \tau = -\infty }^{ \infty }
  %\langle \theta_k(t) \, \theta_k(t + \tau) \rangle
  %\\
  %&=
  =
  \frac{1}{T}
  \sum_{\tau = -\infty}^\infty
    \kappa_k(\tau)
  =
  \frac{ \Gamma_k }{ T }
.
\end{align*}
%
Thus,
\begin{equation}
  E
  =
  \frac{ 1 } { T }
  \sum_{ k = 2 }^n \Gamma_k
  .
\label{eq:error_equil}
\end{equation}

Now comparing this with the error of
the optimal adaptive simulation,
Eq. \eqref{eq:error_sinc} (with $K = n$),
we find the difference to be negligible
in the long time limit.

The above argument can be extended
to the bandpass updating schemes.
%
If the intrinsic PMF is sufficiently smooth,
we can explicitly filter out some error modes
in the histogram,
say those with $k >  K$,
and truncate the sum in
Eq. \eqref{eq:error_equil} at $k = K$.








\section{Multiple-bin scheme: Error}



\note{The extremal condition of $\Err_A$ is given by
%
\begin{align}
  \sum_{k = 2}^n
  \lambda_k \, u_k\bigl( q(t) \bigr)
  \int_0^T
    \ddot u_k \bigl( q(t') \bigr) \,
    \kappa_k(t' - t) \, dt' = 0.
  \tag{N1}
\label{eq:optimal_mbin}
\end{align}
%
If all $\lambda_k$, hence all $u_k$, are the same,
the above condition can be satisfied with $\ddot u_k = 0$,
which recovers the single-bin case.
%
Unfortunately, the above expression
appears to be useless in general.
}


\note{The next order correction to Eq. \eqref{eq:kappa_delta}
would lead to a correction to $\Err$
that is about $\alpha_{\max}^2$ times as large,
where $\alpha_{\max} = \max \alpha(t)$.
%
This is equivalent to saying that the Fourier transform
  $$
  \tilde \kappa_k(\omega) = \Gamma_k
  $$
  is a roughly constant.
  %
  For the next order correction we would have
  $$
  \tilde \kappa_k(\omega) = \Gamma_k + \Gamma^{(2)}_k \omega^2 + \dots,
  $$
  which leads to a correction to $\Err$
  $$
  \Gamma^{(2)}_k
  \int_0^T \ddot u_k^2\bigl( q(t) \bigr) \, dt
  \propto
  \alpha^2 \, \Err.
  $$
  This correction is of order $\alpha^2$
  compared to the main contribution in Eq. \eqref{eq:error_asym1}.
  %
  Thus, we can assume Eq. \eqref{eq:kappa_delta},
  once $\alpha(t)$ has dropped below a certain level.
}%

\note{
  The extremal property, Eq. \eqref{eq:optimal_mbin},
  now becomes
  %
  \begin{align}
    \sum_{k = 1}^n
      \Gamma_k \, \lambda_k \,
      u_k\bigl( q(t) \bigr) \,
      \ddot u_k\bigl( q(t) \bigr) = 0
    .
    \tag{N2}
    \label{neq:optimal_mbin1}
  \end{align}
  %
  If we interpret $u_k$ as the position,
  and $\Gamma_k \, \lambda_k$ as the mass,
  then the right-hand side gives the virial.
  %
  Thus, the extremal condition demands
  the total virial to be zero.
  $$\,$$
}%


\section{Normalization of eigenmodes}



By assuming Eq. \eqref{eq:w_detailedbalance},
we can diagonalize $\mathbf w$ with a set of
eigenvectors
%
\begin{equation}
  \sum_{i = 1}^n \phi_{ki} \, w_{ij}
  =
  \lambda_k \, \phi_{kj}
  ,
\label{eq:eig_w}
\end{equation}
%
satisfying the orthonormal conditions\cite{vankampen}:
%
\begin{align}
  \sum_{k = 1}^n
    \phi_{ki} \, \phi_{kj}
  &=
  \delta_{ij} \, p_i,
  \label{eq:eig_orthonormal_cols}
  \\
  \sum_{i = 1}^n
    \frac{ \phi_{ki} \, \phi_{li} }
         { p_i }
  &=
  \delta_{kl}
  .
\label{eq:eig_orthonormal_rows}
\end{align}
%
\note{We can define a symmetric matrix $\hat{\mathbf w}$
as $\hat w_{ij} = \sqrt{p_i/p_j} \, w_{ij}$,
which can be diagonalized\cite{vankampen}
with a set of orthonormal eigenvectors:
%
$\sum_{i = 1}^n \varphi_{ki} \, \hat w_{ij} = \lambda_k \, \varphi_{kj}$.
%
Then,
in terms of diagonalizing $\mathbf w$, we have
$$\phi_{ij} \equiv \sqrt{p_j} \, \varphi_{ij}$$
satisfying Eqs. \eqref{eq:eig_orthonormal_cols}
and \eqref{eq:eig_orthonormal_rows}.
%
}%


\section{Equilibrium error}


In terms of the components, we have
%
\begin{equation}
  \left\langle
    y_k^2
  \right\rangle
  =
  \frac 1 2 \, \Gamma_k \, \lambda_k \, \alpha_0.
  \label{eq:y2_eql}
\end{equation}
%


We shall use this result to model the initial error
before starting the schedule $\alpha(t)$ to be optimized.
%
That is, we assume
$\alpha(t) = \alpha_0$
for $t < 0$, and
the system has entered the equilibrium
under a fixed $\alpha_0$ at $t = 0$.
%
We can then use Eq. \eqref{eq:y2_eql}
for the $\langle y_k^2(0) \rangle$'s in Eq. \eqref{eq:error_res}.
%
\note{Another possible approximation
  is to interpret $\Gamma_k$ as twice the autocorrelation time,
  and the kernel is exponential
  $$
  \kappa_k(t) = \exp\left( - \frac{2}{\Gamma_k} |t| \right),
  $$
  then a slightly more accurate value is given by\cite{vankampen}
  $$
  \left\langle
    y_k^2
  \right\rangle
  =
  \frac{      ( \lambda_k \, \alpha_0 )^2     }
       { \lambda_k \, \alpha_0 + 2 / \Gamma_k }.
  $$
  But it appears that the main error of Eq. \eqref{eq:y2_eql}
  is an underestimate instead of an overestimate.
  And a possible source of error might be from
  converting a discrete sum to a continuous integral.
}



\section{
Another characterization of the optimal schedule}


The optimal schedule is given by
\begin{equation}
  \int_{ 0 }^{ q(t) }
    \sqrt{
      \textstyle\sum_{ k = 2 }^n
        \Gamma_k \, \lambda_k^2
        \, u_k^2( q' )
    }
    \;
    d q'
  =
  c \, t
  ,
  \label{eq:q_opt}
\end{equation}
%
where $c$ is to be determined by
the boundary condition at $t = T$ as
%
\begin{equation}
  c =
  \frac 1 T
  \int_{ 0 }^{ q(T) }
    \sqrt{
      \textstyle\sum_{ k = 2 }^n
        \Gamma_k \, \lambda_k^2
        \, u_k^2( q' )
    }
    \;
    d q'
  .
  \notag
\end{equation}
%
Unfortunately,
Eq. \eqref{eq:q_opt} is not an explicit equation of $\alpha(t)$.
%
However,
by differentiating Eq. \eqref{eq:Lagrangian_const},
we have
%
\begin{equation}
  \frac{ d   }
       { d t }
  \left[
    \frac{       1     }
         { \alpha( t ) }
  \right]
  =
  \frac{
    \sum_{ k = 2 }^n
      \Gamma_k \, \lambda_k^3
      \, u_k^2 \bigl[ q(t) \bigr]
  }
  {
    \sum_{ k = 2 }^n
      \Gamma_k \, \lambda_k^2
      \, u_k^2 \bigl[ q(t) \bigr]
  }
  \equiv
  \lambda(t)
  .
  \label{eq:dinvadt}
\end{equation}
%
The expression in the middle is an average
of the eigenvalues, $\lambda_k$'s,
weighted by $\Gamma_k \, \lambda_k^2 \, u_k^2$.
%
We shall refer to the average as the instantaneous
eigenvalue, $\lambda(t)$,
which gives the rate of change of $\alpha^{-1}(t)$.

\note{For the single-bin scheme, $\lambda_k = 1$,
and $\lambda(t)$ is always unity.
%
So,
$$
\frac { d   }
      { d t }
\left[
  \frac{      1    }
       { \alpha(t) }
\right]
=
1,
$$
which again leads to Eq. \eqref{eq:alpha_invt1}.


For a general multiple-bin scheme,
we can characterize the solution of
Eq. \eqref{eq:dinvadt} as
\begin{equation}
  \alpha(t)
  =
  \frac{                  1                     }
       { \lambda(t) \, \bigl[ t + t_0(t) \bigr] }
  ,
\notag
%\label{eq:alpha_approxinvt}
\end{equation}
%
where
$t_0(t)$ is a slowly-varying function.
}


By further differentiating Eq. \eqref{eq:dinvadt},
we can show that $\lambda(t)$
is an increasing function.
%
\note{Proof that $\lambda(t)$ is an increasing function.
  Using $\dot u_k = \lambda_k \, u_k \, \alpha(t)$,
  we have
  $$
  \begin{aligned}
    \dot \lambda
    &=
    \frac
    {
      \sum_{k = 2}^n 2 \, \Gamma_k \, \lambda_k^3 \, u_k \, \dot u_k
    }
    {
      \sum_{k = 2}^n \Gamma_k \, \lambda_k^2 \, u_k^2
    }
    -
    \frac
    {
      \sum_{k = 2}^n \Gamma_k \, \lambda_k^3 \, u_k^2
      \sum_{k = 2}^n 2 \, \Gamma_k \, \lambda_k^2 \, u_k \, \dot u_k
    }
    {
      \left(
        \sum_{k = 2}^n \Gamma_k \, \lambda_k^2 \, u_k^2
      \right)^2
    }
    \\
    &=
    \frac
    {
      \sum_{k = 2}^n \Gamma_k \, \lambda_k^4 \, u_k^2
      \,
      \sum_{k = 2}^n \Gamma_k \, \lambda_k^2 \, u_k^2
      -
      \left(
        \sum_{k = 2}^n \Gamma_k \, \lambda_k^3 \, u_k^2
      \right)^2
    }
    {
      \left(
        \sum_{k = 2}^n \Gamma_k \, \lambda_k^2 \, u_k^2
      \right)^2
    }
    2 \alpha(t).
  \end{aligned}
  $$
  The numerator of the fraction of the last expression
  is nonnegative by the Cauchy-Schwarz inequality,
  or by the nonpositivity of the discriminant
  of the following nonnegative definite quadratic form of $X$:
  $$
  \sum_{k = 2}^n \Gamma_k \, \lambda_k^2 \, u_k^2 \, (X - \lambda_k)^2 \ge 0.
  $$
  Thus, $\dot \lambda \ge 0$, and $\lambda(t)$ is monotonically increasing.
}%
%
Below we shall study the initial and final values.
%
At the end of the simulation,
$t = T$, we have $u_k = 1$, and
%
\begin{equation}
  \lambda(T)
  =
  \frac{
    \sum_{ k = 2 }^n
      \Gamma_k \, \lambda_k^3
  }
  {
    \sum_{ k = 2 }^n
      \Gamma_k \, \lambda_k^2
  }
  ,
\notag
%\label{eq:dinvadt_limit2}
\end{equation}
is independent of the simulation length, $T$.
%
Further, $\Gamma_k$ often increases with $\lambda_k$,
such that the relative weight $\Gamma_k \, \lambda_k^2$
also increases with $\lambda_k$.
%
It follows that $\lambda(T)$ will be dominated
by the larger values of $\lambda_k$'s.


By contrast,
at the beginning of the simulation,
the relative weight, $\Gamma_k \, \lambda_k^2 \, u_k^2(0)$,
is heavily influenced by the exponential factor
$u_k^2(0) = e^{ -2 \, \lambda_k \, q(T) }$.
%
Thus, $\lambda(0)$
is sensitive to the simulation length, $T$.
%
A longer simulation means a smaller $\lambda(0)$,
and that $\lambda(t)$ would stay small
for longer near the beginning of the simulation.






\section{
Eigenmodes of a non-periodic variable}




Parallel to Eq. \eqref{eq:phi_pbc},
we define the out-of-boundary values
of $\phi_i$ as
%
\begin{equation}
  \phi_i
  =
  \begin{dcases}
    \phi_{ 1 - i }           & \mathrm{for \;} i \le 0, \\
    \phi_{ 2 \, n + 1 - i }  & \mathrm{for \;} i > n,
  \end{dcases}
\label{eq:phi_refl}
\end{equation}
%
such that Eq. \eqref{eq:wmul_to_convol}
still holds.
%
\note{The derivation of Eq. \eqref{eq:wmul_to_convol}
  in this case is similar,
  $$
  \begin{aligned}
    \sum_{j = 1}^n w_{ij} \, \phi_j
    &=
    \sum_{j = 1}^n
      \mu_{i - j} \, \phi_j
    +
    \sum_{j = 1}^n
      \mu_{i + j - 1} \, \phi_j
    +
    \sum_{j = 1}^n
      \mu_{i + j - 2 \, n - 1} \, \phi_j
    \\
    &=
    \sum_{j = 1}^n
      \mu_{i - j} \, \phi_j
    +
    \sum_{l = 1 - n}^0
      \mu_{i - l} \, \phi_l
    +
    \sum_{l = n + 1}^{ 2 \, n }
      \mu_{i - l} \, \phi_l
    \\
    &=
    \sum_{j = 1 - n}^{ 2 \, n}
      \mu_{i - j} \, \phi_j.
  \end{aligned}
  $$
}%
Thus, we also have $n$ orthonormal eigenvectors,
$\pmb\phi^{(1)}, \dots, \pmb\phi^{(n)}$,
compatible with Eq. \eqref{eq:phi_refl},
%
\begin{equation}
  \phi^{(k)}_i
  =
  \phi_{k i}
  =
  \frac{ \sqrt{ 2 - \delta_{k, 1} } }
       {             n              }
  \cos \frac{ ( k - 1) \, \left( i - \frac 1 2 \right) \, \pi}
            {                    n                           }
  ,
\notag
%\label{eq:wband_eigenvector_refl}
\end{equation}
%
with eigenvalues
%
\begin{align}
  \lambda_k
  &=
  \mu_0
  +
  2
  \sum_{l = 1}^b
    \mu_l
    \cos \frac{(k - 1)  \, l \pi}{n}
  .
\label{eq:wband_eigenvalue_refl}
\end{align}
%
\note{Derivation.
  First consider the unnormalized eigenvectors,
  $$
  \Phi^{(k)}_i
  =
  \cos \frac{ \left( i - \frac 1 2 \right) (k - 1) \, \pi}{n},
  $$
  which satisfies the reflective boundary condition,
  Eq. \eqref{eq:phi_refl}.
  %
  So, by Eq. \eqref{eq:wmul_to_convol},
  $$
  \begin{aligned}
  \sum_{i = 1}^n
    \Phi^{(k)}_i \, w_{ij}
  &=
  \sum_{l = -b}^b
    \Phi^{(k)}_{j - l} \, \mu_l
  \\
  &=
    \Phi^{(k)}_j \, \mu_0
  + \sum_{l=0}^{b}
    \mu_l \,
    \left[
      \Phi^{(k)}_{j-l}
      +
      \Phi^{(k)}_{j+l}
    \right]
  \\
  &= \Phi^{(k)}_j \, \lambda_k,
  \end{aligned}
  $$
  with $\lambda_k$ given by Eq. \eqref{eq:wband_eigenvalue_refl}.

  \hrulefill

  Next, let us compute the normalization factor.
  %
  If $k = 1$, $\Phi^{(1)}_i = 1$, and
  $
  \sum_{i = 1}^n \left( \Phi^{(1)}_i \right)^2 = n.
  $
  For $k \ge 2$,
  $$
  \begin{aligned}
    \sum_{i = 1}^n \cos^2 \left[\left(i - \frac1 2 \right) a\right]
    &=
    \frac n 2
    +
    \frac 1 2
    \sum_{i = 1}^n \cos\left[(2 i - 1)\, a \right]
    =
    \frac n 2,
  \end{aligned}
  $$
  where $a = \frac{k-1}{n} \pi$.
  Thus, the normalization factor $\sqrt{(2 - \delta_{k1})/n}$
  encompasses both cases.

  \hrulefill

  To show the column-wise orthogonality,
  we need a lemma, for $a = q \, \pi/n$,
  with $q$ a positive integer.
  $$
  \begin{aligned}
  \sum_{k = 1}^n \cos[(k - 1) \, a]
  &=
  1 + \cos a + \dots + \cos[(n - 1) \, a]
  \\
  &=
  \frac{
        \sin\frac a 2
      + \sin \left[ \left( n - \frac 1 2 \right) a \right]
      }
      {
        2 \, \sin \frac a 2
      }
  \\
  &=
  \frac{ 1 - (-1)^q } { 2 }
  = \operatorname{odd}(q),
  \end{aligned}
  $$
  %
  where, we have used
  $\sin \left[ \left( n - \frac 1 2 \right) a \right]
  = \sin \left( q \, \pi - \frac a 2 \right)
  = -(-)^q\sin\frac a 2.$
  %
  and $\operatorname{odd}(q)$
  yields $1$ for odd $q$ or $0$ otherwise.
  %
  Adding the case of $q = 0$, where the sum yields $n$,
  we get
  $$
  \sum_{k = 1}^n
    \cos\left[(k - 1) \, \frac { q \, \pi } { n }  \right]
  = n \, \delta_{q, 0}
  + \operatorname{odd}(q).
  $$

  Now, for the orthogonality,
  $$
  \begin{aligned}
    \sum_{k = 1}^n
    \Phi^{(k)}_i \, \Phi^{(k)}_j
    &=
    \frac 1 2
    \sum_{k = 1}^n
    \left[
      \cos \tfrac{ (i - j) (k - 1) \, \pi }
                 {         n              }
      +
      \cos \tfrac{ (i + j - 1) (k - 1) \, \pi }
                 {             n              }
    \right]
    \\
    &=
    \frac 1 2
    \left[
      n \, \delta_{i, j}
      +
      \operatorname{odd}(i - j)
      +
      \operatorname{odd}(i + j - 1)
    \right]
    \\
    &=
    \frac n 2 \, \delta_{i, j}
    + \frac 1 2.
  \end{aligned}
  $$
  where we have used the fact
  that $i - j$ shares the parity with $i + j$
  in the last step.
  %
  So
  $$
    \sum_{k = 1}^n
    \phi^{(k)}_i \, \phi^{(k)}_j
    =
    \frac 2 { n^2 }
    \sum_{k = 1}^n
    \Phi^{(k)}_i \, \Phi^{(k)}_j
    -
    \frac 1 { n^2 }
    =
    \frac 1 n \,
    \delta_{i, j}.
  $$
}
%


\section{Examples of the inversion formula
for a non-periodic variable}


In the non-periodic case,
the formula is
%
\begin{align}
  \mu_l
  =
  \frac 1 { 2 \, n }
  \sum_{ k = 1 }^{ 2 \, n }
    \lambda_{ k } \,
    \cos \frac{ (k - 1) \, l \, \pi }
              {            n        }
  ,
\label{eq:mu_from_lambda_refl}
\end{align}
%
where
we have defined
$\lambda_k \equiv \lambda_{2 \, n + 2 - k}$
for $k = n + 2, \dots, 2 \, n$,
as well as
%
\begin{align}
  \lambda_{ n + 1 }
  =
  (-1)^{ n - 1 }
  \lambda_1
  +
  2 \, \sum_{ k = 2 }^{ n }
      (-1)^{n - k} \, \lambda_k
  ,
\label{eq:lambdan}
\end{align}
to satisfy the constraint $\mu_n = 0$.
%
\note{Eq. \eqref{eq:lambdan}
  ensures that the $\mu_n$
  computed from Eq. \eqref{eq:mu_from_lambda_refl}
  vanishes.

  %The inversion formula is derived from the cosine transform.
  %
  If we define $\mu_n = 0$ and
  %
  \begin{align}
    \mu_{ 2 n - l } = \mu_l
    \quad
    \mathrm{for\;} l = 1, \dots, n - 1,
  \notag
  %\label{eq:mu_reflection}
  \end{align}
  %
  then Eq. \eqref{eq:wband_eigenvalue_refl} can be rewritten as
  %
  \begin{align}
    \lambda_{k+1}
    =
    \sum_{ l = 0 }^{ 2 \, n - 1 }
    \mu_l \, \cos \frac{ k \, l \, \pi } { n }.
  \notag
  %\label{eq:lambda_cosine_sum}
  \end{align}
  %
  This formula for $\lambda_{k+1}$
  can be readily extended to $k = 2 \, n - 1$,
  and we have
  %
  \begin{align}
    \lambda_{ 2 \, n + 2 - k } = \lambda_k.
    \tag{N3}
    \label{neq:lambda_reflection}
  \end{align}
  %
  Thus,
  %
  $$
  \begin{aligned}
    \sum_{ k = 0 }^{ 2 \, n - 1 }
      \lambda_{ k + 1 } \,
      \cos \frac{ k \, p \, \pi }
                {      n        }
    &=
    \sum_{ k = 0 }^{ 2 \, n - 1 }
      \sum_{ l = 0 }^{ 2 \, n - 1 }
        \mu_l \,
        \cos \frac{ k \, l \, \pi }
                  {      n        }
        \cos \frac{ k \, p \, \pi }
                  {      n        }
    \\
    &=
    \sum_{ l = 0 }^{ 2 \, n - 1 }
      \frac{ \mu_l } { 2 }
      \sum_{ k = 0 }^{ 2 \, n - 1 }
        \cos \frac{ k \, (p + l) \, \pi }
                  {      n        }
                  +
        \cos \frac{ k \, (p - l) \, \pi }
                  {      n        }
    \\
    &=
    \sum_{ l = 0 }^{ 2 \, n - 1 }
      \mu_l \, n \left(
        \delta_{ p + l - 2 \, n, 0 }
        +
        \delta_{ p - l, 0 }
      \right)
    \\
    &=
    n \, \left( \mu_p + \mu_{ 2 \, n - p} \right)
    =
    2 \, n \, \mu_p.
  \end{aligned}
  $$
  This entails
  $$
  \begin{aligned}
    \mu_l
    &=
    \frac{    1   }
         { 2 \, n }
    \sum_{ k = 0 }^{ 2 \, n - 1 }
      \lambda_{ k + 1 } \,
      \cos \frac{ k \, l \, \pi }
                {      n        }
              \\
    &=
    \frac{    1   }
         { 2 \, n }
    \left[
      \lambda_1
      +
      (-1)^l \, \lambda_{n + 1}
      +
      2 \sum_{ k = 1 }^{ n - 1 }
        \lambda_{ k + 1 } \,
        \cos \frac{ k \, l \, \pi }
                  {      n        }
    \right],
  \end{aligned}
  $$
  where we have used Eq. \eqref{neq:lambda_reflection}
  in the last step.

  However, we are usually given only $\lambda_1, \dots, \lambda_n$
  without $\lambda_{n + 1}$.
  %
  Fortunately, we can deduce the latter from
  the condition that $\mu_n = 0$, which means
  $$
  \begin{aligned}
    0 = \mu_n
    =
    \frac{1}{n}
    \left[
      \frac{ \lambda_1 + (-1)^n \, \lambda_{n+1} }
           {               2                     }
      +
      \sum_{ k = 1 }^{ n - 1 }
      \lambda_{k+1} (-1)^k
    \right].
  \end{aligned}
  $$
  This allows us to solve for $\lambda_{ n + 1 }$,
  yielding Eq. \eqref{eq:lambdan}.


  Some examples for checking.
  For $n = 2$,
  $\lambda_3
  = \mu_0 - 2 \, \mu_1
  = -(\lambda_1 - 2 \, \lambda_2)$,
  and
  $$
  \begin{aligned}
  \mu_0
  =
  \lambda_2,
  \qquad
  \mu_1
  =
  \frac{ \lambda_1 - \lambda_2 }
       {           2           }
  .
  \end{aligned}
  $$
  For $n = 3$,
  $\lambda_4
  = \mu_0 - 2 \, \mu_1 + 2 \, \mu_2
  = \lambda_1 - 2 \, \lambda_2 + 2 \, \lambda_3$,
  and
  $$
  \begin{aligned}
  \mu_0
  =
  \frac{ \lambda_1 + 2 \, \lambda_3 } { 3 }
  ,
  \quad
  \mu_1
  =
  \frac{ \lambda_2 - \lambda_3 } { 2 }
  ,
  \quad
  \mu_2
  =
  \frac { 2 \, \lambda_1 - 3 \, \lambda_2 + \lambda_3 } { 6 }
  .
  \end{aligned}
  $$
}



\section{Bandpass scheme for a non-periodic variable}



For a non-periodic variable, we demand
$$
\lambda_1 = \cdots = \lambda_{K+1} = 1,
\qquad
\lambda_{K+2} = \cdots = \lambda_n = 0.
$$
Then Eq. \eqref{eq:lambdan} gives
$\lambda_{n+1} = (-1)^{n+K-1}$,
and from Eq. \eqref{eq:mu_from_lambda_refl},
we have
\begin{equation}
  \mu_l
  =
  \frac{1}{2 \, n}
  \left[
    (-1)^{n+K+l-1}
    +
    \frac{
      \sin
      \dfrac{ (2 K + 1) \, l \, \pi }
           {         2 \, n        }
    }
    {
      \sin \dfrac{ l \, \pi } { 2 \, n }
    }
  \right]
  .
\label{eq:mu_sinc_refl}
\end{equation}
\note{Derivation.
$$
\begin{aligned}
  \lambda_{n+1}
  &=
  (-1)^{n-1}
  \left[
    \lambda_1
    + 2 (-\lambda_2 + \lambda_3 - \cdots + (-1)^K \lambda_{K+1})
  \right]
  \\
  &=
  \begin{dcases}
    (-1)^{n-1} & K \mathrm{\; even,} \\
    (-1)^n     & K \mathrm{\; odd.}
  \end{dcases}
\end{aligned}
$$
So
$$
\begin{aligned}
  \mu_l
  &=
  \frac{1}{2\,n}
  \left[
    1 +
    2 \sum_{k=2}^{K+1}
    \cos \frac { (k - 1) \, l \pi } { n }
    +
    (-1)^{n+K-1} (-1)^l
  \right]
  \\
  &=
  \frac{1}{2\,n}
  \left[
    (-1)^{n+K+l-1}
    +
    \sum_{k=-K}^{K}
    \cos \frac { k \, l \pi } { n }
  \right]
  .
\end{aligned}
$$

\hrulefill

Example 1. If $n = 3, K = 1$,
then $\lambda_1 = \lambda_2 = 1$, $\lambda_3 = 0$,
and $\lambda_4 = \lambda_1 - 2 \, \lambda_2 + 2 \, \lambda_3 = -1$.
%
We can verify that $\mu_0 = 1/3, \mu_1 = 1/2, \mu_2 = -1/6$.
%
Although $\mu_1 > \mu_0$, the system will not be trapped
in the current bin because of the reflective boundary condition.
%
Upon a visit to the middle bin, $j = 2$,
$v_i$ at bin $i = 1$ or $3$ is updated by $1/2 - 1/6 = 1/3$,
same as the update on bin $2$.
Upon a visit to bin $i = 1$, the three bins are updated
by $1/2 + 1/3 = 5/6$, $1/2 - 1/6 = 1/3$, and $-1/6$,
respectively.

\hrulefill

Example 2. If $n = 3, K = 2$,
then $\lambda_1 = \lambda_2 = \lambda_3 = 1$, $\lambda_4 = 1$.
We have $\mu_0 = 1, \mu_1 = \mu_2 = 0$,
recovering the single-bin scheme.

\hrulefill

Example 3. If $n = 4, K = 1$,
then $\lambda_1 = \lambda_2 = 1, \lambda_3 = \lambda_4 = 0$,
$\lambda_5 = (-1)(\lambda_1 - 2 \, \lambda_2 + 2 \, \lambda_3 - 2 \, \lambda_4 = 1$.
The kernel is given by
$\mu_0 = 1/2, \mu_1 = \sqrt{2}/8, \mu_2 = 1/4, \mu_3 = -\sqrt{2}/8$.
}



\section{Results}


\subsection{\label{sec:results_invt}
Inverse-time schedule}



For the single-bin updating scheme, we
%we chose $\alpha_0 = 10^{-4}$ and
ran the simulation for $T = 10^8$ steps.
%
As shown in Fig. \ref{fig:errsbin},
the numerical results of the terminal error agreed well with
the analytical prediction.
%
For both global and local sampling processes,
the optimal $\lambda$ occurred around $1.0$,
verifying the optimality of Eq. \eqref{eq:alpha_invt1}.
%
%Recall that $1/\lambda$ in Eq. \eqref{eq:alpha_invtlambda}
%represents the relative updating magnitude.
%%
%Figure \ref{fig:errsbin} also shows
%an asymmetry of the error curve.
%A smaller than optimal value of
%the updating magnitude, $1/\lambda$,
%tends to increase the error more rapidly
%than a larger than optimal value,
%suggesting that the over-updating
%is generally preferable to the under-updating
%when the optimal value is unknown.
%
The two error functions
differed only by a multiplicative constant,
also in agreement with the theory
(cf. Appendix \ref{sec:invt_singlebin}).
%


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig1/bak/errsbin.pdf}
  }
  \caption{
    \label{fig:errsbin}
    Error, $E$, versus $\lambda$
    in Eq. \eqref{eq:alpha_invtlambda}
    for the single-bin (WL) scheme.
    %
    The results were averaged over $1000$ independent runs.
    %
    \note{The figure was produced by \texttt{doc/fig1/bak/errsbin.gp}.
      The data were produced by the script \texttt{data/invtcscan.py},
      and saved under \texttt{data/singlebin}
      (see the \texttt{Makefile} there).
    }%
  }
\end{center}
\end{figure}


As an example of multiple-bin schemes,
we tested the Triple-bin updating scheme
described in Sec. \ref{sec:nnscheme},
%
with
$\mu_1 = 0.24$ to make the smallest eigenvalue
close to zero.
%
Again, simulation and theory agreed well,
as shown in Fig. \ref{fig:errnnbr}.
%
The optimal value of $1/\lambda$,
increased with the simulation length, $T$,
in the global sampling case
(cf. Appendix \ref{sec:invt_deplength}).
%
However, the shift was suppressed
for the local sampling case,
with the optimal $\lambda$ remained around $1.0$
[cf. Eq. \eqref{eq:lambda_nn_onestep}].



%To understand this dependence,
%we observe that for shorter simulations,
%long-wavelength modes
%(with larger $\lambda_k$ and smaller $k$),
%dominate the error function in Eq. \eqref{eq:error_invt}
%because of the prefactor $\lambda_k \, \Gamma_k$,
%which is inherited from
%the equilibrium error under a fixed updating magnitude,
%Eq. \eqref{eq:error_eql}.
%%
%For longer simulations,
%short-wavelength modes
%(with smaller $\lambda_k$ and larger $k$)
%become more important,
%for $\lambda_k$ also gives the decay rate
%of the $k$th mode.
%%and errors in smaller $\lambda_k$ modes
%%are harder to damp out.
%%
%This drives the optimal $\lambda$ to shift toward
%a smaller $\lambda_k$ value
%as the simulation lengthens.
%%
%Note also that while increasing $\lambda$
%also increases the error of the long-wavelength modes,
%this rate of increase is much smaller,
%as shown in the $\nu_k > 1$ branch
%in Fig. \ref{fig:err_component}.
%%
%The shift of $\lambda$, however, is less noticeable
%under the local sampling scheme,
%because longer-wavelength modes more heavily
%weighted by much larger values of $\Gamma_k$.


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig1/bak/errnnbr.pdf}
  }
  \caption{
    \label{fig:errnnbr}
    Normalized error, $(T + t_0) \, E$,
    versus the %proportionality constant,
    relative updating magnitude,
    $1/\lambda$,
    in Eq. \eqref{eq:alpha_invtlambda}
    for the triple-bin updating scheme with $\mu_1 = 0.24$.
    %
    The results were averaged over $1000$ independent runs.
    %
    \note{This figure was produced by \texttt{doc/fig1/bak/errnnbr.gp}.
      The data were produced by \texttt{data/invt2run.py},
      saved under \texttt{data/nb0.24}
      (see the \texttt{Makefile} there).
    }%
  }
\end{center}
\end{figure}





%\subsection{Gaussian (metadynamics) updating scheme}
%
%
%The error of Gaussian updating scheme is similar to
%that of the triple-bin scheme,
%as shown in Fig \ref{fig:err_sig5}.
%%
%The dependency of the simulation length is more prominent.
%%
%Even for local sampling,
%there is now a visible shift of the optimal $\lambda$.
%
%
%\begin{figure}[h]
%\begin{center}
%  \makebox[\linewidth][c]{
%    \includegraphics[angle=0, width=0.95\linewidth]{fig1/bak/errsig5.pdf}
%  }
%  \caption{
%    \label{fig:err_sig5}
%    Normalized error, $(T + t_0) \, E$,
%    versus the %proportionality constant,
%    relative updating magnitude,
%    $1/\lambda$,
%    in Eq. \eqref{eq:alpha_invtlambda}
%    for the Gaussian updating scheme with $\sigma = 5$ bins.
%    %
%    The results were averaged over $1000$ independent runs.
%  }
%\end{center}
%\end{figure}
%




\subsection{\label{sec:results_optschedule}
Optimal schedule}



We now turn to the optimal schedule
predicted by Eq. \eqref{eq:q_opt}.
%
For the Gaussian updating scheme
with $\sigma = 10$,
we computed the schedule for the local sampling process,
and compared it with the inverse-time schedule
Eq. \eqref{eq:alpha_invtlambda}
with the optimal $\lambda$.
%
As shown in Fig. \ref{fig:optacmp}(a),
the optimal schedule is a more complex curve
than the optimized inverse-time schedule.
%
The complexity of the optimal schedule
corresponds to the non-exponential tail
of the mass distribution function,
as shown Fig. \ref{fig:optacmp}(b)
[cf. Sec. \ref{sec:mass_distr}].
%
The errors in the two cases were
$9.1 \times 10^{-5}$
and
$1.7 \times 10^{-4}$,
respectively,
%for the local sampling process,
showing the superiority of
the optimal schedule from Eq. \eqref{eq:q_opt}.
%
Besides, we verified the scaling property
discussed in Sec. \ref{sec:mass_distr},
by comparison with a simulation [Fig. \ref{fig:optacmp}(a)]
of half length, $T' = 5 \times 10^7$
and doubled initial updating magnitude,
$\alpha_0 = 2 \times 10^{-4}$,
which left the normalized mass distribution invariant,
as shown in Fig. \ref{fig:optacmp}(b).


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig1/optacmp.pdf}
  }
  \caption{
    \label{fig:optacmp}
    Optimal schedules from Eq. \eqref{eq:q_opt},
    versus the optimized inverse-time schedule
    Eq. \eqref{eq:alpha_invtlambda}
    for the Gaussian updating scheme
    with $\sigma = 10$
    and the local updating process.
    %
    \note{This figure was produced by \texttt{doc/fig1/optacmp.gp}.
      The data were computed by the program \texttt{prog/predict},
      see particularly the function \texttt{intq\_save()}
      in \texttt{prog/intq.h}.
      The data were saved under the directory \texttt{data/opta}.
      See the \texttt{make} object \texttt{sigsingle}
      in the \texttt{Makefile} there.
    }%
  }
\end{center}
\end{figure}




On the other hand,
the optimal schedules of the bandpass updating schemes
are always given by the inverse-time formula,
Eq. \eqref{eq:alpha_invt1},
with $t_0$ given by Eq. \eqref{eq:t0_sinc},
as shown in Fig. \ref{fig:sinc}(a).
%


\appendix


\section{\label{sec:invt_schedule}
Inverse-time schedules}



The following contains a more complete discussion
on the inverse-time schedule


\subsection{\label{sec:invt_error}
Error
}



Using Eq. \eqref{eq:alpha_invtlambda}
in Eq. \eqref{eq:error_asym1} yields
%
\begin{align}
\Err_A
&=
\frac{    1    }
     { T + t_0 }
\sum_{k = 2}^n
  \frac{ \Gamma_k \, \nu_k^2 }
       {    2 \, \nu_k - 1   }
\left[
  1 - \left(
        \frac {     t_0 }
              { T + t_0 }
      \right)^{ 2 \, \nu_k - 1 }
\right],
\label{eq:error_asym_invt}
\end{align}
%
where $\nu_k \equiv \lambda_k / \lambda$.
%
The error is asymptotically minimal
only if $2\,\nu_k > 1$ for every $k$.
%
At long times, $T \to \infty$, we get
$$
\begin{aligned}
  \Err_A
  =
  \sum_{k = 2}^n
  \begin{dcases}
    \frac{    1    }
         { T + t_0 }
    \frac{ \Gamma_k \, \nu_k^2 }
         {   2 \, \nu_k - 1    }
    &
    \mathrm{if \;} 2 \, \nu_k > 1,
    \\%[1em]
    %
    %
    \frac{    \Gamma_k    }
         { 4 \, (T + t_0) }
    \ln \frac{ T + t_0 }
             {   t_0   }
    &
    \mathrm{if \;} 2 \, \nu_k = 1,
    \\
    %
    %
    \frac{  t_0^{ 2 \, \nu_k  - 1}  }
         { (T + t_0)^{ 2 \, \nu_k } }
    \frac{ \Gamma_k \, \nu_k^2 }
         {   1 - 2 \, \nu_k    }
    &
    \mathrm{if \;} 2 \, \nu_k < 1.
  \end{dcases}
\end{aligned}
$$
%
The last two cases
are asymptotically suboptimal
as they decay slower than $1/T$.
%
In this case,
we can drop the second term
in the square brackets,
and identify the optimal $\lambda$ as
the smallest real root of
%
\begin{equation}
  \sum_{k = 2}^n
    \frac { \lambda_k - \lambda }
          { \left(2 - \lambda/ \lambda_k \right)^2 }
  = 0.
\notag
%\label{eq:optimal_lambda_approx}
\end{equation}
%
Since
$$
\frac{ \nu_k^2        }
     { 2 \, \nu_k - 1 }
=
\frac{ \lambda_k^2 }
     { \lambda \, (2 \, \lambda_k - \lambda) }
\ge 1
,
$$
with the equality achieved only at $\lambda = \lambda_k$,
the optimal scheme must satisfy
$\lambda_2 = \dots = \lambda_n$,
corresponding to the single-bin scheme
(cf. Sec. \ref{sec:optWL}).



For a finite-length simulation,
we need to include the residual error, $\Err_R$.
%
Assuming Eqs. \eqref{eq:error_res1} and \eqref{eq:t0_sinc},
we get
%
\begin{equation}
\Err_R
=
\sum_{k = 2}^n
  \frac{ \Gamma_k \, \nu_k }
       {        t_0   }
  \left(
      \frac{   t_0   }
           { T + t_0 }
   \right)^{ 2 \, \nu_k },
\notag
%\label{eq:error_res_invt}
\end{equation}
%
with
%
\begin{equation}
  q(T)
  =
  \frac{ 1 } { \lambda }
  \ln\left(
    \frac{ T + t_0 } { t_0 }
  \right)
  ,
\notag
%\label{eq:qt_invtlambda}
\end{equation}
%
from integrating Eq. \eqref{eq:alpha_invtlambda}.
%
%We assume that
%the system was initially equilibrated
%at a constant $\alpha_0$,
%such that the values of
%$\left\langle y_k^2(0) \right\rangle$
%can be computed from Eq. \eqref{eq:y2_eql},
%as well as Eq. \eqref{eq:t0_sinc}.
%
The total error is
%
\begin{align}
\Err
&=
\Err_R + \Err_A
\notag
\\
&=
\sum_{ k = 2 }^n
  \frac
  {
    \Gamma_k \, \nu_k \,
    \left[
      \nu_k
      +
      (\nu_k - 1)
      \left(
        \frac{ t_0 } { T + t_0 }
      \right)^{ 2 \, \nu_k - 1 }
    \right]
  }
  {
    (T + t_0) \, (2 \, \nu_k - 1)
  }
  .
\label{eq:error_invt}
\end{align}
%
%where
%$$
%\Err^f_k
%\equiv
%\left\langle
%  y_k^2
%\right\rangle_{ \alpha(t) }
%=
%\frac{  \Gamma_k \, \nu_k   }
%     {    2 \, (T + t_0)    }
%,
%$$
%is the equilibrium, or saturated, error of mode $k$
%at the terminal updating magnitude
%of $\alpha(T) = 1/[\lambda (T + t_0)]$.

%\note{Derivation of Eq. \eqref{eq:error_invt}:
%$$
%\begin{aligned}
%\Err_R
%&=
%\sum_{ k = 2 }^n
%  \Err^f_k \left( \frac{   t_0   }
%                       { T + t_0 }
%          \right)^{ 2 \nu_k - 1 }
%\\
%\Err_A
%&=
%\sum_{ k = 2 }^n
%  \frac{ 2 \, \Err^f_k \, \nu_k }
%       {    2 \, \nu_k - 1      }
%  \left[
%    1 -
%    \left(
%      \tfrac{   t_0   }
%            { T + t_0 }
%      \right)^{2 \, \nu_k - 1}
%  \right]
%\\
%&=
%\sum_{ k = 2 }^n
%  \Err^f_k \,
%  \left[
%    1 -
%    \left(
%      \tfrac{   t_0   }
%            { T + t_0 }
%      \right)^{2 \, \nu_k - 1}
%  \right]
%  +
%  \frac{   \Err^f_k     }
%       { 2 \, \nu_k - 1 }
%  \left[
%    1 -
%    \left(
%      \tfrac{   t_0   }
%           { T + t_0 }
%      \right)^{2 \, \nu_k - 1}
%  \right].
%\end{aligned}
%$$
%Adding the two yields Eq. \eqref{eq:error_invt}.
%}



%We give several remarks on Eq. \eqref{eq:error_invt}.
%

%Clearly, the final error is always greater than
%the saturated equilibrium error
%at the terminal $\alpha(T)$:
%$$
%E \ge E_\mathrm{sat} \equiv \sum_{k = 2}^n E^f_k,
%$$
%because the second term in the braces
%of Eq. \eqref{eq:error_invt}
%is nonnegative no matter the sign of $2 \, \nu_k - 1$
%(in fact, it is a decreasing function
%that vanishes only at $\nu_k \to +\infty$).
%%
%For a set of nonidentical $\lambda_k$'s,
%the equality is reached only at $T = 0$.





\subsection{\label{sec:optlambda}
Optimal proportionality constant
}




Intuitively,
we expect the optimal $\lambda$
that minimizes the error of the inverse-time schedule,
Eq. \eqref{eq:error_invt},
to be a kind of time average of
the instantaneous eigenvalue $\lambda(t)$
in Sec. \ref{sec:optschedule}.
%
Thus, it should also decrease with
the simulation length, $T$.
%
Below we show this is indeed so.

Let us first rewrite Eq. \eqref{eq:error_invt} as
%
$$
\Err
=
\sum_{ k = 2 }^n
  \frac{ \Gamma_k  }
       { (T + t_0) }
  G\left( \nu_k, \frac{ t_0 } { T + t_0} \right),
$$
%
where the $k$th component is proportional to
%
$$
G\left( \nu_k, r \right)
=
\frac { \nu_k
        \,
        \left[
          \nu_k
          +
          (\nu_k - 1) \,
          r^{2 \, \nu_k - 1}
        \right]
      }
      { 2 \, \nu_k - 1 }
.
$$
%
As shown in Fig. \ref{fig:err_component},
the function $G(\nu_k, r)$ depends non-trivially on $\nu_k$.
%
For a fixed and sufficiently small $r = t_0 / (T + t_0)$,
$G(\nu_k, r)$ has two local minima along $\nu_k$,
one at $\nu_k = 0$,
and the other shallower one around $\nu_k = 1$.
%
For a short simulation, or a large $r$,
the minimal error may be achieved at $\nu_k \approx 0$,
corresponding to a large value of $\lambda$
($\lambda \gg \lambda_k$).
%
However, this minimum is unstable,
for $G(\nu_k, r)$ increases rapidly
with decreasing $r$, or increasing simulation length,
under a fixed $\nu_k$.
%
In other words,
asymptotically, the minimal error
is always achieved at the other minimum,
$\nu_k = 1$ or $\lambda = \lambda_k$.
%
In this case,
$G(\nu_k, r) \to G(1, 0) = 2$
and the total error is roughly twice $E_\mathrm{sat}$.



\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig1/bak/errcomp.pdf}
  }
  \caption{
    \label{fig:err_component}
    Component of the error, $G(\nu_k, r)$,
    versus $\nu_k = \lambda_k / \lambda$,
    where $r = t_0 / (T + t_0)$.
    %
    \note{This figure is produced by
      \texttt{doc/fig1/bak/errcomp.gp}.
    }
  }
\end{center}
\end{figure}



For a set of nonidentical eigenvalues,
$\lambda_2, \dots, \lambda_n$,
it is impossible to achieve $\lambda = \lambda_k$
for every $k$,
and we shall argue that
the optimal $\lambda$
tends to be biased toward
the low end of the spectrum
in a long simulation.
%
From Fig. \ref{fig:err_component},
it is seen that around the minimum at $\nu_k = 1$,
the error function $G(\nu_k, r)$
has a steeper slope for the $\nu_k < 1$
(or $\lambda > \lambda_k$) side
than for the $\nu_k > 1$ (or $\lambda < \lambda_k$) side.
%
This means a smaller $\lambda$ is preferred
in terms of minimizing the error.
%
Further, the difference in slopes
between the two sides increases
with the simulation length, $T$
(or as $r$ decreases).
%
We thus expect the optimal $\lambda$
to shift toward the smaller values of $\lambda_k$'s,
and ultimately to approach $\min \lambda_k$
in the infinite $T$ limit.

The shift of the optimal $\lambda$
is also affected by the values of $\Gamma_k$'s.
%
If large $\lambda_k$'s are associated with
large $\Gamma_k$'s,
these modes weight more in the error function,
and the above shift toward smaller $\lambda$
slows down.
%
For example,
in the nearest-neighbor or Gaussian updating scheme
discussed in Sec. \ref{sec:band-matrix},
the index $k$ represents the wave number
(or the inverse wave length)
and $\lambda_k$ decreases with $k$.
%
As shown in Eq. \eqref{eq:Gamma_onestep},
in the one-step sampling process,
$\Gamma_k$ also decreases with $k$,
and thus, $\lambda_k$ and $\Gamma_k$
are positively correlated.
%
Therefore, we expect the shift of the optimal $\lambda$
in the above case
to be slower than in that in the case of perfect sampling,
where $\Gamma_k \equiv 1$ is a constant.
%
If $\Gamma_k$ decreases too quickly with $k$,
the shift can be hard to detect
[see Eq. \eqref{eq:lambda_nn_onestep} for an example].


\subsection{\label{sec:invt_deplength}
Dependence on the simulation length}



For a finite-length simulation,
we generally expect the optimal $\lambda$
that minimizes the total error to
decrease with the simulation length $T$.
%
From Eq. \eqref{eq:alpha_invt}, we have
$$
  \lambda
  =
  \Delta \left( \frac{ 1 } { T \, \alpha } \right)
  \equiv
  \left.
  \frac { 1 } { T \, \alpha(t) }
  \right|_{t = 0}^{t = T}
  .
$$
This expression corresponds to the height difference
of the mass distribution
$
\bigl. m(Q) \bigr|_{Q = q(T)}^{Q = 0}
$
in the optimal schedule
(cf. Sec. \ref{sec:mass_distr}).
%
With a fixed $\alpha_0$,
increasing the simulation length $T$
also increases the optimal span $q(T)$ of the distribution
(cf. Sec. \ref{sec:optinitalpha}),
and thus reduces the height difference.
%
This effect is significant
if the distribution $m(Q)$ has a long tail,
or if some eigenvalues $\lambda_k$'s are near zero.
%
It is often suppressed in local sampling processes,
because the eigenmodes with large $\lambda_k$'s
are accompanied by large $\Gamma_k$'s,
which shortens the distribution tail.





\subsection{Nearest-neighbor updating scheme}


For the nearest-neighbor updating scheme
introduced in Sec. \ref{sec:nnscheme},
we can find an analytical expression
of the optimal value of $\lambda$
in the limit of large $n$.
%
In this case,
we convert Eq.
\eqref{eq:error_asym_invt}
%and
%\eqref{eq:optimal_lambda_approx}
to an integral,
$\sum_i \to \frac{2 \, n}{\pi} \int dp$, and
%
\begin{align}
\Err_A
=
\frac{2 \, n}{\pi \, T}
\int_0^{\pi/2}
  \frac{ \Gamma(p) \, \left(1 - 4 \, \mu_1 \, \sin^2 p \right)^2    }
       {   \lambda \, \left(2 - 8 \, \mu_1 \, \sin^2 p - \lambda \right) }
\, dp.
\notag
%\label{eq:error_nn_asym_int}
\end{align}



For perfect sampling,
$\Gamma(p) = 1$ from Eq. \eqref{eq:Gamma_perfect},
and
$$
\begin{aligned}
  \Err_A
  %&=
  %\frac{2 \, n}{\pi \, T}
  %\int_0^{\pi/2}
  %\frac{ \left(1 - 4 \, \mu_1 \, \sin^2 p \right)^2 }
  %{ \lambda \, \left(2 - 8 \, \mu_1 \, \sin^2 p - \lambda \right) }
  %\, dp
  %\notag \\
  &=
  \frac{n}{4 \, T}
  \left(
    \frac{2 - 4 \, \mu_1 + \lambda}{ \lambda }
    +
    \frac{ \lambda }
    { \sqrt{ (2 - \lambda) (2 - 8 \, \mu_1 - \lambda) } }
  \right)
.
\end{aligned}
$$
\note{The integral is evaluated by contour integration.
%
It is somewhat more convenient to change variable $p \to \frac{ \pi } { 2 } - p$,
and
$$
\begin{aligned}
  \Err_A
  &=
  \frac{n}{\lambda \, T}
  \frac{1}{2 \pi i}
  \int_0^{2 \, \pi}
  \frac{ \left(1 - 4 \, \mu_1 \, \cos^2 p \right)^2 }
  { 2 - 8 \, \mu_1 \, \cos^2 p - \lambda }
  \, dp
  \\
  &=
  \frac{n}{\lambda \, T}
  \frac{1}{2 \pi i}
  \oint
  \frac{ \left[1 - \mu_1 \, \left(z+\frac{1}{z}\right)^2 \right]^2 }
  { 2 - 2 \, \mu_1 \, \left(z + \frac{1}{z}\right)^2 - \lambda }
  \, \frac{dz}{z}
  ,
\end{aligned}
$$
where the contour is the unit circle
in the complex plane of $z$.
%
This integral has five poles, one at the origin,
which produces a residue
$\frac{2 - 4 \, \mu_1 - \lambda}{4}$
(from series expansion of small $z$),
and the other four at
$$
z + \frac{1}{z} = \pm\sqrt\frac{2-\lambda}{2 \, \mu_1},
$$
with two of them inside the unit circle:
$$
z_\pm = \pm \frac{\sqrt{2-\lambda} -\sqrt{2 - 8 \, \mu_1 - \lambda}}
{\sqrt{8 \mu_1}}.
$$
Thus,
$$
\begin{aligned}
\Err_A
&=
\frac{      n       }
     { \lambda \, T }
\left(
 \frac{ 2 - 4 \, \mu_1 - \lambda }
      {          4               }
 +
 \sum_{z = z_{\pm} }
 \frac{ \left(z^2 - \mu_1 (1 + z^2)^2 \right)^2 }
 { 2 z^4 (2 - 4 \mu_1 - \lambda - 4 \mu_1 z^2) }
\right)
\\
&=
\frac{       n      }
     { \lambda \, t }
\left(
  \frac{ 2 - 4 \, \mu_1 - \lambda }
       {          4               }
 +
 \frac{ \left(1 - \mu_1 \left(z + \frac{1}{z} \right)^2 \right)^2 }
      { 2 - 4 \mu_1 - \lambda - 4 \mu_1 z_{\pm}^2                 }
\right)
\\
&=
\frac{       n      }
     { \lambda \, T }
\left(
  \frac{ 2 - 4 \, \mu_1 - \lambda }
       {          4               }
 +
 \frac{ (\lambda/2)^2 }
 { \sqrt{(2-\lambda) (2 - 8 \mu_1 -\lambda)} }
\right).
\end{aligned}
$$
}
%
Minimizing the function yields
%
\begin{equation}
  \lambda
  =
  \frac{ 1 - 4 \, \mu_1 }
       { 1 - 2 \, \mu_1 }
  ,
\notag
%\label{eq:lambda_nn_perfect}
\end{equation}
%
and the asymptotic error in the optimal case is given by
%
\begin{equation}
  \Err_A
  =
  \frac{n}{T}
  \left(
    1+ \frac{2 \, \mu_1^2}{1-4 \, \mu_1}
  \right)
  ,
\notag
%\label{eq:error_nn_perfect}
\end{equation}
%
%From Eq. \eqref{eq:error_nn}, it is clear that
which
increases with the magnitude of $\mu_1$
no matter its sign,
and the minimal value, obtained at $\mu_1 = 0$,
corresponds to the single-bin scheme.
%
\note{Note that
we have ignored the residual error
and assumed $\lambda < 2 \, \min \lambda_k = 2 - 8 \, \mu_1$
for all $k$ in the above solution.
%
Fortunately,
the above optimal $\lambda$ % from Eq. \eqref{eq:lambda_nn_perfect}
satisfies this condition
by the stability condition,
$4 \, \mu_1 < 1$.
}%
\note{This is because,
$$
\frac{ 1 - 4 \, \mu_1 } { 1 - 2 \, \mu_1 } < 2 - 8 \, \mu_1
\quad \leftrightarrow \quad
1 < 2 - 4 \, \mu_1.
$$
}%


For the one-step sampling process,
we find from Eq. \eqref{eq:Gamma_onestep}
that $\Gamma(p) = \cot^2 p$
is peaked around $p \approx 0$,
%
Then
$$
  \Err_A
  \propto
  \frac{   2 \, n }
       { \pi \, T }
  \frac{             1             }
       {  \lambda \, (2 - \lambda) }
  .
$$
\note{
$$
\begin{aligned}
  \Err_A
  &
  \propto
  \frac{   2 \, n }
       { \pi \, T }
  \left.
  \frac{            \left(1 - 4 \, \mu_1 \, \sin^2 p \right)^2         }
       { \lambda \, \left(2 - 8 \, \mu_1 \, \sin^2 p - \lambda \right) }
  \right|_{ p \approx 0 }
  %\notag \\
  %&
  \approx
  \frac{   2 \, n }
       { \pi \, T }
  \frac{             1             }
       {  \lambda \, (2 - \lambda) }
  .
\end{aligned}
$$
}%
%
The optimal $\lambda$ is, therefore,
%
\begin{equation}
\lambda \approx 1.
\label{eq:lambda_nn_onestep}
\end{equation}
%
\note{We can be a bit more precise here.
%
From Eq. \eqref{eq:Gamma_onestep}, we get
%
$$
\Gamma(p) = \cot^2 p \approx \cos^2 p / (\sin^2 p + \delta^2),
$$
%
where we have introduced
$\delta \propto \sin[ \pi / (2 \, n) ] \sim 1/n$
as a small parameter to avoid the divergence
around the origin.
%
$$
\begin{aligned}
\Err_A
&=
\frac{   2 \, n }
     { \pi \, t }
\int_0^{\pi/2}
    \frac{            \left(1 - 4 \, \mu_1 \, \sin^2 p \right)^2         }
         { \lambda \, \left(2 - 8 \, \mu_1 \, \sin^2 p - \lambda \right) }
    \frac{ \cos^2 p }
         { \sin^2 p + \delta^2 }
\, dp
\notag \\
&
\stackrel{    \delta \ll 1     }
         { =\joinrel=\joinrel= }
\frac{   2 \, n }
     { \pi \, t }
\frac{             1             }
     {  \lambda \, (2 - \lambda) }
\frac{    1   }
     { \delta }
.
\end{aligned}
$$
The integral is evaluated as follows.
%
We again change variable $p \to \frac{ \pi } { 2 } - p$,
and
$$
\begin{aligned}
\Err_A
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\int_0^{2 \, \pi}
\frac{ \left(1 - 4 \, \mu_1 \, \cos^2 p \right)^2 }
     {       2 - 8 \, \mu_1 \, \cos^2 p - \lambda }
\cdot
\frac{ \sin^2 p            }
     { \cos^2 p + \delta^2 }
\, dp
\\
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\oint
\frac{ \left[1 -      \mu_1 \, \left(z + \frac{1}{z}\right)^2 \right]^2 }
     {       2 - 2 \, \mu_1 \, \left(z + \frac{1}{z}\right)^2 - \lambda }
\cdot
\frac{ 4 - \left( z + \frac 1 z \right)^2 }
     { \left( z + \frac 1 z \right)^2 + 4 \, \delta^2 }
\, \frac{dz}{z}
\\
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\oint
\frac{ \left[z^2 -      \mu_1 \, (z^2 + 1)^2 \right]^2   }
     {  (2 - \lambda) \, z^2 - 2 \, \mu_1 \, (z^2 + 1)^2 }
\cdot
\frac{ 4 \, z^2 - ( z^2 + 1 )^2 }
     { ( z^2 + 1 )^2 + 4 \, \delta^2 \, z^2 }
\, \frac{ dz }{ z^3 }
\\
&=
\frac{ n } { \lambda \, t }
\sum_l \operatorname{Res}_l
,
\end{aligned}
$$
where the contour is the unit circle
in the complex plane of $z$,
and the integral is reduced to a sum of residues
around the poles within.

The poles of the integral come from three sources,
namely,
% 1.
$z = 0$,
% 2.
the zeros of
$(2 - \lambda) \, z^2 - 2 \, \mu_1 \, (z^2 + 1)^2$,
% 3.
and the zeros of
$( z^2 + 1 )^2 + 4 \, \delta^2 \, z^2$.

For the first pole around the origin,
we need to expand the integrand as
$$
\frac{ A + B \, z^2 + \cdots }
     {          z^3          },
$$
then the residue is $B$.
%
Since
$$
\begin{aligned}
\left[ z^2 - \mu_1 (z^2 + 1)^2 \right]^2
&
\approx
[ - \mu_1 + (1 - 2 \, \mu_1) \, z^2]^2
\\
&
\approx
\mu_1^2
\left[
  1 + \left(4 - \tfrac { 2  } { \mu_1 } \right) z^2
\right],
\\
%
4 \, z^2 - ( z^2 + 1 )^2
&
\approx
-( 1 - 2 \, z^2 ),
\\
%
(2 - \lambda) \, z^2 - 2 \, \mu_1 \, (z^2 + 1)^2
&
\approx
-2 \, \mu_1 \,
\left[
  1 + \left(
        2 + \tfrac{ \lambda - 2 } { 2 \, \mu_1 }
      \right)
      \, z^2
\right],
\\
%
( z^2 + 1 )^2 + 4 \, \delta^2 \, z^2
&
\approx
1 + (2 + 4 \, \delta^2) \, z^2,
\end{aligned}
$$
the integrand is
$$
\begin{aligned}
\frac{ \mu_1 }
     {   2   }
\left[
  1 + \left(
        - \frac{ 2 + \lambda } { 2 \, \mu_1 }
        - 2 - 4 \, \delta^2
      \right) \, z^2
\right].
\end{aligned}
$$
and the residue is
$$
\operatorname{Res}_1
=
  - \frac{ 2 + \lambda } { 4 }
  - \mu_1 \, \left( 1 + 2 \, \delta^2 \right).
$$


For the second part,
we have four roots of
$$
(2 - \lambda) \, z^2 = 2 \, \mu_1 \, (z^2 + 1)^2,
$$
which are
\begin{equation}
  z
  =
  \pm
  \sqrt { \frac{ 2 - \lambda }
               { 8 \, \mu_1  } }
  \pm
  \sqrt { \frac{ 2 - \lambda }
               { 8 \, \mu_1  } - 1 }
  .
\tag{N5}
\label{neq:nnint_local_zeros_part2}
\end{equation}
%
We shall distinguish two cases.
\textbf{Case A.}
If $\lambda < 2 - 8 \, \mu_1$,
two of the roots lie in the unit circle, namely
$$
z
=
\pm
\left(
\sqrt { \frac{ 2 - \lambda }
             { 8 \, \mu_1  } }
-
\sqrt { \frac{ 2 - \lambda }
             { 8 \, \mu_1  } - 1 }
\right),
$$
which satisfy
$$
4 \, \mu_1 \, ( z^2 + 1)
=
2 - \lambda
-
\sqrt{ ( 2 - \lambda ) ( 2 - \lambda - 8 \, \mu_1 ) }.
$$
Thus,
$$
\begin{aligned}
\left[ z^2 - \mu_1 \, \left( z^2 + 1 \right)^2 \right]^2
&=
\left( z^2 - \tfrac{ 2 - \lambda } { 2 } z^2 \right)^2
=
\frac{ \lambda^2 } { 4 } z^4.
\\
%
4 z^2 - \left( z^2 + 1 \right)^2
&=
-\frac{ 2 - \lambda - 8 \, \mu_1  } { 2 \, \mu_1 } z^2,
\\
%
\left( z^2 + 1 \right)^2 + 4 \, \delta^2 \, z^2
&=
\frac{ 2 - \lambda + 8 \, \mu_1 \, \delta^2 }
     { 2 \, \mu_1 }
     z^2,
\\
%
(2 - \lambda) \, 2 \, z
-
8 \, \mu_1 \, \left( z^2 + 1 \right) \, z
&=
2 \, z \, \left[2 - \lambda - 4 \,  \mu_1 \, (z^2 + 1) \right]
\\
&= 2 \, z \, \sqrt{ (2 - \lambda) ( 2 - \lambda - 8 \, \mu_1 ) },
\end{aligned}
$$
where we have used the L'H\^{o}pital's rule
and taken the derivative for the last factor.
%
Note that the two roots within the unit share the same
residue and the sum for this part is
%
$$
\operatorname{Res}_2
=
-\frac{                 \lambda^2                     }
      { 4 \, ( 2 + 8 \, \mu_1 \, \delta^2 - \lambda ) }
\sqrt{ 1 - \frac{ 8 \, \mu_1 } { 2 - \lambda } }.
$$
%
\textbf{Case B.}
If $2 - 8 \, \mu_1 \le \lambda < 2$,
the four zeros in Eq. \eqref{neq:nnint_local_zeros_part2}:
%
\begin{equation}
  z
  =
  \pm
  \sqrt { \frac{ 2 - \lambda }
               { 8 \, \mu_1  } }
  \pm i \,
  \sqrt { 1 - \frac{ 2 - \lambda }
                   { 8 \, \mu_1  } }
  ,
\notag
%\label{eq:nnint_local_zeros_part2b}
\end{equation}
%
all satisfy $|z| = 1$, and hence
all lie on the border of the unit circle.
%
Thus, each contributes half of the residue there.
%
However, since the four zeros satisfy
$$
2 - \lambda - 4 \, \mu_1 \, ( z^2 + 1)
=
\pm i
\sqrt{ ( 2 - \lambda ) ( 8 \, \mu_1 - 2 + \lambda ) },
$$
the sum of the four residues gives zero in this case:
$$
\operatorname{Res}_2 = 0.
$$
In summary,
$$
\operatorname{Res}_2
=
\begin{dcases}
-\frac{                 \lambda^2                     }
      { 4 \, ( 2 + 8 \, \mu_1 \, \delta^2 - \lambda ) }
\sqrt{ 1 - \frac{ 8 \, \mu_1 } { 2 - \lambda } }
&
\mathrm{if \;}
\lambda < 2 - 8 \, \mu_1,
\\
0
&
\mathrm{otherwise}.
\end{dcases}
$$


For the last part,
we have four roots of
$(z^2 + 1)^2 = - 4 \, \delta^2 \, z^2$,
with two of them lying in the unit circle:
$$
z = \pm i \left( \sqrt{ 1 + \delta^2 } - \delta \right),
$$
satisfying
$$
z^2 + 1 + 2 \, \delta^2 = 2 \, \delta \, \sqrt{1 + \delta^2}.
$$
Then,
$$
\begin{aligned}
z^2 - \mu_1 \, \left( z^2 + 1 \right)^2
&= z^2 \, (1 + 4 \, \mu_1 \, \delta^2),
\\
4 \, z^2 - \left( z^2 + 1 \right)^2
&=
4 \, z^2 \, (1 + \delta^2)
\\
%
(2 - \lambda) \, z^2
- 2 \, \mu_1 \, \left( z^2 + 1 \right)^2
&=
\left(
  2 - \lambda + 8 \, \mu_1 \, \delta^2
\right) \, z^2
\\
%
4 \, z \, \left( z^2 + 1 \right)
+
8 \, \delta^2 \, z
&=
4 \, z \, (z^2 + 1 + \delta^2)
=
8 \, z \, \delta \, \sqrt{ 1 + \delta^2 }.
\end{aligned}
$$
Note that the above two roots share the same
residue and the sum for this part is
$$
\operatorname{Res}_3
=
\frac{ \left( 1 + 4 \, \mu_1 \, \delta^2 \right)^2 }
     {        2 + 8 \, \mu_1 \, \delta^2 - \lambda }
\frac{ \sqrt{ 1 + \delta^2 } }
     {        \delta         }.
$$

Now as $\delta \to 0$, the last contribution will dominate,
and we have
$$
\Err_A
\approx
\frac{ n } { T }
\frac{ (1 + 4 \, \mu_1 \, \delta^2)^2 }
{ \lambda \, (2 + 8 \, \mu_1 \, \delta^2 - \lambda) }
\frac{ 1 } { \delta }
\approx
\frac{ n } { T }
\frac{ 1 }
{ \lambda \, (2 - \lambda) }
\frac{ 1 } { \delta },
$$
and the minimum is reached at $\lambda \approx 1$.

The weakness of the above calculation is two fold.
%
First, it ignores the residual error.
%
Second, the simplified expression for the asymptotic error
$E_A$ may also be misused.
%
Namely, with $\lambda \approx 1$,
we cannot assume $\lambda < 2 \, \lambda_k$ in general.
%
So it only works for a very small value of $\mu_1$.
}



However, Eq. \eqref{eq:lambda_nn_onestep}
is an approximation good only
for sufficiently short simulations,
since we have taken the limit of $n \to \infty$
for a fixed $T$.
%
For very long simulations,
the optimal $\lambda$
will ultimately drift to
the smallest eigenvalue,
$\min \lambda_k = 1 - 4 \, \mu_1$.
%as discussed in Sec. \ref{sec:optlambda}.





\section{\label{sec:upperbound}
Asymptotic error for an updating scheme
with a set of well-separated eigenvalues}


We will derive here an upper bound of the asymptotic error
that can be associated to updating schemes
with a set of well-separated eigenvalues,
in parallel to the lower bound reached by
the single-bin scheme with a set of identical eigenvalues
(cf. Sec. \ref{sec:optWL}).
%
For example, the smaller eigenvalues of the Gaussian updating scheme
are well separated because of the rapid exponential decay.


Starting from the definition Eq. \eqref{eq:mass_func}, we have
$$
  M(Q)
  \le
  \sum_{k = 2}^n
  \sqrt{ \Gamma_k } \, \lambda_k \, e^{ -\lambda_k \, Q }
  .
$$
Thus,
it follows from Eq. \eqref{eq:mint} that, as $q(T) \to \infty$,
%
\begin{equation}
  C
  =
  \int_0^\infty
  M(q') \, dq'
  \le
  \sum_{k = 2}^n \sqrt{ \Gamma_k }
  ,
  \label{eq:mint_ubound}
\end{equation}
%
and the asymptotic error from Eq. \eqref{eq:error_asym2}
is bounded as
\begin{equation}
  \Err_A(T)
  =
  \frac{ C^2 } { T }
  \le
  \frac 1 T
  \left(
    \sum_{k=2}^n \sqrt{ \Gamma_k }
  \right)^2
  .
  \label{eq:error_asym_ubound}
\end{equation}
%
It is readily seen that this bound is always
greater than the lower bound given by Eq. \eqref{eq:error_asym_singlebin},
%
$$
\frac{T}{(T+t_0)^2} \, \sum_{k=2}^n \Gamma_k
\le
\frac{1}{T} \sum_{k=2}^n \Gamma_k
\le
\frac{1}{T} \left( \sum_{k=2}^n \sqrt{ \Gamma_k } \right)^2
.
$$



Next, we will show that this bound represents
updating schemes with a set of very different eigenvalues.
%
%First, observe that the summand of Eq. \eqref{eq:mass_func},
%$\Gamma_k \, \left( \lambda_k e^{-\lambda_k \, Q} \right)^2$
%is a function that peaks at $\lambda_k = 1/Q$,
%so that the sum is dominated by the term at $k = k(Q)$ with
%%
%\begin{equation}
%  \lambda_{k(Q)}
%  \approx
%  \frac 1 Q
%  .
%  \label{eq:kQ_def}
%\end{equation}
%
%Thus,
First, we have for any mode $k^*$ $(k^* \ge 2)$,
%
\begin{align}
  M(Q)
  &
  \ge
  %\approx
  \sqrt{ \Gamma_{ k^* } } \,
  \lambda_{ k^* } \,
  e^{ -\lambda_{ k^* } \, Q }
  ,
\label{eq:MQ_lbound1}
\end{align}
%
which would approach an equality if the $k^*$ mode
dominates the mass function, $M(Q)$,
as defined in Eq. \eqref{eq:mass_func}.
%
Then, for the $k$th component of the asymptotic error,
we have from Eq. \eqref{eq:error_asym_Lagrangian}
%
\begin{align}
  \left\langle
  y_k^2(T)
  \right\rangle_A
  &=
  \int_0^T
  \Gamma_k \, \lambda_k^2 \, e^{2 \, \lambda_k \, [q(t) - q(T)] }
  \, {\dot q}^2(t) \, dt
  \notag
  \\
  &=
  \frac{ C } { T }
  \, \Gamma_k \, \lambda_k^2
  \int_0^{q(T)}
  \frac{ e^{-2 \, \lambda_k \, Q } } { M(Q) }
  \,  \, dQ
  \notag
  \\
  &\le
  \frac C T \,
  \frac{ \Gamma_k \, \lambda_k^2 } { \sqrt{ \Gamma_{k^*} } \, \lambda_{k^*} }
  \int_0^{q(T)}
    e^{-(2 \, \lambda_k - \lambda_{k^*} ) \, Q}
    \, dQ
  \notag
  \\
  &\le
  \frac C T \,
  \cdot
  \frac{ \Gamma_k }
  { \sqrt{ \Gamma_{k^*} } }
  \cdot
  \frac{ \lambda_k^2 }
  { \lambda_{k^*} \, (2 \, \lambda_k -\lambda_{k^*}) }
  ,
  \label{eq:y2_ubound_k}
\end{align}
%
where we have
used Eqs. \eqref{eq:Lagrangian_const} and \eqref{eq:MQ_lbound1}
on the second and third lines, respectively,
and extended the upper limit to infinity on the last line.
%
For a set of well-separated eigenvalues,
$\lambda_k$ depends on the mode index, $k$,
much more critically than $\Gamma_k$.
%
Thus, the tightest upper bound
can be approximately obtained from minimizing the last factor
on the right-hand side of Eq. \eqref{eq:y2_ubound_k},
and the minimal is reached at $k^* = k$,
when $\lambda_{k^*} = \lambda_k$.
%
So
%
\begin{equation}
  \left\langle
  y_k^2(T)
  \right\rangle_A
  \le
  %\approx
  \frac C T
  \sqrt{ \Gamma_k }
  .
  \label{eq:y2_ubound}
\end{equation}
%
By summing over $k$
and using Eq. \eqref{eq:mint_ubound},
we recover Eq. \eqref{eq:error_asym_ubound}.

This result shows that with a set of well-separated eigenvalues,
the asymptotic error profile would be
independent of $\lambda_k$'s and
be flatter than the initial one from
Eq. \eqref{eq:y2_eql}
(cf. Fig. \ref{fig:xerr}
for a Gaussian example).
%
Note, however, that to approach this upper bound
the simulation has to be long enough
such that the upper limit of the integral
can be extended from $q(T)$ to infinity
in the last step of Eq. \eqref{eq:y2_ubound_k}.
%
As a result,
the flatter profile from Eq. \eqref{eq:y2_ubound}
would stop at mode $k$,
at which $\lambda_k \approx 1/q(T)$.
%In other words,
%the above asymptotic limit fails around
%$Q^* \approx q(T)$, or equivalently
%around mode $k^* = k\bigl( q(T) \bigr)$,
%where $\lambda_{ k^* } \approx 1/q(T)$.
%%
%\note{This case also exemplifies
%  the sensitivity to the simulation length.
%  %
%  In this limit,
%  $M(Q) \propto 1/Q$ by Eq. \eqref{eq:MQ_lbound},
%  and $C \propto \ln\bigl[ q(T) / q_0 \bigr]$
%  for some $q_0$,
%  and we expect that with fixed $a_0$,
%  $q(T)$ to increase almost linearly with $T$
%  by Eq. \eqref{eq:opt_qT}.
%  %
%  So the normalized asymptotic error,
%  $E_A \cdot T \propto \bigl( \ln[ q(T)/q_0 ] \bigr)^2 \sim (\ln T)^2$,
%  is sensitive to the simulation length,
%  and worse than the ideal case of a constant.
%}

\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig1/xerr.pdf}
  }
  \caption{
    \label{fig:xerr}
    Predicted error components for the Gaussian updating scheme
    with $\sigma = 10$
    from the optimal schedule
    and the inverse-time schedule, Eq. \eqref{eq:alpha_invt1}.
    %
    The latter schedule failed to damp out the errors
    from the long-wavelength (local) eigenmodes
    with $k \ge 5$.
    %
    For longer simulations,
    the error components from the optimal schedule
    would roughly maintain a relatively flat profile,
    but meet the initial error curve
    at a lower level (not shown),
    where $\lambda_k \approx 1/q(T)$
    (cf. Appendix \ref{sec:upperbound}).
    %
    \note{This figure was produced by \texttt{doc/fig1/xerr.gp}.
      The data were saved under the directory \texttt{data/xerr}.
    }%
  }
\end{center}
\end{figure}




\section{\label{sec:results}
Numerical results}



We tested the above theory on a model system
described in Sec. \ref{sec:results_system}.
%
We verify the method of computing errors
%using an inverse-time schedule
in Sec. \ref{sec:results_invt}
%
and study the optimal schedule from
Eq. \eqref{eq:q_opt}
in Sec. \ref{sec:results_optschedule}.
%
Finally, we compare the Gaussian and
bandpass updating schemes %of different widths
in Sec. \ref{sec:results_cmpschemes}.



\subsection{\label{sec:results_system}
Test system and simulation protocol}



Our test system is a one-dimensional model system, in which
$i$ represents a non-periodic variable,
and $n = 100$.
%
Both the target and intrinsic distributions
are assumed to be flat:
$p^*_i = p_i = 1/n$,
along with the distribution, $\rho_i$, in Eqs.
\eqref{eq:vav_def} and \eqref{eq:error_def}.
%
%We wish to see how
%the schedule, $\alpha(t)$,
%affects the fluctuating error
%of the bias potential
%at the end of the simulation.
%
Initially,
we set the bias potential to zero,
$u_i \equiv 0$,
and equilibrate the system for $10^7$ steps
under a constant updating magnitude,
$a_0 = 10^{-4}$.
%
This equilibration process models
a stage of the WL algorithm or
a regular metadynamics
with a fixed updating magnitude.
%
Although the bias potential is correct initially,
it would deviate from this value
at the end of the equilibration.
%
Our aim is to see how well
the bias potential is restored
at the end of the testing schedule.
%
After the equilibration,
we reset the origin of the time, $t$, to zero,
and start the testing schedule
for $T = 10^8$ steps.
%
We repeat the above process $1000$ times (unless specified otherwise)
and report the averaged results.
%specified by either
%Eq. \eqref{eq:q_opt}
%or
%Eq. \eqref{eq:alpha_invtlambda}.



For the Gaussian updating scheme,
the kernel was truncated at
$b = \min\{10 \, \sigma, n - 1\}$ bins
and numerically stabilized
using the technique
in Appendix \ref{sec:stabilize_wband}.



%Since the error depends on
%the underlying sampling process,
We tested two sampling processes.
%a global one and a local one.
%
Both are based on
a Metropolis MC algorithm\cite{
  metropolis1953, newman, frenkel,
  landau_binder}:
%
in each step, a new bin index $j$ is proposed
and then accepted with probability
%
$
A(i \to j) = \min\{ 1, \exp(u_i - u_j) \}.
$
However,
in the first or global sampling process,
we choose the destination $j$
uniformly out of the $n$ bins.
%
In the second or local sampling process,
$j$ is either $i - 1$ or $i + 1$
(adjusted by the boundary condition)
with equal probability $1/2$.
%
Asymptotically,
the above global and local sampling processes
emulate the perfect and one-step processes
discussed in Appendix \ref{sec:Gamma},
respectively,
and we will use
Eqs. \eqref{eq:Gamma_perfect}
and \eqref{eq:Gamma_onestep}
for the respective $\Gamma_k$'s
in the theoretical predictions.
%



\subsection{\label{sec:results_invt}
Scaled inverse-time schedule}


Using the model system above, we
tested our analysis of computing errors
on the scaled inverse-time schedule,
%
\begin{equation}
\alpha(t) = \frac{1}{\lambda \, (t + t_0) },
\label{eq:alpha_invtlambda}
\end{equation}
%
where $\lambda$ is a free parameter,
and $t_0$ is given by Eq. \eqref{eq:t0_sinc}.
%and $t_0 = 2/a_0$.
%
The error of this schedule
can be computed analytically
(cf. Appendix \ref{sec:sinvt_schedule}),
and tested against simulation.



We performed adaptive FDS simulations
using the single-bin updating scheme.
%and the triple-bin updating scheme with $\mu_1 = 0.24$.
%
%For each value of $\lambda$,
%we ran $1000$ independent simulations
%of $10^8$ steps, and averaged the results.
%
As shown in Fig. \ref{fig:errinvt},
the errors from the simulations
agreed well with those from the theory.


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig1/errinvt.pdf}
  }
  \caption{
    \label{fig:errinvt}
    Error, $\Err$, versus the parameter, $\lambda$,
    for the single-bin updating scheme
    using the scaled inverse-time schedule,
    Eq. \eqref{eq:alpha_invtlambda}.
    %
    The points are results from averaging over $1000$ independent runs;
    the curves are theoretical predictions.
    %
    \note{The figure was produced by \texttt{doc/fig1/errinvt.gp}.
      The data were produced by the script \texttt{data/invtcscan.py},
      and saved under \texttt{data/invt}
      (see the \texttt{Makefile} there).
    }%
  }
\end{center}
\end{figure}

For the single-bin updating scheme,
the optimal $\lambda$ occurred around $1.0$
for both global and local sampling processes.
%
The sampling process affected the error function
only by a multiplicative constant, $\Gamma$
(cf. Appendix \ref{sec:sinvt_schedule}).
%
%For the triple-bin updating scheme,
%the optimal $\lambda$ was less than $1.0$
%for the global sampling process,
%although it stayed around $1.0$
%for the local sampling process,
%also in agreement with the discussion
%in Appendix \ref{sec:invt_nn}.
%%
%Thus, the inverse-time schedule,
%Eq. \eqref{eq:alpha_invt1},
%is optimal for the single-bin updating scheme,
%but not necessarily so for a general multiple-bin updating scheme.
%%




\subsection{\label{sec:results_optschedule}
Optimal schedule}



To demonstrate the optimal schedule from Eq. \eqref{eq:q_opt},
we used the Gaussian (with $\sigma = 10$ bins) updating scheme
as an example.
%
This updating scheme is typically used in metadynamics simulations.
%
In Table \ref{tab:error_Gaussian},
we showed that the optimal schedule
produced smaller errors than
the inverse-time schedules.
%
However, the optimal schedules
decayed more slowly than
the inverse-time formula, Eq. \eqref{eq:alpha_invt1},
and depended on the sampling process,
as shown in Fig. \ref{fig:optacmp}.
%
The impeded decay of the optimal schedule
was necessary in order to
damp out the short-wavelength eigenmodes
(i.e. local noise).
%
%As shown in Fig. \ref{fig:xerr},
%the inverse-time schedule was effective in error reduction
%only for the first few eigenmodes,
%whose eigenvalues were around $1.0$.
%%
%We can, however, ignore these local eigenmodes
%by explicitly filtering them out from the final bias potential,
%and truncating the error sum in Eq. \eqref{eq:error_def}
%to the first few eigenmodes.
%%
%Then, the corresponding optimal schedule
%from Eq. \eqref{eq:q_opt} that minimizes the truncated error
%became similar to the inverse-time schedule,
%Eq. \eqref{eq:alpha_invt1},
%as exemplified in Fig. \ref{fig:optacmp}.
%%



\begin{table}[h]\footnotesize
  \caption{\label{tab:error_Gaussian}
    Error of the Gaussian updating scheme of $\sigma = 10$
    under different updating schedules.
    In Eqs. \eqref{eq:alpha_invt1} and \eqref{eq:alpha_invtlambda},
    $t_0$ is given by Eq. \eqref{eq:t0_sinc};
    and in Eq. \eqref{eq:alpha_invtlambda},
    the optimal value of $\lambda$ is used.
    \note{\newline
      The data were collected in
      \texttt{data/tab1} by running
      the program \texttt{invt}.
    }%
  }
  \setlength{\tabcolsep}{2pt}
  \renewcommand\arraystretch{1.2}
  \begin{tabular} { l | p{2.2cm} p{2.4cm} p{2.0cm} }
    \hline
      Schedule
    &
      $1/t$, \newline
      Eq. \eqref{eq:alpha_invt1}
    &
      Scaled $1/t$, \newline
      Eq.~\eqref{eq:alpha_invtlambda}
    &
      Optimal, \newline
      Eq. \eqref{eq:q_opt}
    \\
    \hline
    Global
    &
    $4.36(9) \times 10^{-6}$
    &
    $7.1(1) \times 10^{-7}$
    &
    $2.6(1) \times 10^{-7}$
    \\
    \hline
    Local
    &
    $3.83(7) \times 10^{-4}$
    &
    $1.72(4) \times 10^{-4}$
    &
    $9.1(2) \times 10^{-5}$
    \\
    \hline
  \end{tabular}
\end{table}


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig1/optacmp.pdf}
  }
  \caption{
    \label{fig:optacmp}
    Optimal schedules from Eq. \eqref{eq:q_opt}
    for the Gaussian updating scheme
    with $\sigma = 10$.
    %
    The mode-limited schedules were produced by
    limiting the sum in Eq. \eqref{eq:q_opt}
    to the first four terms,
    or the four longest wavelength modes.
    %
    They became similar to the inverse-time schedule,
    Eq. \eqref{eq:alpha_invt1}.
    %
    \note{This figure was produced by \texttt{doc/fig1/optacmp.gp}.
      The data were computed by the program \texttt{prog/predict},
      see particularly the function \texttt{intq\_save()}
      in \texttt{prog/intq.h}.
      The data were saved under the directory \texttt{data/opta}.
    }%
  }
\end{center}
\end{figure}


An interesting prediction from the theory is Eq. \eqref{eq:half_alpha0},
which states that the optimal schedule should always start from
half of the magnitude from the previous equilibration run.
%
To verify this result,
we computed the optimal schedules
for different values of $q(T)$
for the Gaussian updating scheme,
and plotted the final error
against the initial updating magnitude, $\alpha(0)$.
%
As shown in Fig. \ref{fig:erriascan},
the minimal error was indeed reached at
$\alpha(0) = a_0/2 = 5 \times 10^{-5}$,
although the dependence on $\alpha(0)$
is generally weak.\footnote{The weak dependence is best illustrated in
the case of single-bin updating scheme,
as the initial updating magnitude only
affects the schedule by a shift of the origin of
simulation time in the inverse-time schedule,
Eq. \eqref{eq:alpha_invt1}.}


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig1/erriascan.pdf}
  }
  \caption{
    \label{fig:erriascan}
    Relative error
    versus the initial updating magnitude, $\alpha(0)$,
    for optimal schedules with different $q(T)$.
    Assuming the Gaussian updating scheme
    with $\sigma = 10$,
    and $a_0 = 10^{-4}$.
    %
    The points were obtained from averaging results
    from over 10000 independent simulations;
    the curves are theoretical predictions.
    %
    The minimal should always occur at
    $\alpha(0) = a_0/2 = 5 \times 10^{-5}$,
    as predicted by Eq. \eqref{eq:half_alpha0}.
    %
    \note{This figure was produced by
      \texttt{doc/fig1/erriascan.gp}.
      The data were saved under
      \texttt{data/iascan}.
    }%
  }
\end{center}
\end{figure}






\subsection{\label{sec:results_cmpschemes}
Optimal updating schemes}



For a finite-length simulation,
the optimal updating scheme
depends on the simulation length, $T$,
and other parameters.
%
As an example,
we compare Gaussian updating schemes
with different widths, $\sigma$.
%
In Fig. \ref{fig:errsigscan},
we present the errors, $\Err(T)$,
normalized by the shifted simulation length, $T+t_0$,
so that they could
stay constant in the limit of single-bin updating scheme
($\sigma \to 0$).
%
As shown in Fig. \ref{fig:errsigscan}(a)
that the minimal error was achieved
by the widest window %for the shorter simulation of $T = 10^8$,
for the global sampling process,
%
although
the minimum at $\sigma = 0$ became more favorable
as the simulation lengthens.
%
However,
for the local sampling process,
the minimum stayed at zero width,
as shown in Fig. \ref{fig:errsigscan}(b).


In contrast,
the normalized errors of the bandpass updating schemes
were insensitive to the simulation length,
in agreement with Eq. \eqref{eq:error_sinc}.
%
The bandpass updating schemes
yielded smaller errors than
the corresponding Gaussian ones,
although the differences tended to
be small for shorter simulations
with the local sampling process.



\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig1/errsigscan.pdf}
  }
  \caption{
    \label{fig:errsigscan}
    Normalized error, $(T + t_0) \, \Err(T)$,
    versus the width, $\sigma$,
    for the Gaussian
    and the bandpass updating schemes,
    with $t_0$ given by Eq. \eqref{eq:t0_sinc}.
    %
    In the latter case,
    the equivalent width
    $\sigma$ is given by Eq. \eqref{eq:sigma_equiv}.
    %
    The points were obtained from averaging results
    from 1000 independent simulations;
    the curves are theoretical predictions.
    %
    \note{This figure was produced by
      \texttt{doc/fig1/errsigscan.gp}.
      The data were saved under
      \texttt{data/scan}.
    }%
  }
\end{center}
\end{figure}



As an example, we used the method to compute
the integrals, $\Gamma_k$'s, from
a local MC simulation
(cf. Sec. \ref{sec:results_system})
and an MD simulation,
and compared them with those from
the one-step sampling process.
%
As shown in Fig. \ref{fig:gamcmp},
the $\Gamma_k$'s of the small-$k$ eigenmodes
from the three sampling processes
were almost proportional,
suggesting that
the optimal schedule
for simulations using local sampling processes
would be similar in general properties.



\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig1/gamcmp.pdf}
  }
  \caption{
    \label{fig:gamcmp}
    Integrals of the autocorrelation functions
    from a local MC simulation
    and an MD simulation.
    %
    Each simulation was ran for $10^{10}$ steps.
    %
    The bin size $\Delta z = 2\, \pi /n$ with $n = 100$.
    %
    For the MD simulation,
    the step size $\Delta t_\mathrm{MD} = 0.005$.
    %
    The velocity rescaling thermostat\cite{bussi2007} was
    applied for the unit reduced temperature
    every MD step with viscosity $20$.
    %
    The reflective boundary condition was used.
    %
    \note{The figure was produced by \texttt{doc/fig1/gamcmp.gp}.
      The data were saved under \texttt{data/gamcmp}
      (see the \texttt{Makefile} there).
    }%
  }
\end{center}
\end{figure}



\section{\label{sec:wltoinvt}
Inverse-time schedule as the optimal limit of the WL algorithm}


We may understand the inverse-time schedule %, Eq. \eqref{eq:},
as an optimal limit of the stage-wise WL updating scheme.
%
%Our calculation below will focus on the one-variable problem,
%as in Sec. \ref{sec:onevar}.

To begin, we split the entire simulation into $S$ stages,
each with a fixed magnitude, $\alpha_s$ ($1 \le s \le S$).
%
We denote the number of steps in stage $s$ as $T_s$, and
$\sum_{s=1}^S T_s = T$.
%
%\begin{equation}
%  \sum_{s = 1}^S T_s = T
%  ,
%  \label{eq:sumT}
%\end{equation}
%
We assume that
the magnitude follows geometrically,
%
\begin{equation}
  \alpha_s = \alpha_0 \, r^s
  ,
  \label{eq:alphas}
\end{equation}
%
with $0 < r < 1$.
%
%and that the system is properly equilibrated
%under constant magnitude initially.
%
We wish to find the best distribution of the simulation time
into the $S$ stages
as well as
the optimal number of stages and $r$.


If we denote the error at the end of stage $s$ as $E_s$,
then $q\left( \tau^{(s)} \right) = \alpha_s \, \tau^{(s)}$,
where $\tau^{(s)}$ denotes the number of steps in stage $s$,
and from Eq. \eqref{eq:ET_average}, we have
%
\begin{align}
  E_s
  &=
  E_{s - 1} \, e^{-2 \, \alpha_s \, T_s }
  +
  \Gamma \, \int_0^{T_s}
  \alpha_s^2 \,
  e^{ -2 \, \alpha_s \, \left(T_s - \tau^{(s)}\right) }
  \, d \tau^{(s)}
  \notag
  \\
  &=
  \left(
    E_{s - 1} - \frac{ \Gamma \, \alpha_s } { 2 }
  \right)
  e^{ - 2 \, \alpha_s \, T_s }
  +
  \frac{ \Gamma \, \alpha_s } { 2 }
  .
  \label{eq:errorstage}
\end{align}
%
Our aim is to minimize the error
of the final stage, $E_S$.
%
To do so, let us consider shifting simulation time
by $\delta T$ from
stage $s+1$ to stage $s$,
then $E_{s+1}$ is affected as
%
\begin{align*}
  \delta E_{s+1}
  =
\left[
  2 \, \alpha_{s+1}
  \left(
    E_s - \tfrac{ \Gamma \, \alpha_{s+1} } { 2 }
  \right)
  +
  \frac{ \partial E_s } { \partial T_s }
\right]
e^{-2 \, \alpha_{s+1} \, T_{s+1} }
\,
\delta T
.
\end{align*}
%
Since this variation does not directly affect the error of later stages,
the above expression must be zero to minimize the final error, i.e,
%
\begin{align*}
  2 \, \alpha_{s+1}
  \left(
    E_s - \frac{ \Gamma \, \alpha_{s+1} } { 2 }
  \right)
  &=
  -\frac{ \partial E_s } { \partial T_s }
  %\\
  %&=
  %2 \, \alpha_s \, e^{-2 \, \alpha_s \, T_s}
  %\left(
  %  E_{s-1} - \frac{ \Gamma \, \alpha_s } { 2 }
  %\right)
  %\\
  =
  2 \, \alpha_s \, \left(E_s - \frac{ \Gamma \, \alpha_s } { 2 }  \right)
  ,
\end{align*}
%
where we have used Eq. \eqref{eq:errorstage}
in the second step.
%
Thus,
\begin{equation}
  E_s
  =
  \frac{ \Gamma } { 2 } \, ( \alpha_s + \alpha_{s+1} )
  =
  \frac{ \Gamma } { 2 } \, \alpha_s ( 1 + r )
  .
  \label{eq:Es}
\end{equation}
%
Using Eqs. \eqref{eq:Es} and \eqref{eq:alphas}
in Eq. \eqref{eq:errorstage}, we get
%
\begin{equation}
  e^{-\alpha_s \, T_s } = r
  .
  \label{eq:exp_r}
\end{equation}
%

To compute the optimal schedule,
we map the overall simulation time, $t$,
to the stage number, $s$, as
%
\begin{align*}
  t
  = \sum_{s' = 1}^s T_{s'}
  = \frac{ 1 - r^s } { 1 - r } \, T_s
  = \frac{ 1 } { \gamma } \,
  \left(
    \frac{1}{\alpha_s}
    -
    \frac{1}{\alpha_0}
  \right)
  ,
\end{align*}
where
$\gamma = ( 1 - r ) / \ln(1/r)$,
and we have used Eq. \eqref{eq:exp_r} in the last step.
%
Thus,
%
\begin{equation}
  \alpha_s =
  \frac{ 1 }
  { \gamma \, t + \alpha_0^{-1} }
  ,
  \label{eq:alphas_t}
\end{equation}
%
and the final error is given by
%
\begin{align}
  E_S
  =
  \frac{\Gamma}{2}
  \frac{ 1 + r }
  { \gamma \, T + \alpha_0^{-1} }
  ,
  \notag
\end{align}
%
which is minimized as $r \to 1$
for a large $T$
(at this point, $\gamma \to 1$).
%
\note{
  For a large $T$, $E_S \approx \frac{\Gamma}{2T} \frac{1+r}{\gamma}$.
  Define $\delta = 1 - r$,
  we have $1/\gamma = 1+ \delta/2 + \delta^2/3 + \cdots$,
  and
  $$
  \frac{1+r}{\gamma}
  =
  (2 - \delta)
  \left(
  1 + \frac{\delta}{2} + \frac{\delta^2}{3} + \cdots
  \right)
  =
  2 + \sum_{n=1}^\infty \frac{n-1}{n(n+1)}\delta^n \ge 2
  .
  $$
}
%
If we identify the parameter, $\alpha_0$,
as the initial updating magnitude, $\alpha(t = 0)$,
then this result agrees with Eq. \eqref{eq:t0_sinc},
with $t_0 = \alpha_0^{-1}$,
and the schedule given by Eq. \eqref{eq:alphas_t}
is identical to that from Eq. \eqref{eq:alpha_invt1}.



\subsection{\label{sec:stabilize_wband}
Stabilization}



%A practical problem of using homogeneous updating scheme
%is the following.
%
A homogeneous updating scheme
specified by an arbitrary kernel
is not necessarily stable,
i.e. some eigenvalues can be negative.
%
Here we show how to minimally modify
the updating kernel
to stabilize it.
%
\note{The relative code is \texttt{trimwindow()} in \texttt{invt.h}.
}


Given an updating kernel,
we can compute the eigenvalues from
Eq. \eqref{eq:wband_eigenvalue}.
%
By replacing negative eigenvalues with zeros and
using the inversion formula,
Eq. \eqref{eq:mu_from_lambda},
we get a new updating kernel,
which is stable by construction.
%
However, the new kernel is widened
as $\mu_l$ may be nonzero for $l > b$.
%
If we truncate the new kernel at $\mu_b$,
the stability problem remains,
although the negative eigenvalues
tend to be smaller in magnitude.
%
Thus, to stabilize the updating kernel
without increasing the width,
we need to iterate the above process many times,
resulting the following algorithm.


%
\begin{enumerate}
  \item
    Given an updating kernel, $\mu_0, \dots, \mu_b$,
    compute the eigenvalues,
    $\lambda_1, \dots, \lambda_{n-1}$,
    from Eq. \eqref{eq:wband_eigenvalue}.
  \item
    If all eigenvalues are nonnegative within the numerical error,
    we are done. % and exit the loop.
  \item
    Otherwise, replace negative eigenvalues by zeros,
    and compute the new kernel,
    $\mu_0, \dots, \mu_b$, from
    Eq. \eqref{eq:mu_from_lambda},
    with $\mu_l = 0$ for $l > b$.
    Go to step 1.
\end{enumerate}











\section{\label{sec:sinvt_schedule}
Scaled inverse-time schedule}



The error of the scaled inverse-time schedule,
Eq. \eqref{eq:alpha_invtlambda},
can be found by using Eq. \eqref{eq:alpha_invtlambda}
in Eqs. \eqref{eq:error_asym} and \eqref{eq:error_res1}:
%
\begin{align}
  \Err_A(T)
  &=
  \frac{    1    }
       { T + t_0 }
  \sum_{k = 1}^{n-1}
    \frac{ \Gamma_k \, \nu_k^2 }
         {    2 \, \nu_k - 1   }
  \left[
    1 - \left(
          \frac {     t_0 }
                { T + t_0 }
        \right)^{ 2 \, \nu_k - 1 }
  \right]
  ,
  \notag
  %\label{eq:error_asym_invt}
  \\
  \Err_R(T)
  &=
  \frac { a_0 } { 2 }
  \sum_{k = 1}^{n-1}
  \Gamma_k \, \lambda_k \,
  \left(
      \frac{   t_0   }
           { T + t_0 }
  \right)^{ 2 \, \nu_k }
  ,
  \notag
  %\label{eq:error_res_invt}
\end{align}
%
where $\nu_k \equiv \lambda_k / \lambda$.
%
%with
%$
%  q(T)
%  =
%  ( 1 / \lambda )
%  \ln\left(
%    1 + T / t_0
%  \right)
%$
%%
%from integrating Eq. \eqref{eq:alpha_invtlambda}.
%
%We assume that
%the system was initially equilibrated
%at a constant $a_0$,
%such that the values of
%$\left\langle {\tilde v}_{*k}^2(0) \right\rangle$
%can be computed from Eq. \eqref{eq:xt2_eql},
%as well as Eq. \eqref{eq:t0_sinc}.
%
%The total error is
%%
%\begin{align}
%\Err
%&=
%\Err_R + \Err_A
%\notag
%\\
%&=
%\sum_{ k = 1 }^{n-1}
%  \frac
%  {
%    \Gamma_k \, \nu_k \,
%    \left[
%      \nu_k
%      +
%      (\nu_k - 1)
%      \left(
%        \frac{ t_0 } { T + t_0 }
%      \right)^{ 2 \, \nu_k - 1 }
%    \right]
%  }
%  {
%    (T + t_0) \, (2 \, \nu_k - 1)
%  }
%  .
%%\notag
%\label{eq:error_invt}
%\end{align}



In particular,
for the single-bin scheme, we have
$\lambda_1 = \cdots = \lambda_{n-1} = 1$,
and
$\Gamma_k$'s affect the error
only via the sum, $\Gamma = \sum_{k = 1}^{n-1} \Gamma_k$,
as a multiple.
%
In the long time limit,
we have for $\lambda < 2$,
$$
  \Err_A(T)
  =
  \frac { 1 } { T + t_0 }
  \frac {         \Gamma           }
        { \lambda \, (2 - \lambda) }
  ,
$$
which is minimal at $\lambda = 1$.
%
\note{A higher-order correction:
  $
  \lambda = 1 -
  \left[
    1 - \frac 1 2 a_0 \, (T+t_0)
  \right] r \, \log r
  ,
  $
  where $r = t_0 / (T + t_0)$.
}


\section{\label{sec:Gamma}
Integrals of the autocorrelation functions}



Here, we evaluate the integrals of
the autocorrelation functions,
$\Gamma_k$'s,
in two special cases
in Secs. \ref{sec:Gamma_perfect}
and \ref{sec:Gamma_onestep}
in the asymptotic limit,
and describe a general method of measuring them
in Sec. \ref{sec:Gamma_measure}.



\subsection{\label{sec:Gamma_perfect}
Perfect sampling}


First, if the sampling is perfect,
%
%\begin{equation}
%  \Gamma_k = 1.
%\label{eq:Gamma_perfect}
%\end{equation}
%
%To see this, we note that
the bin index, $i$, is an independent random variable
at each time step $t$.
%
Further, in the asymptotic limit,
$\langle h_i(t) \rangle = p_i$,
we find from Eq. \eqref{eq:h_split} that
\begin{equation*}
  \zeta_{*i}(t)
  =
  \frac{ h_i(t) } { p_i }
  -
  \sum_{ r = 1 }^n
    \rho_r \frac{ h_r(t) } { p_r }
  ,
\end{equation*}
%
and
%
\begin{align}
  \left\langle
    \zeta_{*i}(t) \, \zeta_{*j}(t')
  \right\rangle
  %&=
  %\left\langle
  %  \left(
  %    \frac{ h_i(t) } { p_i }
  %    -
  %    \sum_{ r = 1 }^n
  %      \rho_r \frac{ h_r(t) } { p_r }
  %  \right)
  %\right.
  %\notag \\
  %&\hphantom{==}\cdot
  %\left.
  %  \left(
  %    \frac{ h_j(t') } { p_i }
  %    -
  %    \sum_{ s = 1 }^n
  %      \rho_s \frac{ h_s(t') } { p_s }
  %  \right)
  %\right\rangle
  %\notag
  %\\
  &=
  \left(
    \frac{ \delta_{ij} } { p_i }
    -
    \frac{ \rho_i } { p_i }
    -
    \frac{ \rho_j } { p_j }
    +
    \sum_{r = 1}^n
    \frac{ \rho_r^2 } { p_r }
  \right) \,
  \delta_{t, t'}
  .
\label{eq:zz_perfect}
\end{align}
%
%$\delta(t - t')$ is the equivalent of $\delta_{t, t'}$
%in the continuous limit.
%
\note{To show Eq. \eqref{eq:zz_perfect}, we first note that
  with $t \ne t'$ (going back to the discrete time case),
  $$
  \begin{aligned}
  \left\langle
    \zeta_{*i}(t) \, \zeta_{*j}(t')
  \right\rangle
  &=
  \left\langle
    \zeta_{*i}(t)
  \right\rangle
  \left\langle
    \zeta_{*j}(t')
  \right\rangle
  =
  0.
  \end{aligned}
  $$
  Next, for $t = t'$, we have
  $$
  \langle
    h_i(t) \, h_j(t)
  \rangle
  = \delta_{ij} \, p_i
  ,
  $$
  which leads to Eq. \eqref{eq:zz_perfect}.
}
%
Thus,
for $k > 0$, we get
%
\begin{equation}
  \kappa_k(t - t')
  =
  \sum_{i,j = 1}^n
  \phi_{ki} \phi_{kj}
  \langle \zeta_{*i}(t) \, \zeta_{*j}(t') \rangle
  %
  =
  \sum_{i = 1}^n \frac{ \phi_{ki}^2 } { p_i }
  \, \delta_{t, t'},
\notag
%\label{eq:kappa_perfect}
\end{equation}
%
where we have used
Eqs. \eqref{eq:ortho1},
\eqref{eq:zz_perfect}.
%
Thus, the white-noise approximation is exact,
with
$\Gamma_k = \sum_{ i = 1}^n \phi_{ki}^2 / p_i$.
\note{Proof.%Eq. \eqref{eq:kappa_perfect},
  $$
  \begin{aligned}
  \kappa_k(t - t')
  &=
  \sum_{i,j = 1}^n
  \phi_{ki}
  \phi_{kj}
    \langle \zeta_{*i}(t) \, \zeta_{*j}(t') \rangle
  \\
  %
  &=
  \sum_{i,j = 1}^n
  \phi_{ki}
  \phi_{kj}
  \left(
    \frac{ \delta_{ij} } { p_i }
    -
    \frac{ \rho_i } { p_i }
    -
    \frac{ \rho_j } { p_j }
    +
    \sum_{r = 1}^n
    \frac{ \rho_r^2 } { p_r }
  \right) \,
  \, \delta_{t, t'}
  .
  \end{aligned}
  $$
  where,
  we have used
  Eq. \eqref{eq:zz_perfect}
  on the second line.
  %
  With Eq. \eqref{eq:ortho1},
  only the first term in the parentheses survives.
}
In particular, if $p_i = \rho_i$, then, by
Eq. \eqref{eq:eig_orthonormal_rows}, we have
$\kappa_k(t - t') = \delta_{t, t'}$, or
\begin{equation}
  \Gamma_k = 1
  .
\label{eq:Gamma_perfect}
\end{equation}




\subsection{\label{sec:Gamma_onestep}
One-step process}


Another special case is the
one-dimensional one-step process\cite{vankampen},
which models a local sampling process.
In each step, the system hops to
one of the two neighboring bins
with equal probability, $1/2$.
%
The transition matrix is
%
\begin{equation}
  \arraycolsep=3.6pt\def\arraystretch{1.4}
  \mathbf A
  =
  \left(
    \begin{array}{cccccccc}
      \frac \sigma 2 & \frac 1 2 & 0 & \dots & \frac {1 - \sigma} 2 \\
      \frac 1 2 & 0         & \frac 1 2 & \dots & 0 \\
      \vdots & &  & & \vdots \\
      \frac {1 - \sigma} 2 & \dots &  & \frac 1 2 & \frac \sigma 2
    \end{array}
  \right),
\notag
%\label{eq:T_nn}
\end{equation}
%
where $\sigma = 0$ for a periodic variable, or $1$ otherwise.
%
For many updating schemes
(cf. Sec. \ref{sec:band-matrix}),
we can further assume that
the transition matrix $\mathbf A$ and
the updating matrix $\mathbf w$
share the eigenvectors,
with $\rho_i = p_i = 1/n$.
%but not the eigenvalues.
%
The eigenvalues of $\mathbf A$ are
$$
  \Lambda_k
  =
  \cos \frac{ k \, 2 \, \pi }
            { g \, n        }
  ,
$$
where
$g = 1$ for a periodic variable, or $2$ otherwise.
%Note that these $\Lambda_k$'s are
%not to be confused with the eigenvalues of $\mathbf w$,
%$\lambda_k$'s.
%
In this case,
the autocorrelation function, $\kappa_k(t)$,
decays as $\kappa_k(0) \, \Lambda_k^t$,
with $\kappa_k(0) = 1$ being the same as
the value in perfect sampling.
%
Thus,
%$\Gamma_k$ is roughly twice the autocorrelation time, and
from Eq. \eqref{eq:Gamma_sum}, we get
%
\begin{equation}
  \Gamma_k
  =
  1 + 2 \, \sum_{t = 1}^\infty \Lambda_k^t
  =
  \frac{ 1 + \Lambda_k }
       { 1 - \Lambda_k }
  =
  \cot^2 \left[
    \frac{ k \, \pi }
         { g \, n   }
  \right]
  .
\label{eq:Gamma_onestep}
\end{equation}


The
  following method appeared to have some stability issues,
  so we switched back to the double-WL approach,
  which hopefully is stable.
  %
  The reason of instability might be
  that the inter-umbrella fluctuation
  and the intra-umbrella fluctuation are some coupled.
  So the updating magnitude cannot be adjusted individually
  for the each umbrella.
  %
  Following the method for weight determination
  in simulated tempering\cite{park2007,
  *nguyen2013, *zhang2015st},
  we can estimate $Z^{(j)}/Z^{(i)}$
  for a neighboring pair, $(i, j)$, as
  %
  \begin{align*}
    &\ln\frac{ Z^{(j)} }{ Z^{(i)} }
    \approx
    \ln\frac{ \sigma_z^{(j)} }{ \sigma_z^{(i)} }
    +
    \frac{ \Delta z_c } { 2 }
    \left(
      \frac{ c_1^{(i)} } { \sigma_z^{(i) 2} }
      +
      \frac{ c_1^{(j)} } { \sigma_z^{(j) 2} }
    \right)
    +
    \frac{ c_2^{(j)} - c_2^{(i)} } { \sqrt 2 }
    \\
    &-
    \left(
      \frac{ \sqrt 2 \, c_2^{(j)} - 1 }
           { 2 \, \sigma_z^{(j)} }
      -
      \frac{ \sqrt 2 \, c_2^{(i)} - 1 }
           { 2 \, \sigma_z^{(i)} }
    \right)
    \left(
      \frac{ \Delta z_c^{2} } { 6 }
      +
      \frac{ \sigma_z^{(i) 2} + \sigma_z^{(j) 2} } { 2 }
    \right)
    ,
  \end{align*}
  %
  where
  $\Delta z_c \equiv z_c^{(j)} - z_c^{(i)}$.
  For equal width, we have
  \begin{align*}
    \ln\frac{ Z^{(j)} }{ Z^{(i)} }
    \approx
    \frac{ \Delta z_c \, ( c_1^{(i)} + c_1^{(j)} ) } { 2 \, \sigma_z }
    -
    \left(
      \frac{ c_2^{(j)} - c_2^{(i)} }
           { 6 \sqrt 2 }
    \right)
    \left(
      \frac{ \Delta z_c^{2} } { \sigma_z^2 }
    \right)
    .
  \end{align*}

Next we will use a model to show that $\Gamma_k$
assumes a large value for a small $k$,
and then decays rapidly to $1$ as $k$ increases
in a local sampling process.
%
We assume that the propagation along the collective-variable is Markovian
such that it can be approximately
described by a transition matrix, $\mathbf A$, with
\begin{align*}
\langle h_i(t) \rangle = \rho_i,
\quad
\langle h_i(t) \, h_j(0) \rangle = \bigl(\mathbf A^t\bigr)_{ij} \, \rho_j
\quad \mbox{(for $t \ge 0$)}
.
\end{align*}
%
We further assume that
the transition matrix
has a complete set of orthonormal eigenvectors,
and the eigenvectors coincide with
those of the updating matrix, $\phi_{ki}$'s.
Then the transition matrix can be decomposed as
$A_{ij} = \sum_{l=0}^{n-1} \Lambda_l \, \phi_{li} \, \phi_{lj} / \rho_j$,
with $\Lambda_l$ being the eigenvalue of mode $l$,
and
%
$\bigl\langle
  \tilde f_k(t) \, \tilde f_k(0)
\bigr\rangle
=
\Lambda_k^t$.
%If the transition matrix % satisfies detailed balance
%has a complete set of orthonormal eigenvectors, $\psi_{li}$
%%that are orthonormal with respect to $\pmb\rho$
%[cf. Eqs. \eqref{eq:eig_orthonormal_cols} and
%\eqref{eq:eig_orthonormal_rows}],
%it can be decomposed as
%$A_{ij} = \sum_{l=0}^{n-1} \Lambda_l \, \psi_{li} \, \psi_{lj} / \rho_j$,
%with $\Lambda_l$ being the eigenvalue of mode $l$.
%%
%The autocorrelation function can then be computed as
%%
%\begin{align*}
%\bigl\langle
%  \tilde f_k(t) \, \tilde f_k(0)
%\bigr\rangle
%=
%\sum_{l = 0}^{n-1} \Lambda_l^t \,
%\left(
%  \sum_{i=1}^n \frac{ \phi_{ki} \, \psi_{li} }{ \rho_i }
%\right)^2
%.
%\end{align*}
%%
%In particular, if the eigenvectors, $\pmb \phi_k$
%and $\pmb \psi_k$, coincide,
%we would have
%$\bigl\langle
%  \tilde f_k(t) \, \tilde f_k(0)
%\bigr\rangle
%=
%\Lambda_k^t$.
%and
%\begin{equation}
%  \Gamma_k = 1 + 2 \, \sum_{t=1}^\infty
%  \bigl\langle
%    \tilde f_k(t) \, \tilde f_k(0)
%  \bigr\rangle
%  = \frac{1+\Lambda_k}{1-\Lambda_k}
%  .
%  \notag
%  %\label{eq:Gamma_est}
%\end{equation}
%
To model a local sampling process,
we can borrow Eq. \eqref{eq:lambda_Gaussian} for $\Lambda_k$,
with $\sigma_z$ reinterpreted as
the average move size along $z$ during sampling,
then
\begin{align}
  \Gamma_k = 1 + 2 \, \sum_{t=1}^\infty
  \bigl\langle
    \tilde f_k(t) \, \tilde f_k(0)
  \bigr\rangle
  %= \frac{1+\Lambda_k}{1-\Lambda_k}
%\Gamma_k
  = \coth\left(
  \frac{ \pi^2 \, k^2 \, {\sigma_z^\mathrm{(samp)}}^2 } { 4 \, \Delta z^2}
  \right).
  \notag
  %\label{eq:Gamma_local}
\end{align}
Thus, we have $\Gamma_k \propto \Delta z^2/k^2$
for small $k$,\cite{bussi2006}
whereas $\Gamma_k \to 1$ as $k$ increases.
%
However, this model tends to underestimate
the first few $\Gamma_k$'s in a typical simulation.
%which means that doubling the sampling range, $\Delta z$,
%would make the histogram flattening
%at least four times as difficult,
%i.e. FDS methods are most effective
%for short ranges\cite{wang2001, wang2001pre}.
%
%Using finer bins would, however, not change this value.
%

\bibliography{simul}
\end{document}
