\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{hyperref}

\hypersetup{
    colorlinks,
    linkcolor={red!30!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\begin{document}

\title{Statistical Redundancy $1 + 2 \, \tau$}
\author{}
\date{\vspace{-10ex}}
\maketitle


\section{Introduction}


In molecular simulations, we compute trajectory averages.
%
The simulation engine
%running either Monte Carlo (MC) or molecular dynamics (MD),
generates a trajectory of molecular configurations,
and saves the trajectory frames in regular intervals.
%
From the trajectory,
we can compute the average of a quantity of interest, $x$,
%
\begin{equation}
  \bar x = S / n,
  \notag
  %\label{eq:xbar}
\end{equation}
%
and
%
\begin{equation}
  S = x_1 + \cdots + x_n,
  \notag
  %\label{eq:sumx}
\end{equation}
%
where $n$ is the number of saved frames,
and $x_i$ is the value of $x$ from frame $i$.

However, successive trajectory frames are usually correlated.
%
While this correlation does not affect the expected value
of the average, it affects the variance, which represents
the statistical redundancy of the trajectory.

There is, however, a simple formula
which accurately measures the statistical redundancy, $R$,
relative to a perfectly independent trajectory:
%
\begin{equation}
  R = 1 + 2 \, \tau,
  \label{eq:1p2T}
\end{equation}
%
where $\tau$ is the integral autocorrelation time measured
in multiples of frame separations.
%
In this article, we shall show why Eq. \eqref{eq:1p2T} is true.


\section{Proof}


We shall first sketch the proof of Eq. \eqref{eq:1p2T}.
%
For simplicity, we assume that
%
\begin{equation}
  \left\langle x_i \right\rangle = 0,
  \label{eq:ave_xi}
\end{equation}
%
and
%
\begin{equation}
  \left\langle x_i^2 \right\rangle = 1.
  \label{eq:var_xi}
\end{equation}
%
This can always be done by a proper scaling and translation.
%
Our argument has three steps.

\begin{enumerate}
\item
The integral autocorrelation time is given by
%
\begin{equation}
  \tau
  =
  \sum_{k = 1}^\infty c_k
  ,
  \label{eq:act}
\end{equation}
%
where $c_k$
is the normalized autocorrelation function with $c_0 = 1$, and
\begin{equation}
  c_k = \langle x_{i + k} \, x_i \rangle
  .
  \label{eq:ck_def}
\end{equation}

\item
The statistical redundancy is given by
%
\begin{equation}
  R
  = \sum_{k = -\infty}^\infty c_k
  = 1 + 2 \, \sum_{k = 1}^\infty c_k
  ,
  \label{eq:redund}
\end{equation}
%

\item
The desired result, Eq. \eqref{eq:1p2T},
follows from Eqs. \eqref{eq:act} and \eqref{eq:redund}.

\end{enumerate}


\section{Integral autocorrelation time}


We shall prove Eq. \eqref{eq:act} in this section
by building an underlying model for the correlation.
%
In each frame,
$x_{ i + 1 }$
can either inherit the previous value, $x_i$
with probability $c_1$,
or be an independent data point
with probability $c_0 - c_1 = 1 - c_1$.
%
In the former case,
$x_{i + 2}$ may hold on to the value of $x_i$,
with a smaller probability $c_2 \le c_1$,
and so on.
%
Generally,
the probability for $x_i$ to survive in the sequence for precisely
the next $k$ frames is given by
%
\begin{equation}
  p_k = c_k - c_{k + 1}
  .
  \notag
  %\label{eq:pk}
\end{equation}
%
So we can define the integral autocorrelation time as
the average of the life span, $k$,
%
$$
  \tau = \sum_{k = 0}^\infty k \, p_k.
$$

To show Eq. \eqref{eq:act} is true, we note that
$$
\begin{aligned}
\tau
&=
\sum_{ k = 0 }^\infty k \, c_k
- \sum_{ k = 0 }^\infty k \, c_{k + 1}
\\
&=
\sum_{ k = 1 }^\infty k \, c_k
- \sum_{ k = 0 }^\infty (k + 1 - 1) \, c_{k + 1}
\\
&=
\sum_{ k = 1 }^\infty k \, c_k
-
\sum_{ k = 0 }^\infty (k + 1) \, c_{ k + 1 }
+
\sum_{ k = 0 }^\infty c_{k + 1}
\\
&=
\sum_{ k = 1 }^\infty c_k,
\end{aligned}
$$


\section{\label{sec:redund}
Statistical redundancy}


In this section,
we shall prove Eq. \eqref{eq:redund}.
%
We shall first give an intuitive definition
of the statistical redundancy
and then show it is given by the variance.
%
Consider two trajectories generated by
different simulation methods with the same length.
%
The two simulations produce
$\bar x$ and $\bar x'$ for the average,
or
$S$ and $S'$ for the sum, respectively.
%
Since the two values are computed for the same system,
they share the same expectation.
%
But as the simulation methods differ, so are the variances.

We wish to obtain a better estimate of the sum, $\hat S$,
by a linear combination of $S$ and $S'$, i.e.,
\begin{equation}
  \hat S
  =
  c \, S + c' \, S',
  \notag
\end{equation}
%
with normalized coefficients
%
\begin{equation}
  c + c' = 1.
  \label{eq:csum}
\end{equation}
%
We wish to minimize the variance of $\hat S$ under the constraint
Eq. \eqref{eq:csum}.  Then the target function is
$$
\operatorname{var} \hat S - \lambda \, (c + c')
=
c^2 \, \operatorname{var} S
+
c'^2 \, \operatorname{var} S'
-
\lambda \, (c + c'),
$$
with $\lambda$ being a Lagrange multiplier.
%
Differentiating the expression with respect to $c$ and $c'$ yields
$$
2 \, c \, \operatorname{var} S
=
2 \, c' \, \operatorname{var} S'
=
\lambda,
$$
which shows that the coefficients of combination
are inversely proportional to the respective variances.
%
In other words, the variance serves as the discounting factor
for statistical redundancy in an optimal combination.

We can now show Eq. \eqref{eq:redund},
%
$$
\begin{aligned}
\operatorname{var} S
=
\left\langle S^2 \right\rangle
&=
\sum_{i = 1}^n
  \sum_{j = 1}^n
    \left\langle x_i \, x_j \right\rangle
\\
&=
\sum_{i = 1}^n
\left(
\sum_{j = 1}^n
c_{ |j - i| }
\right),
\end{aligned}
$$
%
where we have used Eq. \eqref{eq:ck_def}
in the last step.
%
For a very large $n$,
the inner sum is peaked around $j = i$,
and except for a few $i$ around $1$ or $n$,
we have
$$
\sum_{j = 1}^n c_{ | j - i | }
\approx
\cdots + c_2 + c_1 + 1 + c_1 + c_2 + \cdots,
$$
where the sum extends to infinity in both ends.
%
Thus,
\begin{equation}
  \operatorname{var} S
  =
  n \times ( 1 + 2 \, c_1 + 2 \, c_2 + \cdots ).
  \label{eq:varS}
\end{equation}
If the data points are independent, then $c_1 = c_2 = \cdots = 0$,
and this serves as the reference value
\begin{equation}
  \operatorname{var} S^\mathrm{perfect}
  =
  n
  .
\end{equation}
%
The ratio of the two yields the relative statistical redundancy,
$$
R \equiv
\frac{ \operatorname{var} S }
     { \operatorname{var} S^{ \mathrm{perfect} } }
=
1 + 2 \, c_1 + 2 \, c_2 + \cdots
,
$$
which is Eq. \eqref{eq:redund}.


\end{document}
