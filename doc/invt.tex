\documentclass[reprint]{revtex4-1}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}



\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{64,0,42}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}
\newcommand{\answer}[1]{{\color{DarkBlue}\footnotesize \textsc{Answer.} #1}}
\newcommand{\summary}[1]{{\color{DarkPurple}\footnotesize \textsc{Summary.} #1}}


\newcommand{\Err}{E}


\begin{document}



\title{Optimal updating factor in Wang-Landau and metadynamics simulations}



\begin{abstract}
  The Wang-Landau (WL) algorithm and metadynamics
  are two closely-related techniques in free energy simulations.
  %
  They allow one to sample a flat distribution
  along a quantity of interest, $z$,
  by adaptively constructing a bias potential
  that offsets the potential of mean force (PMF).
  %
  The two algorithms differ by
  the manner of updating the bias potential:
  %
  in the WL case, each simulation step triggers
  an update of the bias potential to only
  the bin containing the current $z$,
  while in the metadynamics case,
  the bias potential of several neighboring bins
  are affected as well.
  %
  The asymptotic error of the resulting PMF
  is determined by the updating magnitude,
  and the rate of the reduction of the magnitude
  over the simulation time.
  %
  In the WL case,
  a simple and effective formula for the magnitude
  is given by the inverse of the number of simulation steps.
  %
  In this study, we shall reexamine the optimality of this prescription,
  and attempt to extend it to the metadynamics case.
  %
  Further, we compared the two algorithms, and found that
  although metadynamics produces a smoother PMF profile,
  it is less effective in reducing errors with the shorter wavelengths.
  %
  Thus, the latter relies more heavily on the assumption of
  the smoothness of the PMF profile,
  especially with a wide updating window.
\end{abstract}

\maketitle



\section{Introduction}



Free energy calculation\cite{frenkel} is a central theme
in computational physics and chemistry.
%
Given a system,
the problem is to compute,
via either Monte Carlo (MC) and
molecular dynamics (MD) simulations,
a distribution, $p^*(z)$,
along a quantity of interest $z$, and to see how
the distribution changes with external conditions,
such as temperatures and pressures.
%
The negative logarithm $-\log p^*(z)$,
or the potential of mean force (PMF),
defines a free energy.
%
A straightforward approach of computing the PMF
is to run multiple independent simulations
under different conditions,
and then piece together the information
gathered there.
%
This solution is often unsatisfactory
for a complex and glassy system,
because the system can be trapped
in a local free energy minimum
for a long time.



A better solution is to artificially alter
the target distribution,
such that we can, in a single simulation, sample
a flat distribution\cite{mezei1987, berg1992, lee1993,
wang2001, wang2001pre, laio2002, laio2008, barducci2011, sutto2012}
along $z$ over a wide range.
%
This alleviates the above problem of local trapping
as the system is now forced to travel more flexibly
along the $z$ direction,
which often represents a slow reaction coordinate.
%
It, however, requires one to efficiently construct a
bias potential for an often unknown system.
%
Note that the bias potential must exactly cancel the PMF
to achieve a flat distribution.



The Wang-Landau (WL) algorithm\cite{wang2001, wang2001pre}
and metadynamics\cite{laio2002, laio2008, barducci2011, sutto2012}
are two widely-used and closely-related\cite{micheletti2004}
techniques for this purpose,
in which a bias potential is actively built-up on-the-fly
during simulation.
%
In essence, these techniques
encourage the system to leave a region that has been visited
by elevating the bias potential there.
%
A main difference of the two techniques
is that in the WL case, an update is limited to the bin
containing the current $z$,
while metadynamics adopts an extended window
covering several neighboring bins.
%
The latter ensures a smooth profile
suitable for numerical differentiation for force calculation,
and hence is more often used molecular dynamics simulations.
%
To be effective, the updates to bias potential
are made at every few steps,
while a side effect is that the underlying dynamics
is no longer a equilibrium sampling
required by statistical mechanics.
%
Thus, one has to reduce the magnitude of updating
over the course of simulation.



Naturally, the manner of reducing
the updating magnitude\cite{liang2007,
belardinelli2007, belardinelli2007jcp, belardinelli2008,
morozov2007, zhou2008, morozov2009,
komura2012, caparica2012, caparica2014,
barducci2008, dickson2011, dama2014, dickson2015}
determines the precision of the final bias potential,
hence that of PMF.
%
For the WL algorithm, it is found
that asymptotically the optimal magnitude of updating $\ln f$
should be given by the
inverse time\cite{liang2007,
belardinelli2007, belardinelli2007jcp, belardinelli2008,
morozov2007, zhou2008},
where ``time'' is defined as
the number of simulation steps
divided by the number of bins.


In this study,
we shall give another proof of the inverse-time formula.
%
We then try to derive a similar result
for the optimal updating magnitude
for updating schemes involving several neighboring bins,
such as those used in metadynamics.
%
It turns out that the optimal updating magnitude
depends nontrivially on the simulation length
and other parameters in the latter case.
%
We also shall show that
the single-bin scheme used by the WL algorithm
is best in asymptotic convergence,
although practical simulations may not need
to reach this regime.
%
The article is organized as follows.
%
We present the analytical results in Sec. \ref{sec:theory},
numerically verify some key aspects
in Sec. \ref{sec:results},
and conclude the article
in Sec. \ref{sec:conclusion}.




\section{\label{sec:theory}
Theory}


In this section,
we first review the basics and fix notations
in Sec. \ref{sec:background}.
%
We then prove the optimality
of the inverse-time formula
for the single-bin WL algorithm
in Sec. \ref{sec:single-bin}.
%
Next, we discuss the generalized
multiple-bin schemes
in Secs. \ref{sec:multiple-bin}
and \ref{sec:band-matrix}.
%



\subsection{\label{sec:background}
Background}



\subsubsection{Flat-distribution sampling}



Given a system,
consider the problem of computing
the distribution, $p^*_i$
along a discrete quantity $i$.
%
%The discreteness of $i$ is natural
%in discrete models.
%
For example, $i$ can be the energy $E$
in lattice spin models; or the temperature index
in a simulated tempering simulation.
%
For a continuous quantity, $z$,
which is typical in molecular systems,
we can discretize $z$
such that the integer $i$ represents
the index of a small interval, or a bin, $(z, z + dz)$.
%
In both cases,
the distribution is normalized as
$\sum_{i = 1}^n p^*_i = 1$.



For a large system,
the distribution $p^*_i$ tends to
be localized around some $i_0$,
%
and to find out the global property,
it is often desirable to carry out
a biased sampling that targets
a wider distribution $p_i$.
%
%Here, we refer to simulations that target
%a flat or nearly flat distribution
%as entropic or multicanonical sampling.



To do so, we need to introduce a bias potential $V_i$
to modify the target distribution to
%
\begin{equation}
  \pi_i \propto p^*_i \, e^{-V_i}.
  \label{eq:pi_p_phi1}
\end{equation}
%
Upon normalization, $\sum_{i = 1}^n \pi_i = 1$,
we get
%
\begin{equation}
  \pi_i =
  \frac{ p^*_i \, e^{-V_i} }
  { \sum_{j = 1}^n p^*_j \, e^{-V_j} }.
  \label{eq:pi_p_phi}
\end{equation}
%
Particularly,
to achieve a flat distribution $\pi_i$,
the bias potential $V_i$
must coincide with $\log p^*_i$,
up to an additive constant.



Generally, we may wish to sample
a nearly flat distribution\cite{dayal2004, trebst2004, singh2011},
$p_i$,
by adjusting the bias potential $V_i$
on the fly.
%
For example,
we can monitor the histogram accumulated over
a period of time and use it for $\pi_i$ in
Eq. \eqref{eq:pi_p_phi1},
and then update
$V_i$ according to
$$
V_i^{\mathrm{new}}
=
V_i^{\mathrm{old}}
-
\log \frac{ p_i } { \pi_i }.
$$
%
%From Eq. \eqref{eq:pi_p_phi1},
%it is clear that to achieve the target distribution
%$\pi_i = p_i$ requires
%%
%\begin{equation}
%\log p^*_i
%=V_i + \ln p_i + c
%\label{eq:bias_pmf}
%\end{equation}
%%
%%
%In the end of the simulation,
%the PMF, $-\log p^*_i$,
%can be deduced from Eq. \eqref{eq:bias_pmf}.
%
The above inversion process,
adopted by early multicanonical or
entropic sampling\cite{berg1992, lee1993},
can be inconvenient in practice.
%
For example, a reasonably precise the histogram
requires an accumulation period
longer than the autocorrelation time,
whereas the latter is difficult to
estimate beforehand for an unknown system.


For later convenience, we introduce a shifted bias potential
%
\begin{equation}
  v_i \equiv V_i - \ln p^*_i + \ln p_i.
  \label{eq:v_def}
\end{equation}
%
Since the last two terms of Eq. \eqref{eq:v_def}
remain constant during the course of simulation,
updates to $V_i$'s are equivalent to those to $v_i$'s.
%
In terms of $v_i$'s, Eq. \eqref{eq:pi_p_phi}
becomes
%
\begin{equation}
  \pi_i
  =
  \frac{                p_i \, e^{-v_i} }
       { \sum_{j = 1}^n p_j \, e^{-v_j} }
  \propto
  p_i \, e^{-v_i}.
  \label{eq:pi_p_v}
\end{equation}
%
According to Eq. \eqref{eq:pi_p_phi1},
$v_i$ should approach a constant of $i$
upon convergence (i.e., when $\pi_i \approx p_i$).




\subsubsection{Updating schemes}



In the WL algorithm\cite{wang2001, wang2001pre},
$v_i$ are updated
in each MC step $t$,
and $v_i(t)$ as a function of $t$
is updated as
%
\begin{equation}
  v_i(t+1)
  =
  v_i(t)
  +
  \delta_{i, \, i(t)}
  \frac{ \alpha(t) } { p_i }.
  \label{eq:wl_update}
\end{equation}
%
where $i(t)$ is the bin at step $t$,
and $\alpha(t)$ is the updating magnitude.
%
We refer to the above scheme as the \emph{single-bin scheme},
for it applies only to the bias potential
at the current bin, $i(t)$,
%
In a more general \emph{multiple-bin scheme}, however,
several neighboring bins are updated:
%
\begin{equation}
  v_i(t+1)
  =
  v_i(t)
  +
  w_{i, \, i(t)}
  \frac{ \alpha(t) } { p_i },
  \label{eq:mbin_update}
\end{equation}
%
where $w_{ij}$ are a set of coefficients satisfying
$\sum_{j=1}^n w_{ij} = 1$.
%
We shall discuss other restrictions on $w_{ij}$
in Sec. \ref{sec:updating-matrix}.
%
The multiple-bin scheme is more commonly
used in metadynamics.



One can show that with a constant $\alpha(t) = \alpha > 0$,
the distribution collected from
the trajectory is identical to $p_i$.
%
However, the above updates
change $v_i(t)$ continuously,
and Eq. \eqref{eq:pi_p_v} no longer holds
at all times.
%
In other words,
the error in $v_i(t)$
depends on the updating magnitude\cite{
  zhou2005, liang2005, laio2005, bussi2006, poulain2006, liang2007,
  morozov2007, zhou2008, morozov2009, crespo2010,
  atchade2011, fort2015}.
%%
%The source of the deviation is two-fold.
%%
%First, since $v_i(t)$ is updated continuously,
%there is a random noise that is proportional
%to $\sqrt \alpha$.
%%
%Second, there is a systematic error
%that comes from the updating dynamics itself,
%as it breaks the Markovian nature
%that underlies Eq. \eqref{eq:pi_p_v}.
%%
%
Thus, one has to decrease $\alpha$ over time
to reduce the error of $v_i$,
in order to get the unbiased distribution
$\ln p_i^*$ via Eq. \eqref{eq:v_def}.





In the original WL scheme,
the updating magnitude $\alpha$ (or $\ln f$
in the original paper) is kept as a constant
for a period of time,
which is referred to as a stage below.
%
The histogram collected in the stage is monitored.
%
Once the histogram is sufficiently flat,
we are allowed to enter a new stage
with a reduced $\alpha$\cite{wang2001, wang2001pre}
(usually half as large as
that in the previous stage).
%
This scheme works well for early stages.
%
However, in later stages, it tends to reduce $\alpha$
too quickly, making the asymptotic error
saturate\cite{belardinelli2007, belardinelli2007jcp, belardinelli2008}.
%
Similarly, for metadynamics,
there are various ways of reducing the updating magnitude\cite{
  marsili2006, barducci2008, dickson2011, dama2014, dickson2015}.




\subsubsection{$1/t$ formula}



A more effective way
of updating the $\alpha(t)$
is to follow the formula
%
\begin{equation}
  \alpha(t) = \frac{1}{t},
  \label{eq:alpha_invt}
\end{equation}
%
where $t$ is the number of steps
from the beginning of the MC simulation,
which shall be referred to as the ``time'' below.
%
This surprisingly simple formula has attracted
several studies\cite{belardinelli2007, belardinelli2007jcp, belardinelli2008,
morozov2007, zhou2008, morozov2009,
komura2012, caparica2012, caparica2014}.
%
There are, however, some reservation about
the constant of proportionality:
the optimal formula for $\alpha(t)$
might be $C/t$ with a constant $C$
different from $1.0$\cite{
morozov2007, zhou2008, morozov2009}.



%Here we shall derive the optimal $\alpha(t)$
%For the WL algorithm, Eq. \eqref{eq:wl_update}.
%%
%Further, we shall generalize the result
%To a class of multiple-bin schemes,
%In which an update affects
%Not only the current bin $i(t)$,
%But also a few neighboring ones.
%%(such schemes are designed
%%to maintain a smooth bias potential).
%%
%We shall show that
%The constant of proportionality, $1$,
%In Eq. \eqref{eq:alpha_invt}
%Is optimal for the single-bin scheme,
%Eq. \eqref{eq:wl_update}
%(as in the WL algorithm),
%But not necessarily so
%For a multiple-bin scheme.
%%
%Below, we discuss the single-bin scheme
%In Section \ref{sec:single-bin},
%And the multiple-bin case
%In Section \ref{sec:multiple-bin}.



\subsection{\label{sec:single-bin}
Single-bin scheme}



In this section,
we shall derive the optimal $\alpha(t)$
for the single-bin scheme,
Eq. \eqref{eq:wl_update}.
%
To do so,
we shall first express the error of $v(t)$
as a functional of $\alpha(t)$,
and then minimize it by variation.



\subsubsection{Differential equation}



We first approximate Eq. \eqref{eq:wl_update}
by a differential equation
%
\begin{equation}
  \dot v_i(t)
  =
  \alpha(t) \, \frac{ h_i(t) } { p_i },
  \label{eq:vt_diffeq}
\end{equation}
%
where
$\dot v_i(t) \equiv dv_i(t)/dt$,
%
and $h_i(t) = \delta_{i, i(t)}$
is the instantaneous histogram,
which is equal to $1.0$
for the current bin $i(t)$
or zero otherwise.



Next, we split $h_i(t)$ into a deterministic part
and a noise part,
%
\begin{equation}
  h_i(t) = \langle h_i(t) \rangle + \zeta_i(t).
  \label{eq:h_split}
\end{equation}
%
Here, the deterministic part can be related
to the ``ensemble average'' of $h_i$.
%
The ensemble consists of many similar simulation copies
that have experienced the same schedule $\alpha(t)$
and have reached the same $v_i(t)$
at time $t$.
%
The initial states and the stochastic forces
during the process may, however, be different.



For sufficiently small $\alpha(t)$,
the $v_i$'s remain roughly the same for a short period.
%
Then,
the sampling process is approximately Markovian, % of a finite order,
and we may assume Eq. \eqref{eq:pi_p_v}
for the deterministic part
%
\begin{equation}
  \langle h_i(t) \rangle
  \approx
  \pi_i
  =
  \frac{ p_i \, e^{-v_i} }
  { \sum_{j = 1}^n p_j \, e^{-v_j} }.
  \label{eq:h_ave}
\end{equation}
%
%
%
For the noise part, we have
$\langle \zeta_i(t) \rangle = 0$,
%
%The noise is not necessarily white,
and the autocorrelation functions
depend only on the time difference
%
\begin{equation}
  \langle \zeta_i(t) \, \zeta_j(t') \rangle
  =
  \sigma_{ij}(t - t'),
  \label{eq:zeta_zeta_correlation}
\end{equation}
%
where $\sigma_{ij}(t)$ is an even function of $t$
that vanishes at large $t$.
%
More explicitly,
if the transition matrix of a period $\tau$
is $T^\tau_{ij}$,
then
$
  \langle \zeta_i(t) \, \zeta_j(t') \rangle
  =
  \langle h_i(t) \, h_j(t') \rangle
  -
  \langle h_i(t) \rangle \, \langle h_j(t') \rangle
  =
  T^{t - t'}_{ij} \pi_j - \pi_i \, \pi_j.
$




\subsubsection{Linear approximation}



To proceed, we first observe that
the average $\bar v = \sum_{i = 1}^n p_i \, v_i$
increases steadily over time:
%
\begin{equation}
\frac{ d \bar v } { d t }
=
\sum_{i = 1}^n p_i \dot v_i
=
\alpha(t) \sum_{i = 1}^n h_i(t) = \alpha(t).
\label{eq:dvbardt}
\end{equation}
%
However, we expect the difference between $v_i$ and $\bar v$,
%
\begin{equation}
  x_i \equiv v_i - \bar v = v_i - \sum_{j = 1}^n p_j \, v_j,
  \label{eq:x_def}
\end{equation}
%
to be small in the asymptotic regime,
and expand Eq. \eqref{eq:h_ave} as
%
\begin{align}
\langle h_i(t) \rangle
&\approx
\frac{                 p_i \, e^{- x_i} }
     { \sum_{ j = 1}^n p_j \, e^{- x_j} }
\approx
\frac{                 p_i (1 - x_i) }
     { \sum_{ j = 1}^n p_j (1 - x_j) }
\notag
\\
&\approx
p_i \, \left(
         1 - x_i + \sum_{j=1}^n p_j \, x_j
       \right)
=
p_i \, (1 - x_i),
\label{eq:hdet_linearize}
\end{align}
%
where we have used $\sum_{j=1}^n p_j = 1$,
and
%
\begin{equation}
  \sum_{i = 1}^n p_i \, x_i = 0,
  \label{eq:px_sum}
\end{equation}
which follows directly from Eq. \eqref{eq:x_def}.
%
From Eqs.
\eqref{eq:vt_diffeq},
\eqref{eq:h_split},
\eqref{eq:dvbardt}-\eqref{eq:hdet_linearize},
we get a set of decoupled equations
%
\begin{equation}
  \dot x_i(t)
  =
  -\alpha(t) \, \left[ x_i(t) - \frac{ \zeta_i(t) } { p_i } \right].
  \label{eq:dxdt_WL}
\end{equation}
\note{The derivation:
$$
\begin{aligned}
  \dot x_i
  &\stackrel{\mathrm{Eq.\; \eqref{eq:x_def}}}
            {=\joinrel=\joinrel=\joinrel=\joinrel=}
  \dot v_i - \dot{ \bar v }
  \stackrel{\mathrm{Eqs.\; \eqref{eq:vt_diffeq}\; and\; \eqref{eq:dvbardt}}}
            {=\joinrel=\joinrel=\joinrel=\joinrel=\joinrel=\joinrel=\joinrel=\joinrel=\joinrel=\joinrel=}
  \alpha(t) \left( \frac{ h_i } { p_i } - 1 \right)
  \\
  &\stackrel{\mathrm{Eq.\; \eqref{eq:h_split}}}
            {=\joinrel=\joinrel=\joinrel=\joinrel=}
  \alpha(t) \left( \frac{ \langle h_i \rangle } { p_i } + \frac{ \zeta_i } { p_i } - 1 \right)
  \stackrel{\mathrm{Eq.\; \eqref{eq:hdet_linearize}}}
            {=\joinrel=\joinrel=\joinrel=\joinrel=}
  \alpha(t) \left[ -x_i(t) + \frac{ \zeta_i } { p_i } \right].
\end{aligned}
$$
}


The total error of the bias potential can be written in
terms of $x_i$,
\begin{equation}
\Err
=
\sum_{i = 1}^n p_i \, \langle x_i^2 \rangle
=
\sum_{i = 1}^n p_i \, \left\langle (v_i - \bar v)^2 \right\rangle,
\label{eq:error_sum}
\end{equation}
where
$\bar v = \sum_{i = 1}^n p_i v_i$.
%
Our aim is to find the $\alpha(t)$
that minimizes Eq. \eqref{eq:error_sum}.
%
Fortunately, we shall find that it suffices to consider
a simpler one-variable problem
in this case.



\subsubsection{Optimization of a one-variable problem}



Consider the following equation
of a single variable:
%
\begin{equation}
\dot x(t) = -\alpha(t) \, \lambda \left[ x(t) - \xi(t) \right],
\label{eq:dxdt_alpha}
\end{equation}
%
where $\xi(t)$ is a generalized noise
that is invariant under time translation:
%
\begin{equation}
\left\langle \xi(t) \, \xi(t') \right\rangle
=
\kappa(t - t').
\label{eq:noise_correlation}
\end{equation}
%
Additionally, we require
\begin{equation}
  \lim_{t \rightarrow \pm\infty} \kappa(t) = 0.
  \label{eq:kappat_limit}
\end{equation}
%
For example, for a white noise,
$\kappa(t)$ is proportional to
Dirac's $\delta$-function, $\delta(t)$.
%
We wish to show that the $\alpha(t)$
of minimizing $\langle x^2(t) \rangle$ at long times
is given by
%
\begin{equation}
  \alpha(t) = \frac{1}{\lambda \, (t + t_0)}.
\label{eq:alpha_opt}
\end{equation}



To do so, we first recall
the formal solution of Eq. \eqref{eq:dxdt_alpha}:
%
\begin{equation}
x(t) = x(0) \, e^{-\lambda \, q(t)}
+ \int_0^t \dot u\bigl( q(t') \bigr) \, \xi(t') \, dt',
\label{eq:xt_solution}
\end{equation}
%
where,
%
\begin{equation}
q(t) \equiv \int_0^t \alpha(t') \, dt',
\label{eq:qt_definition}
\end{equation}
%
and
%
\begin{align}
u(q')
&\equiv
e^{-\lambda \, q(t) + \lambda \, q'}.
\label{eq:ut_definition}
\end{align}
So
\begin{align}
&\left\langle x^2(t) \right\rangle
=
\langle x^2(0) \, \rangle e^{-2 \, \lambda \, q(t)}
\notag
\\
&\qquad
+
\int_0^t \int_0^t
  \dot u\bigl( q(t') \bigr) \,
  \dot u\bigl( q(t'') \bigr) \,
  \kappa(t' - t'') \, dt'' \, dt'.
\label{eq:x2t_average}
\end{align}


We now fix the endpoint value of $q(t)$
and variate the curve.
%
This is equivalent to variating $u$
with fixing endpoint values,
$u\bigl( q(0) \bigr)  = e^{-\lambda \, q(t)}$
and
$u\bigl( q(t) \bigr) = 1$.
%
Note also that the first term on the right-hand side
of Eq. \eqref{eq:x2t_average} is fixed during
the variation process.
%
%We shall further demand that
%%
%\begin{equation}
%  \lim_{t \to \infty} q(t) \to \infty.
%  \label{eq:qt_limit}
%\end{equation}
%%
%Then, for a large $t$,
%the second term on the right-hand side
%of Eq. \eqref{eq:xt_solution} can be neglected, and
%%
%\begin{align}
%\left\langle x^2(t) \right\rangle
%%&=
%%\int_0^t \int_0^t \dot u(t') \, \dot u(t'')
%%    \left\langle \xi(t') \xi(t'') \right\rangle dt'' \, dt'
%%\notag
%%\\
%&=
%\int_0^t \int_0^t
%  \dot u\bigl( q(t') \bigr) \,
%  \dot u\bigl( q(t'') \bigr) \,
%  \kappa(t' - t'') \, dt'' \, dt'.
%\label{eq:x2t_average_asym}
%\end{align}
%
By the Euler-Lagrange equation, we get
$$
\begin{aligned}
0
&=
\frac{d}{d\tau} \int_0^t
  \dot u\bigl( q(t') \bigr) \, \kappa(t' - \tau) \, dt'
\\
&= \int_0^t
  \ddot u\bigl( q(t') \bigr) \, \kappa(t' - \tau) \, dt',
\end{aligned}
$$
where we have dropped the boundary terms
by using Eq. \eqref{eq:kappat_limit}.
%
%In fact, we can repeat the process $n$ times, and
%$$
%\int_0^t u^{(n)}(t') \, \kappa(t' - \tau) \, dt' = 0,
%\qquad (n \ge 1)
%$$
%
In order for this to hold for any $\tau$,
we must have
%
\begin{equation}
\ddot u\bigl( q(t') \bigr) = 0,
\qquad
\mathrm{or}
\;\;
\dot u\bigl( q(t') \bigr) = c,
\label{eq:ddu_eq_0}
\end{equation}
%
where $c$ is a constant of $t'$.
%
Using Eq. \eqref{eq:ut_definition}
we get
$$
e^{-\lambda \, q(t) + \lambda \, q(t')}
=
c \, (t' + t_0),
$$
where $t_0$ is a constant.
%
Taking the logarithm, and differentiating this with respect to $t'$
yields Eq. \eqref{eq:alpha_opt}.



\subsubsection{Optimal $\alpha(t)$ for the single-bin scheme}



Going back to the problem of
minimizing Eq. \eqref{eq:error_sum}
for the single-bin scheme.
%
Note that Eq. \eqref{eq:dxdt_WL}
is of the form of Eq. \eqref{eq:dxdt_alpha}.
%
Thus, with $\lambda = 1$,
Eq. \eqref{eq:alpha_opt}
will optimize each term $\langle x_i^2 \rangle$,
and hence the sum.
%
In other words, the $\alpha(t)$ given by
Eq. \eqref{eq:alpha_invt} is optimal
up to a constant $t_0$.





\subsection{\label{sec:multiple-bin}
Multiple-bin scheme}



We now consider the multiple-bin scheme,
Eq. \eqref{eq:mbin_update}.
%
In this case,
upon a visit to bin $j$,
we update not only the bias potential there,
but also those for a few neighboring bins $i$.
%
The update magnitudes are given by a matrix $\mathbf w$
with elements $w_{ij}$.
%
Then, Eq. \eqref{eq:vt_diffeq} is generalized to
\begin{equation}
  \dot v_i(t)
  =
  \alpha(t) \sum_{j=1}^n \, w_{ij} \frac{ h_j(t) } { p_j }.
  \label{eq:vt_diffeq_mbin}
\end{equation}


The optimization procedure is similar to the single-bin case,
whereas the matrix nature brings about
some technical complexities that require
certain restrictions and approximations.



\subsubsection{\label{sec:updating-matrix}
Updating matrix}



The matrix $\mathbf w$ is not arbitrary.
%
A necessary condition to sample desired distribution
$\mathbf p = (p_1, \dots, p_n)$
is the following.
%
In Eq. \eqref{eq:vt_diffeq_mbin},
if $h_j(t)$ were the same as $p_j$,
the rates of change $\dot v_i(t)$'s
should be independent of $i$.
%
This allows $v_i(t)$'s to grow uniformly
in the asymptotic regime.
%
Thus, by a proper scaling of $\alpha(t)$,
we may write this condition as
%
\begin{equation}
  \sum_{j = 1}^n w_{ij} = 1.
  \label{eq:w_sumj}
\end{equation}
%
The above condition means that $(1, \dots, 1)^T$
is a right eigenvector of $\mathbf w$
with eigenvalue $1$.
%
Thus, the transpose $\mathbf w^T$
resembles a transition matrix,
although the matrix elements can be negative.



To simplify the following discussion,
we shall further impose the
detailed balance condition:
%
\begin{equation}
  p_i \, w_{ij} = p_j \, w_{ji}.
  \label{eq:w_detailedbalance}
\end{equation}
%
It follows that
\begin{equation}
  \sum_{i = 1}^n p_i \, w_{ij}
  =
  \sum_{i = 1}^n p_j \, w_{ji}
  = p_j,
  \label{eq:w_balance}
\end{equation}
%
i.e., $\mathbf p$ is a left eigenvector of
$\mathbf w$ with eigenvalue $1$.



Further, by Eq. \eqref{eq:w_detailedbalance},
we can define a symmetric matrix $\hat{\mathbf w}$
as $\hat w_{ij} = \sqrt{p_i/p_j} \, w_{ij}$
that can be diagonalized\cite{vankampen}
with a set of orthonormal eigenvectors:
%
$\sum_{i = 1}^n \varphi_{ki} \, \hat w_{ij} = \lambda_k \, \varphi_{kj}$.
%
Thus,
in terms of diagonalizing $\mathbf w$, we have
\begin{equation}
  \sum_{i = 1}^n \phi_{ki} \, w_{ij} = \lambda_k \, \phi_{kj},
  \label{eq:eig_w}
\end{equation}
where
$\phi_{ij} \equiv \sqrt{p_j} \, \varphi_{ij}$ satisfies
the orthonormal conditions\cite{vankampen}:
%
\begin{align}
\sum_{k = 1}^n \phi_{ki} \, \phi_{kj}
&= \delta_{ij} \, p_i,
\label{eq:eig_orthonormal_cols}
\\
\sum_{k = 1}^n \frac{ \phi_{ik} \, \phi_{jk} }{ p_k }
&= \delta_{ij}.
\label{eq:eig_orthonormal_rows}
\end{align}



%It is worth pointing out
Note that, in a stable updating scheme,
all eigenvalues $\lambda_k$ of $\mathbf w$
must be nonnegative,
for otherwise there is a fluctuating mode
that can increase indefinitely.



\subsubsection{Linearization and decoupling to eigenmodes}



We can now simplify Eq. \eqref{eq:vt_diffeq_mbin}.
%
By using Eq. \eqref{eq:w_balance},
we can show that
Eqs. \eqref{eq:dvbardt} and \eqref{eq:px_sum}
remain true, and
%
$$
\begin{aligned}
\dot x_i
&= \alpha(t) \sum_{j=1}^n w_{ij}
\left( \frac{ h_j } { p_j }  - 1 \right)
\\
&\approx
-\alpha(t) \sum_{j = 1}^n
w_{ij} \left[ x_j(t) - \frac{\zeta_j (t)}{p_j} \right],
\end{aligned}
$$
where
we have expanded the right-hand side
in linear terms of $x_j$.

\note{The proof of Eq. \eqref{eq:dvbardt},
$$
\begin{aligned}
  \dot{ \bar v }
  &=
  \sum_{i = 1}^n p_i \, \dot v_i
  %
  \stackrel{\mathrm{Eq.\; \eqref{eq:vt_diffeq_mbin}}}
  {=\joinrel=\joinrel=\joinrel=\joinrel=}
  %
  \sum_{j = 1}^n \alpha(t) \, \frac{ h_j(t) } { p_j }
                 \sum_{ i = 1 }^{n} p_i \, w_{ij}
  \\
  &
  \stackrel{\mathrm{Eq.\; \eqref{eq:w_balance}}}
  {=\joinrel=\joinrel=\joinrel=\joinrel=}
  \sum_{j = 1}^n \alpha(t) \, h_j(t)
  =
  \alpha(t).
\end{aligned}
$$
Equation \eqref{eq:px_sum} follows directly from the definition
Eq. \eqref{eq:x_def}.
}

Now by diagonalizing the matrix $\mathbf w$
using the eigenvectors defined in Eqs. \eqref{eq:eig_w},
\eqref{eq:eig_orthonormal_cols},
and
\eqref{eq:eig_orthonormal_rows},
we get a set of decoupled equations
%
\begin{equation}
\dot y_k(t)
=
-\alpha(t) \, \lambda_k \, [y_k(t) - \eta_k(t)],
\label{eq:yt_diffeq}
\end{equation}
%
where
\begin{align}
  y_k &= \sum_{j=1}^n \phi_{kj} \, x_j,
  \label{eq:y_def}
  \\
  \eta_k &= \sum_{j=1}^n \phi_{kj} \frac{ \zeta_j}{ p_j}.
  \label{eq:eta_def}
\end{align}



\subsubsection{Error function}



The error function can be obtained
from Eq. \eqref{eq:eig_orthonormal_cols}
as
\begin{align}
  \Err
  \equiv
  \sum_{i = 1}^n p_i \,
                 \langle x_i^2 \rangle
  =
  \sum_{i, j, k=1}^n \phi_{ki} \, \phi_{kj} \,
                     \langle x_i \, x_j \rangle
  =
  \sum_{k = 1}^n \langle y_k^2 \rangle,
  \label{eq:y2_sum}
\end{align}
%
which is a superposition of the components of the modes.
%
Note that there is always one mode, say
$y_1 = \sum_{j=1}^n p_j \, x_j$
(corresponding to $\lambda_1 = 1$),
vanishes identically by Eq. \eqref{eq:px_sum}.
%
Thus, we can start the sum in Eq. \eqref{eq:y2_sum}
from $k = 2$.



In analogous to Eq. \eqref{eq:x2t_average},
we have, from Eq. \eqref{eq:y2_sum},
%
\begin{equation}
  \Err
  =
  \Err_R + \Err_A,
  \label{eq:error_split}
\end{equation}
%
which is the sum of the residual error
%
\begin{equation}
  \Err_R
  =
  \sum_{k = 2}^n
    \left\langle y_k^2(0) \right\rangle \,
    e^{ - 2 \, \lambda_k  \, q(t) },
  \label{eq:error_res}
\end{equation}
%
from the decay of the initial errors,
and the asymptotic error
\begin{equation}
  \Err_A
  =
  \int_0^t \int_0^t
  \sum_{k = 2}^n
    \dot u_k\bigl( q(t') \bigr) \,
    \dot u_k\bigl( q(t'') \bigr) \,
    \kappa_k(t' - t'') \, dt' \, dt'',
  \label{eq:error_asym}
\end{equation}
%
where,
\begin{align*}
  u_k(z)
  &\equiv
  e^{\lambda_k \, [z - q(t)]},
  \\
  \kappa_k(t - t')
  &\equiv
  \left\langle
    \eta_k(t) \, \eta_k(t')
  \right\rangle.
\end{align*}
%
\note{The extremal condition of $\Err_A$ is given by
%
\begin{align}
\sum_{k = 2}^n
\lambda_k \, u_k\bigl( q(\tau) \bigr)
\int_0^t
\ddot u_k \bigl( q(t') \bigr) \, \kappa_k(t' - \tau) \, dt' = 0.
\label{eq:optimal_mbin}
\end{align}
%
If all $\lambda_k$, hence all $u_k$, are the same,
the above condition can be satisfied
by Eq. \eqref{eq:ddu_eq_0},
which is the single-bin case.
%
Unfortunately, the above expression
appears to be useless in a general setting.
}



\subsubsection{White-noise approximation}



In the asymptotic regime,
we can approximate $\kappa_k(t)$ as
%
\begin{equation}
  \kappa_k(t) \approx \Gamma_k \, \delta(t)
  \label{eq:kappa_delta}
\end{equation}
%
where,
\begin{equation}
  \Gamma_k = \int_{-\infty}^\infty \kappa_k(t) \, dt,
  \notag
  %\label{eq:Gamma_integral}
\end{equation}
or in discrete time,
\begin{equation}
  \Gamma_k = \sum_{t = -\infty}^\infty \kappa_k(t).
  \label{eq:Gamma_sum}
\end{equation}
%
Then, Eq. \eqref{eq:error_asym} is simplified as
%
\begin{align}
  \Err_A
  &=
  \int_0^t
  \sum_{k = 2}^n
  \Gamma_k \, \dot u_k^2\bigl( q(t') \bigr) \, dt'.
  \label{eq:error_asym1}
\end{align}
%
The next order correction to Eq. \eqref{eq:kappa_delta}
would lead to a correction to $\Err$
that is about $\alpha_{\max}^2$ times as large.


\note{This is equivalent to saying that the Fourier transform
  $$
  \tilde \kappa_k(\omega) = \Gamma_k
  $$
  is a roughly constant.
  %
  For the next order correction we would have
  $$
  \tilde \kappa_k(\omega) = \Gamma_k + \Gamma^{(2)}_k \omega^2 + \dots,
  $$
  which leads to a correction to $\Err$
  $$
  \Gamma^{(2)}_k
  \int_0^t \ddot u_k^2\bigl( q(t') \bigr) \, dt'
  \propto
  \alpha^2 \, \Err.
  $$
  This correction is of order $\alpha^2$
  compared to the main contribution in Eq. \eqref{eq:error_asym1}.
  %
  Thus, we can assume Eq. \eqref{eq:kappa_delta},
  once $\alpha(t)$ has dropped below a certain level.
}

\note{
  The extremal property, Eq. \eqref{eq:optimal_mbin},
  now becomes
  %
  \begin{align}
    \sum_{k = 1}^n
      \Gamma_k \, \lambda_k \,
      u_k\bigl( q(\tau) \bigr) \,
      \ddot u_k\bigl( q(\tau) \bigr) = 0.
    \label{eq:optimal_mbin1}
  \end{align}
  %
  If we interpret $u_k$ as the position,
  and $\Gamma_k \, \lambda_k$ as the mass,
  then the right-hand side gives the virial.
  %
  Thus, the extremal condition demands
  the total virial to be zero.
  $$\,$$
}



\subsubsection{\label{sec:Gamma}
Integrals of the autocorrelation functions}



We can evaluate the $\Gamma_k$'s
in two special case.
%
If the sampling is perfect,
Eq. \eqref{eq:kappa_delta} is exact, and
%
\begin{equation}
  \Gamma_k = 1.
  \label{eq:Gamma_perfect}
\end{equation}
%
In this case,
the bin index $k$ is an independent random variable
at each time step $t$, and
%
\begin{equation}
  \left\langle
    \zeta_i(t) \, \zeta_j(t')
  \right\rangle
  =
  \left( p_i \, \delta_{ij} - p_i \, p_j \right) \, \delta(t - t'),
  \label{eq:zz_perfect}
\end{equation}
%
where $\delta(t - t')$ is understood as an adaption of $\delta_{t, t'}$
from the discrete time case to the continuous limit. % hereinafter.
%
\note{To show Eq. \eqref{eq:zz_perfect}, we first note that
  with $t \ne t'$ (going back to the discrete time case),
  $$
  \begin{aligned}
  \left\langle
    \zeta_i(t) \, \zeta_j(t')
  \right\rangle
  &=
  \left\langle
    \zeta_i(t)
  \right\rangle
  \left\langle
    \zeta_j(t')
  \right\rangle
  \\
  &=
  \bigl\langle h_i(t)  - \langle h_i(t)  \rangle \bigr\rangle
  \bigl\langle h_j(t') - \langle h_j(t') \rangle \bigr\rangle
  =
  0.
  \end{aligned}
  $$
  Next, for $t = t'$, we have
  $$
  \zeta_i(t)
  = h_i(t) - \langle h_i(t) \rangle
  = \delta_{i, i(t)} - p_i,
  $$
  and
  $$
  \begin{aligned}
  \left\langle
    \zeta_i(t) \, \zeta_j(t)
  \right\rangle
  &=
  \left\langle
    \delta_{i, i(t)} \, \delta_{j, i(t)}
  \right\rangle
  -
  p_i \, p_j
  \\
  &=
  \left\langle
    \delta_{i, i(t)}
  \right\rangle
  \delta_{i, j}
  -
  p_i \, p_j
  =
  p_i \, \delta_{ij}
  - p_i \, p_j.
  \end{aligned}
  $$
  %
  The average
  $\left\langle \delta_{i, i(t)} \right\rangle$ is $p_i$,
  because as $i(t)$ goes through $1$ to $n$,
  $\delta_{i, i(t)}$ gives $1$
  if $i(t)$ matches $i$ with probability $p_i$,
  or $0$ otherwise.
}
%
Thus, from Eqs. \eqref{eq:eig_orthonormal_rows} and \eqref{eq:eta_def},
we get, for $k \ge 2$,
%
\begin{equation}
  \kappa_k(t - t')
  =
  \sum_{j,l = 1}^n
  \frac{ \phi_{kj} } { p_j }
  \frac{ \phi_{kl} } { p_l }
  \langle \zeta_j(t) \, \zeta_l(t') \rangle
  %
  = \delta(t - t'),
  \label{eq:kappa_perfect}
\end{equation}
%
\note{To show Eq. \eqref{eq:kappa_perfect},
  $$
  \begin{aligned}
  \kappa_k(t - t')
  &=
  \sum_{j,l = 1}^n
  \frac{ \phi_{kj} } { p_j }
  \frac{ \phi_{kl} } { p_l }
  \langle \zeta_j(t) \, \zeta_l(t') \rangle
  \\
  %
  &=
  \sum_{j,l = 1}^n
  \frac{ \phi_{kj} } { p_j }
  \frac{ \phi_{kl} } { p_l }
  \left(
    p_j \, \delta_{j, l} - p_j \, p_l
  \right)
  \, \delta(t - t')
  \\
  &=
  \left(
    \sum_{j = 1}^n
    \frac{ \phi_{kj}^2 } { p_j }
    -
    \sum_{j = 1}^n \phi_{kj}
    \sum_{l = 1}^n \phi_{kl}
  \right)
  \, \delta(t - t')
  \\
  &=
  (1 - 0 \cdot 0) \, \delta(t - t').
  \end{aligned}
  $$
  where,
  we have used
  Eq. \eqref{eq:eta_def}
  on the first line,
  %
  Eq. \eqref{eq:zz_perfect}
  on the second line,
  and
  Eq. \eqref{eq:eig_orthonormal_rows}
  on the last line.
  %
  Note for $k \ge 2$,
  $$
  \sum_{j = 1}^n \phi_{kj}
  =
  \sum_{j = 1}^n \frac{ \phi_{kj} \, \phi_{1j} } { p_j }
  =
  \delta_{k1}
  =
  0.
  $$
  as the mode $1$, or the first row, $\phi_{1j}$,
  is given by the equilibrium distribution $p_j$.
}


Another special case is the one-step process\cite{vankampen}:
in each step, the system hops to the previous bin $i - 1$
or the next bin $i + 1$ with equal probability $\frac 1 2$,
ignoring out-of-boundary transitions.
%
The transition matrix is
%
\begin{equation}
\arraycolsep=3.6pt\def\arraystretch{1.4}
\mathbf T
=
\left(
  \begin{array}{cccccccc}
    \frac 1 2 & \frac 1 2 & 0 & \dots & 0 \\
    \frac 1 2 & 0         & \frac 1 2 & \dots & 0 \\
    \vdots & &  & & \vdots \\
    0 & \dots & \frac 1 2 & 0  & \frac 1 2 \\
    0 & \dots & 0 & \frac 1 2 & \frac 1 2
  \end{array}
\right).
\label{eq:T_nn}
\end{equation}
%
We shall further assume that
the eigenvectors of $\mathbf w$
coincide with those of $\mathbf T$,
while the eigenvalues may differ
(this is true for a class of matrices
discussed in Sec. \ref{sec:band-matrix}).
%
The eigenvalues of $\mathbf T$ are
$$
\Lambda_k = 1 - 2 \sin^2 \frac{ (k - 1) \, \pi } { 2 \, n }.
$$
Note that these $\Lambda_k$'s are
not to be confused with the eigenvalues of $\mathbf w$,
$\lambda_k$'s.
%
The autocorrelation function $\kappa_k(t)$
decays as $\kappa_k(0) \, \Lambda_k^t$,
with the same-time value $\kappa_k(0) = 1$ being the same as
that in perfect sampling.
%
Thus,
$\Gamma_k$ is roughly twice
the autocorrelation time,
and from Eq. \eqref{eq:Gamma_sum}, we get
%
\begin{equation}
\Gamma_k
=
1 + 2 \, \sum_{t = 1}^\infty \Lambda_k^t
=
\frac{ 1 + \Lambda_k } { 1 - \Lambda_k }
=
\cot^2 \left[ \frac{ (k - 1) \, \pi } { 2 \, n } \right].
\label{eq:Gamma_onestep}
\end{equation}
%
We can use this case to model local sampling processes.


\subsubsection{\label{eq:eqlerr}
Equilibrium error under a fixed updating magnitude
}


Before addressing the optimal $\alpha(t)$,
let us first compute the equilibrium value of $\Err$,
under a fixed $\alpha(t) \equiv \alpha_0$.
%
In this case, we can drop $\Err_R$, and
%
\begin{align}
  \Err
  &=
  \Err_A
  =
  \int_0^t
    \sum_{k = 2}^n
      \Gamma_k \, (\lambda_k \, \alpha_0)^2 \,
      e^{ 2 \, \lambda_k \, \alpha_0 \, (t' - t) }
    \, dt
  \notag
  \\
  &
  \stackrel{ t \to \infty }
  { =\joinrel=\joinrel=\joinrel= }
  %
  \sum_{ k = 2 }^n
    \frac 1 2 \, \Gamma_k \, \lambda_k \, \alpha_0,
  \label{eq:error_eql}
\end{align}
%
or, in terms of the components,
%
\begin{equation}
  \left\langle
    y_k^2
  \right\rangle
  =
  \frac 1 2 \, \Gamma_k \, \lambda_k \, \alpha_0,
  \label{eq:y2_eql}
\end{equation}
%

If we can assume that before entering the asymptotic regime,
the system has entered the equilibrium under a fixed $\alpha_0$,
we can then use Eq. \eqref{eq:y2_eql}
for the $\langle y_k^2(0) \rangle$'s in Eq. \eqref{eq:error_res}.

\note{Another possible approximation
  is to interpret $\Gamma_k$ as twice the autocorrelation time,
  and the kernel is exponential
  $$
  \kappa_k = \exp\left( - \frac{2}{\Gamma_k} |t| \right),
  $$
  then a slightly more accurate value is given by\cite{vankampen}
  $$
  \left\langle
    y_k^2
  \right\rangle
  =
  \frac{      ( \lambda_k \, \alpha_0 )^2     }
       { \lambda_k \, \alpha_0 + 2 / \Gamma_k }.
  $$
  But it appears that the main error of Eq. \eqref{eq:y2_eql}
  is an underestimate instead of an overestimate.
  And a possible source of error might be from
  converting a discrete sum to a continuous integral.
}



\subsubsection{\label{twolimits}
Asymptotically optimal schedule}


The asymptotic error given by Eq. \eqref{eq:error_asym1}
can be rewritten as
%
\begin{equation}
  \Err_A
  =
  \int_0^t
    {\mathcal L} \bigl[ q(\tau)\bigr]
    \, d\tau,
  \notag
  %\label{eq:error_asym_action}
\end{equation}
%
where, we have defined the Lagrangian,
%
\begin{align}
  {\mathcal L} \bigl[ q(\tau) \bigr]
  &=
  \sum_{ k = 2 }^n
    \Gamma_k \, \dot u_k^2\bigl( q(\tau) \bigr)
  \notag
  \\
  &=
  \sum_{ k = 2 }^n
    \Gamma_k \, \lambda_k^2 \, u_k^2 \bigl( q(\tau) \bigr)
  \; \dot q^2( \tau ).
  \label{eq:error_asym_lagr}
\end{align}
%
To minimize the functional,
we notice that the Lagrangian given by Eq. \eqref{eq:error_asym_lagr}
does not explicitly depend on time, $t$.
%
Thus, the Hamiltonian,
%
\begin{equation}
  \mathcal H
  =
  \frac{ \partial \mathcal L }
       { \partial \dot q     }
  \, \partial \dot q
  -
  \mathcal L
  =
  2 \, \mathcal L
  - \mathcal L
  =
  \mathcal L,
  %=
  %\mathrm{const.}
  \notag
  %\label{eq:error_asym_Hamiltonian}
\end{equation}
%
is conserved, and in this case,
the Hamiltonian and the Lagrangian coincide.
%
So we may write
%
\begin{equation}
  \mathcal L
  =
    \textstyle\sum_{ k = 2 }^n
      \Gamma_k \, \lambda_k^2
      \, u_k^2 \bigl[ q(\tau) \bigr]
  \;
  \dot q^2(\tau)
  =
  c^2
  \quad
  (c > 0).
  \label{eq:Lagrangian_const}
\end{equation}
%
Integrating integrate the square root of the left-hand side,
%
\begin{equation}
  \int_0^{ q(\tau) }
    \sqrt{
      \textstyle\sum_{ k = 2 }^n
        \Gamma_k \, \lambda_k^2
        \, u_k^2( q' )
    }
    \;
    d q'
  =
  c \, d\tau
  ,
  \label{eq:q_opt}
\end{equation}
%
we get an implicit equation for the optimal $\alpha(\tau)$
via its integral $q(\tau)$.


Although the explicit solution of Eq. \eqref{eq:q_opt} is difficult,
we can obtain an expression regarding $\alpha(\tau)$
by differentiating Eq. \eqref{eq:Lagrangian_const},
which yields
%
\begin{equation}
  \frac{ d      }
       { d \tau }
  \left(
    \frac{       1        }
         { \alpha( \tau ) }
  \right)
  =
  \frac{
    \sum_{ k = 2 }^n
      \Gamma_k \, \lambda_k^3
      \, u_k^2 \bigl( q(\tau) \bigr)
  }
  {
    \sum_{ k = 2 }^n
      \Gamma_k \, \lambda_k^2
      \, u_k^2 \bigl( q(\tau) \bigr)
  }
  .
  \label{eq:dinvadt}
\end{equation}
%



Consider examine two limiting cases.
%
First, during the beginning of a long simulation,
we have $\tau \ll t$ and $q(t) - q(\tau) \to \infty$,
the sums on the right-hand side of Eq. \eqref{eq:dinvadt}
is dominated by the term with the largest
$u_k = e^{ \lambda_k \, [q(\tau) - q(t)] }$,
or, equivalently, with the smallest $\lambda_k$.
Then
\begin{equation}
  \frac{ d      }
       { d \tau }
  \left(
    \frac{       1        }
         { \alpha( \tau ) }
  \right)
  =
  \lambda_{ \min }
  ,
  \notag
  %\label{eq:dinvadt_limit1}
\end{equation}
%
and
%
\begin{equation}
  \alpha( \tau )
  =
  \frac{               1                  }
       { \lambda_{ \min } ( \tau + t_1 )  }
  .
  \label{eq:alpha_limit1}
\end{equation}
%


Second, near the end of simulation,
we have $\tau \to t$, $u_k \approx 1$,
%
\begin{equation}
  \frac{ d      }
       { d \tau }
  \left(
    \frac{       1        }
         { \alpha( \tau ) }
  \right)
  =
  \frac{
    \sum_{ k = 2 }^n
      \Gamma_k \, \lambda_k^3
  }
  {
    \sum_{ k = 2 }^n
      \Gamma_k \, \lambda_k^2
  }
  =
  \bar \lambda,
  \notag
  %\label{eq:dinvadt_limit2}
\end{equation}
%
where,
$\bar \lambda$
is the average of $\lambda_k$ weighted by $\Gamma_k \, \lambda_k^2$,
and it tends to be dominated
by the largest $\lambda_k$.
%
Thus, in this regime, we have
%
\begin{equation}
  \alpha( \tau )
  =
  \frac{             1                }
       { \bar \lambda ( \tau + t_2 )  }
  .
  \label{eq:alpha_limit2}
\end{equation}
%
Thus, we may characterize the optimal schedule as
%
\begin{equation}
  \alpha(\tau)
  =
  \frac{                      1                          }
       { \lambda(\tau) \, \bigl[ \tau + t_0(\tau) \bigr] }
,
\notag
\end{equation}
%
where $\lambda(\tau)$ and $t_0(\tau)$ are two
slowly-varying functions,
with the former increases gradually from $\lambda_{\min}$
to $\bar \lambda$.

%Both Eqs. \eqref{eq:alpha_limit1} and \eqref{eq:alpha_limit2}
%resemble Eq. \eqref{eq:alpha_opt}.





\subsubsection{\label{sec:optWL}
Asymptotic optimality of the single-bin (WL) scheme}



We now give an interesting deduction
following from the error functional
given in Eq. \eqref{eq:error_asym1}:
the single-bin scheme is asymptotically optimal.
%
Consider a class of $\mathbf w$ matrices
sharing the same set of eigenvectors,
hence same $\Gamma_k$'s,
but with different eigenvalues, $\lambda_k$'s.
%
Using the Cauchy-Schwarz inequality, we have,
for any set of numbers, $c_k \ge 0$,
%
\begin{align}
&
\left(
  \int_0^t dt'
    \sum_{k = 2}^n
      \Gamma_k \, \dot u_k^2\bigl( q(t') \bigr)
\right)
%
\left(
  \int_0^t dt'
    \sum_{k = 2}^n
      \Gamma_k \, c_k^2
\right)
%
\notag
\\
&
\qquad \qquad
\ge
\left(
  \int_0^t dt'
    \sum_{k = 2}^n
      \Gamma_k \, c_k \, \dot u_k \, \bigl( q(t') \bigr)
\right)^2
\notag
\\
&
\qquad \qquad
=
\left(
  \sum_{k = 2}^n \Gamma_k \, c_k
    \left[
      1 - e^{ -\lambda_k q(t) }
    \right]
\right)^2.
\label{eq:CSineq}
\end{align}
%
This inequality follows from the fact
that the following quadratic polynomial of $Y$,
$$
\int_0^t
  dt' \sum_{k = 2}^n \Gamma_k \,
    \left( \dot u_k \, Y - c_k \right)^2
  \equiv
  A \, Y^2 + B \, Y + C
%\ge 0,
$$
is always nonnegative, and hence
has a non-positive discriminant,
$B^2 - 4 \, A \, C$.
%
This condition furnishes the desired inequality.
%
Note that the last expression of \eqref{eq:CSineq}
depends on $q(t')$ only through the end point value,
which is fixed in the variation process.
%
Thus, the inequality sets a lower bound
of the error $\Err$
given in Eq. \eqref{eq:error_asym1}.
%
The equality is achieved
if $\dot u_k\left( q(t') \right) = c_k$
for all $k = 2, \dots, n$ at any $t'$,
up to a multiplicative factor.
%
Solving this set of equations yields
%
$$
\alpha(t') = \frac{              1              }
                  { \lambda_k \, (t' + t_{k0} ) },
\quad
\mathrm{with\;\;}
t_{k0} = \frac{             t            }
              { e^{ \lambda_k q(t) } - 1 }.
$$
%
Such a solution is possible only if
%
\begin{equation}
\lambda_2 = \dots = \lambda_n.
\end{equation}
%
\note{We add some steps for the above solution.
  Integrating $\dot u_k = c_k$ yields
  \begin{equation}
  u_k(t') = c_k \left(t' + t_{k0} \right).
  \label{eq:ui_solution}
  \end{equation}
  Given that $u_k(t) = 1$ and $u_k(0) = e^{-\lambda_k \, q(t)}$,
  we have
  $$
  c_k = \frac{ 1 }{ t + t_{k0} },
  \quad
  \mathrm{and\;}
  \frac{ t_{k0} } { t + t_{k0} }
  =
  e^{ -\lambda_k \, q(t) }.
  $$
  Taking the logarithm of Eq. \eqref{eq:ui_solution} yields
  $$
  -\lambda_k \, q(t) + \lambda_k \, q(t')
  = \log c_k + \log\left( t' + t_{k0} \right).
  $$
  Differentiating this with respect to $t'$ yields
  $$
  \lambda_k \, \alpha(t') = \frac{ 1 } { t' + t_{k0} }.
  $$
}

Now an updating matrix $\mathbf w$ satisfying this condition
is essentially a multiple of the identity matrix
(since $\lambda_1$ corresponds to the mode
of a uniform shift of all $v_k$,
we can freely set it to $\lambda_2$
without changing the nature of the updating scheme).
%
Thus, in terms of asymptotic convergence,
the single-bin scheme
(as adopted in the WL algorithm)
is generally the most efficient
updating scheme.


We must, however, emphasize that the above result
holds only under the long time limit,
and it is not necessarily true
for finite length simulations.
%
For example, consider a system that is initially
prepared under a fixed $\alpha_0$
before starting the schedule of, $\alpha(t)$.
%
Then for shorter times, $t \approx 0$,
the error is given by Eq. \eqref{eq:error_eql},
which is clearly smaller if some eigenvalues
are less than $1.0$ (the single-bin case).
%
In practice, real simulations
may have long achieved the desired precision
before reaching the asymptotic regime,
especially with some $\lambda_k$'s close to $0$.



\subsubsection{Variationally optimal schedule, $\alpha(t)$}



Generally, for an arbitrary updating matrix $\mathbf w$,
direct optimization of Eq. \eqref{eq:error_asym1}
often leads to a complicated expression for $\alpha(t)$.
%
In practice, it is also useful to find
a simple expression for $\alpha(t)$
that is optimal with respect to some free parameter(s).
%
From Eq. \eqref{eq:alpha_opt},
a reasonable candidate would be
%
\begin{equation}
\alpha(\tau) = \frac{1}{\lambda \, (\tau + t_0) },
\label{eq:alpha_invtlambda}
\end{equation}
%
for some value of $\lambda$.
%
Below we shall assume this functional form
and find the optimal value
in some cases.
%
We shall show that the optimal $\lambda$
generally depends on not only the spectrum
of eigenvalues and the sampling method,
but also other parameters,
such as the simulation length, $t$.

Using Eq. \eqref{eq:alpha_invtlambda}
in Eq. \eqref{eq:error_asym1} yields
%
\begin{align}
\Err_A
&=
\frac{    1    }
     { t + t_0 }
\sum_{k = 2}^n
  \frac{ \Gamma_k \, \nu_k^2 }
       {    2 \, \nu_k - 1   }
\left[
  1 - \left(
        1 + \frac{ t }{ t_0 }
      \right)^{1 - 2 \, \nu_k}
\right],
\label{eq:error_asym_invt}
\end{align}
%
where $\nu_k \equiv \lambda_k / \lambda$.
%
At long times, we get
$$
\begin{aligned}
  \Err_A
  \stackrel{         t \to \infty         }
           { =\joinrel=\joinrel=\joinrel= }
  \sum_{k = 2}^n
  \begin{dcases}
    \frac{    1    }
         { t + t_0 }
    \frac{ \Gamma_k \, \nu_k^2 }
         {   2 \, \nu_k - 1    }
    &
    \mathrm{if \;} 2 \, \nu_k > 1,
    \\%[1em]
    %
    %
    \frac{    \Gamma_k    }
         { 4 \, (t + t_0) }
    \ln \frac{ t + t_0 }
             {   t_0  }
    &
    \mathrm{if \;} 2 \, \nu_k = 1,
    \\
    %
    %
    \frac{  t_0^{ 2 \, \nu_k  - 1}  }
         { (t + t_0)^{ 2 \, \nu_k } }
    \frac{ \Gamma_k \, \nu_k^2 }
         {   1 - 2 \, \nu_k    }
    &
    \mathrm{if \;} 2 \, \nu_k < 1.
  \end{dcases}
\end{aligned}
$$
%
The last two cases, $\lambda \ge 2\lambda_k$,
are asymptotically suboptimal
as they decay slower than $1/t$.
%
Thus, the optimal $\lambda$
satisfies $\lambda < 2 \min \lambda_k$,
and in this case,
we can seek that
the optimal value from
the smallest real root of
%
\begin{equation}
\sum_{k = 2}^n
\frac{ \lambda_k - \lambda }
{ \left(2 - \lambda/ \lambda_k \right)^2 }
= 0.
\label{eq:optimal_lambda_approx}
\end{equation}



Under the approximation, we have
$$
\frac{ \lambda_k^2 }{ \lambda \, (2 \, \lambda_k - \lambda) } \ge 1,
$$
with the equality achieved only at $\lambda = \lambda_k$,
we see the from Eq. \eqref{eq:error_asym_invt}
that the minimal error is achieved with
$\lambda_2 = \dots = \lambda_n = \lambda$,
agreeing with the general conclusion
in Sec. \ref{sec:optWL}.



In practice, however,
real simulations
may be relative short
and have not entered
the asymptotic regime.
%
Then, the residual error, $\Err_R$, is not negligible.
%
From Eq. \eqref{eq:error_res}, we get
%
\begin{equation}
\Err_R
=
\sum_{k = 2}^n
  \frac{ \Gamma_k \, \nu_k }
       {        2 \, t_0   }
  \left(
      \frac{   t_0   }
           { t + t_0 }
   \right)^{ 2 \, \nu_k },
\label{eq:error_res_invt}
\end{equation}
%
assuming that at $t = 0$
the system has achieved an equilibrium
at a constant $\alpha_0$,
such that the values of
$\left\langle y_k^2(0) \right\rangle$
can be computed from Eq. \eqref{eq:y2_eql}.
%
The total error is given by the sum of
Eqs. \eqref{eq:error_res_invt} and \eqref{eq:error_asym_invt}.
%
\begin{align}
\Err
&=
\Err_R + \Err_A
\notag
\\
&=
\sum_{ k = 2 }^n
  \Err^f_k \,
  \left\{
    1
    +
    \frac{ 1 - \left[ t_0 / (t + t_0) \right]^{2 \, \nu_k - 1} }
         {                 2 \, \nu_k - 1                      }
%    \frac{       1        }
%         { 2 \, \nu_k - 1 }
%    \left[
%      1 -
%      \left(
%        \tfrac{   t_0   }
%              { t + t_0 }
%      \right)^{2 \, \nu_k - 1}
%    \right]
  \right\}
,
\label{eq:error_invt}
\end{align}
%
where
$$
\Err^f_k
\equiv
\left\langle
  y_k^2
\right\rangle_{ \alpha(t) }
=
\frac{  \Gamma_k \, \nu_k   }
     {    2 \, (t + t_0)    }
,
$$
is the equilibrium error of mode $k$
under the constant terminal value
of $\alpha(t) = 1/[\lambda (t + t_0)]$.

\note{Derivation of Eq. \eqref{eq:error_invt}:
$$
\begin{aligned}
\Err_R
&=
\sum_{ k = 2 }^n
  \Err^f_k \left( \frac{   t_0   }
                       { t + t_0 }
          \right)^{ 2 \nu_k - 1 }
\\
\Err_A
&=
\sum_{ k = 2 }^n
  \frac{ 2 \, \Err^f_k \, \nu_k }
       {    2 \, \nu_k - 1      }
  \left[
    1 -
    \left(
      \tfrac{   t_0   }
            { t + t_0 }
      \right)^{2 \, \nu_k - 1}
  \right]
\\
&=
\sum_{ k = 2 }^n
  \Err^f_k \,
  \left[
    1 -
    \left(
      \tfrac{   t_0   }
            { t + t_0 }
      \right)^{2 \, \nu_k - 1}
  \right]
  +
  \frac{   \Err^f_k     }
       { 2 \, \nu_k - 1 }
  \left[
    1 -
    \left(
      \tfrac{   t_0   }
           { t + t_0 }
      \right)^{2 \, \nu_k - 1}
  \right].
\end{aligned}
$$
Adding the two yields Eq. \eqref{eq:error_invt}.
}



%We give several remarks on Eq. \eqref{eq:error_invt}.
%

Clearly, the final error is always greater than
the saturated equilibrium value at the terminal $\alpha(t)$:
$$
E \ge E_\mathrm{sat} \equiv \sum_{k = 2}^n E^f_k,
$$
because the second term in the braces
is nonnegative no matter the sign of $2 \, \nu_k - 1$
(in fact, it is a decreasing function
that vanishes only at $\nu_k \to +\infty$).
%
For a set of nonidentical $\lambda_k$'s,
the equality is reached only at $t = 0$.



\subsubsection{\label{sec:optlambda}
Optimal parameter $\lambda$}


We shall show that the optimal $\lambda$
depends on the simulation length,
and it tends to decrease as simulation lengthens.
%
Let us first rewrite Eq. \eqref{eq:error_invt} as
%
$$
\Err
=
\sum_{ k = 2 }^n
  \frac{    \Gamma_k    }
       { 2 \, (t + t_0) }
  G\left( \nu_k, \frac{ t_0 } { t + t_0} \right),
$$
%
where the $k$th component is proportional to
%
$$
G\left( \nu_k, r \right)
=
\nu_k
\,
\left(
  1
  +
  \frac{ 1 - r^{2 \, \nu_k - 1} }
       {   2 \, \nu_k - 1       }
\right).
$$
%
As shown in Fig. \ref{fig:err_component},
the function $G(\nu_k, r)$ depends non-trivially on $\nu_k$.
%
For a fixed and sufficiently small $r = t_0 / (t + t_0)$,
$G(\nu_k, r)$ has two local minima along $\nu_k$,
one at $\nu_k = 0$, and the other around $\nu_k = 1$,
with the former being deeper albeit narrower.
%
This means that for a relatively short simulation,
hence a relatively large value of $r$,
the minimal error may be achieved at $\nu_k \approx 0$,
corresponding to a large value of $\lambda$
($\lambda \gg \lambda_k$).
%
However, this minimum is unstable as simulation lengthens,
for $G(\nu_k, r)$ increases rapidly
with decreasing $r$ under a fixed $\nu_k$.
%
In other words,
asymptotically, the minimal error is always achieved at
$\nu_k = 1$ or $\lambda = \lambda_k$.
%
In this case,
$G(\nu_k, r) \to G(1, 0) = 2$
and the total error is roughly twice $E_\mathrm{sat}$.


For an arbitrary spectrum of eigenvalues,
$\lambda_2, \dots, \lambda_n$,
we expect that the optimal $\lambda$ to shift gradually to
smaller values as the simulation length, $t$, increasing.
%
Asymptotically, we expect the optimal $\lambda$
to be around $\min_k \lambda_k$.


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig/errcomp.pdf}
  }
  \caption{
    \label{fig:err_component}
    Component of the error, $G(\nu_k, r)$,
    versus $\nu_k = \lambda_k / \lambda$,
    where $r = t_0 / (t + t_0)$.
    %
  }
\end{center}
\end{figure}






\subsection{\label{sec:band-matrix}
Homogeneous updating schemes}



In practice, we usually use a homogeneous updating scheme
that is translational invariant.
%
We shall study these schemes below.


\subsubsection{
Eigenvalues and stability}


Consider the following matrix $\mathbf w$
%
\begin{equation}
  w_{ij}
  =
  \mu_{ |i - j| }
  + \mu_{ i + j - 1 }
  + \mu_{ 2 n + 1 - i - j },
  \label{eq:w_band}
\end{equation}
%
where $\mu_0, \dots, \mu_b \, (b < n)$
are a set of numbers satisfying
%
\begin{equation}
\mu_0 + 2 \, \mu_1 + \cdots + 2 \, \mu_b = 1,
\label{eq:m_normalization}
\end{equation}
%
and $\mu_l = 0$ for $l > b$.
%
The matrix is translational invariant under the reflective
boundary condition\cite{bussi2006}.
%
On the right-hand side of Eq. \eqref{eq:w_band},
the first term represents the relative updating magnitude
to a neighbor that is $|i - j|$ bins away,
whereas the next two terms are added
%according to the boundary condition
to avoid unintended distortion
to the equilibrium distribution $p_i = 1/n$\cite{bussi2006}.
%
Similarly, if $i$ represents a periodic variable,
the last two terms can be dropped, and
$|i-j|$ is evaluated
under the minimum image convention\cite{dama2014}.
This case is, however, not considered further.


This matrix, being symmetric to the indices $i$ and $j$,
satisfies Eqs. \eqref{eq:w_sumj}-\eqref{eq:w_balance},
for the flat distribution $p_i = 1/n$.
%
It has $n$ orthonormal eigenvectors,
$\pmb\varphi^{(1)}, \dots, \pmb\varphi^{(n)}$,
given by
%
\begin{equation}
\varphi^{(k)}_i
= \sqrt{
    \frac{ 2 - \delta_{k, 1} }
         {       n           }
       }
  \cos \frac{ \left( i - \frac 1 2 \right) (k - 1) \, \pi}
            {                  n                         }.
\label{eq:wband_eigenvector}
\end{equation}
%
with eigenvalues
%
\begin{align}
  \lambda_k
  &=
  \mu_0 + \sum_{l = 1}^b 2 \, \mu_l \cos \frac{(k - 1)  \, l \pi}{n}
  \label{eq:wband_eigenvalue1}
  \\
  &=
  1 - \sum_{l = 1}^b 4 \, \mu_l \sin^2 \frac{(k - 1)  \, l \pi}{ 2 \, n }
  .
  \label{eq:wband_eigenvalue}
\end{align}

\note{We derive Eqs.
  \eqref{eq:wband_eigenvector} and \eqref{eq:wband_eigenvalue}
  below.

  To simplify the calculation,
  let us first consider the unnormalized eigenvectors,
  $$
  \theta^{(k)}_i
  =
  \cos \frac{ \left( i - \frac 1 2 \right) (k - 1) \, \pi}{n},
  $$
  which satisfies the following reflection symmetry,
  \begin{equation}
    \theta^{(k)}_i = \theta^{(k)}_{1 - i} = \theta^{(k)}_{ 2 n + 1 - i }.
    \label{eq:wband_reflection_symmetry}
  \end{equation}
  %
  We shall also define $\mu_l = 0$, for $l \le 0$, then
$$
\begin{aligned}
&\sum_{i = 1}^n
  \theta^{(k)}_i
  \, w_{ij}
=
- \theta^{(k)}_j \, \mu_0
\\
&
+\sum_{i = 1}^n
  \theta^{(k)}_i
  \left(
    \mu_{i - j} + \mu_{j - i} + \mu_{i + j - 1} + \mu_{2 \, n + 1 - i - j}
  \right)
.
\end{aligned}
$$

Now
$$
\begin{aligned}
  &
  \sum_{i = 1}^n
  \theta^{(k)}_i \,
  \left(
    \mu_{i - j} + \mu_{ 2 n + 1 - i - j }
  \right)
  \\
  &=
  \sum_{l=1-j}^{n-j} \theta^{(k)}_{j+l} \, \mu_l
  +
  \sum_{l = n-j+1}^{2n-j} \theta^{(k)}_{2n+1-j-l} \, \mu_l
  \\
  &=
  \sum_{l=1-j}^{2n-j} \theta^{(k)}_{j+l} \, \mu_l
  =
  \sum_{l=0}^{b} \mu_l \, \cos\frac{(j+l-\frac12)(k-1)\pi}{n},
\end{aligned}
$$
where we have used Eq. \eqref{eq:wband_reflection_symmetry}.
%
Similarly,
$$
\begin{aligned}
  \sum_{i = 1}^n
  \theta^{(k)}_i
  \left(
    \mu_{j - i} + \mu_{i+j-1}
  \right)
  =
  \sum_{l=0}^{b} \mu_l \, \cos\frac{(j-l-\frac12)(k-1)\pi}{n}.
\end{aligned}
$$
%
So
$$
\begin{aligned}
\sum_{i = 1}^n \theta^{(k)}_i \, w_{ij}
&=
- \theta^{(k)}_j \, \mu_0
+\sum_{l=0}^{b} \mu_l \, 2 \, \cos\frac{(k-1)l\pi}{n} \, \cos\frac{(j-\frac12)(k-1)\pi}{n}
\\
&= \theta^{(k)}_j \, \lambda_k,
\end{aligned}
$$
with $\lambda_k$ given by Eq. \eqref{eq:wband_eigenvalue}.

Next, let us compute the normalization factor.
%
If $k = 1$, $\theta^{(1)}_i = 1$, and
$
\sum_{i = 1}^n \left( \theta^{(1)}_i \right)^2 = n.
$
For $k \ge 2$,
$$
\begin{aligned}
  \sum_{i = 1}^n \cos^2 \left[\left(i - \frac1 2 \right) a\right]
  &=
  \frac n 2
  +
  \frac 1 2
  \sum_{i = 1}^n \cos\left[(2 i - 1)\, a \right]
  =
  \frac n 2,
\end{aligned}
$$
where $a = \frac{k-1}{n} \pi$.
Thus, the normalization factor $\sqrt{(2 - \delta_{k1})/n}$
encompasses both cases.

We can also show that column-wise orthogonality.
%
For this we need a lemma, for $a = q \, \pi/n$,
with $q$ a positive integer.
$$
\begin{aligned}
\sum_{k = 1}^n \cos[(k - 1) \, a]
&=
1 + \cos a + \dots + \cos[(n - 1) \, a]
\\
&=
\frac{
      \sin\frac a 2
    + \sin \left[ \left( n - \frac 1 2 \right) a \right]
    }
    {
      2 \, \sin \frac a 2
    }
\\
&=
\frac{ 1 - (-1)^q } { 2 }
= \operatorname{odd}(q),
\end{aligned}
$$
%
where, we have used
$\sin \left[ \left( n - \frac 1 2 \right) a \right]
= \sin \left( q \, \pi - \frac a 2 \right)
= -(-)^q\sin\frac a 2.$
%
and $\operatorname{odd}(q)$
yields $1$ for odd $q$ or $0$ otherwise.
%
Adding the case of $q = 0$, where the sum yields $n$,
we get
$$
\sum_{k = 1}^n \cos\left[(k - 1) \, \frac { q \, \pi } { n }  \right]
= n \delta_{q, 0} + \operatorname{odd}(q).
$$


Now, for the orthogonality,
$$
\begin{aligned}
  \sum_{k = 1}^n
  \theta^{(k)}_i \, \theta^{(k)}_j
  &=
  \frac 1 2
  \sum_{k = 1}^n
  \left[
    \cos \tfrac{ (i - j) (k - 1) \, \pi }
               {         n              }
    +
    \cos \tfrac{ (i + j - 1) (k - 1) \, \pi }
               {             n              }
  \right]
  \\
  &=
  \frac 1 2
  \left[
    n \, \delta_{i, j}
    +
    \operatorname{odd}(i - j)
    +
    \operatorname{odd}(i + j - 1)
  \right]
  \\
  &=
  \frac n 2 \, \delta_{i, j}
  + \frac 1 2.
\end{aligned}
$$
where we have used the fact
that $i - j$ shares the parity with $i + j$
in the last step.
%
So
$$
  \sum_{k = 1}^n
  \varphi^{(k)}_i \, \varphi^{(k)}_j
  =
  \frac 2 n
  \sum_{k = 1}^n
  \theta^{(k)}_i \, \theta^{(k)}_j
  -
  \frac 1 n
  =
  \delta_{i, j}.
$$
}
%



We can define a discrete kernel function from the set of numbers as
$$
\hat \mu_i = \mu_{ |i| }.
$$
Then, according to Eq. \eqref{eq:vt_diffeq_mbin},
the rate of change of the bias potential, $\dot v_i(t)$,
is given by a convolution of the above kernel, $\hat \mu_j$,
and the normalized histogram, $h_j/p_j$\cite{bussi2006},
and Eq. \eqref{eq:wband_eigenvalue1}
shows that the eigenvalues, $\lambda_k$, of the updating scheme
is the Fourier transform of the kernel.
%
So, for the updating scheme to be stable,
the Fourier transform of the kernel
must be nonnegative.

We discuss two special cases below.



\subsubsection{Nearest-neighbor updating scheme}



Particularly, if $\mu_2 = \dots = \mu_b = 0$,
the matrix $\mathbf w$ is tridiagonal,
%%
%\begin{equation}
%\arraycolsep=3.6pt\def\arraystretch{1.4}
%\mathbf w
%=
%\left(
%  \begin{array}{cccccccc}
%    1 - \mu_1   & \mu_1 & 0 & \dots & 0 \\
%    \mu_1 & 1 - 2 \, \mu_1  & \mu_1 & \dots & 0 \\
%    \vdots & &  & & \vdots \\
%    0 & \dots & \mu_1 & 1 - 2 \, \mu_1  & \mu_1 \\
%    0 & \dots & 0 & \mu_1 & 1 - \mu_1
%  \end{array}
%\right).
%\label{eq:wnn}
%\end{equation}
%%
%We have
and the eigenvalues are
\begin{equation}
  \lambda_k = 1 - 4 \, \mu_1 \sin^2 \frac{(k - 1) \, \pi}{2n}.
\label{eq:wnn_eigenvalue}
\end{equation}
%
Thus, for this updating matrix to be stable,
we need
$\min \lambda_k = \lambda_n > 0$,
and
$$
\mu_1 < \frac{1}{4 \, \cos^2\frac{\pi}{2n} }.
$$



We can find an analytical expression
of the optimal value of $\lambda$
in the limit of large $n$.
%
In this case,
we convert Eq.
\eqref{eq:error_asym_invt}
%and
%\eqref{eq:optimal_lambda_approx}
to an integral,
$\sum_i \to \frac{2 \, n}{\pi} \int dp$, and
%
\begin{align}
\Err_A
=
\frac{2 \, n}{\pi \, t}
\int_0^{\pi/2}
  \frac{ \Gamma(p) \, \left(1 - 4 \, \mu_1 \, \sin^2 p \right)^2    }
       {   \lambda \, \left(2 - 8 \, \mu_1 \, \sin^2 p - \lambda \right) }
\, dp.
\notag
%\label{eq:error_nn_asym_int}
\end{align}



For perfect sampling,
we have, from Eq. \eqref{eq:Gamma_perfect},
$\Gamma(p) = 1$, and
$$
\begin{aligned}
\Err_A
&=
\frac{2 \, n}{\pi \, t}
\int_0^{\pi/2}
\frac{ \left(1 - 4 \, \mu_1 \, \sin^2 p \right)^2 }
{ \lambda \, \left(2 - 8 \, \mu_1 \, \sin^2 p - \lambda \right) }
\, dp
\notag \\
&=
\frac{n}{4 \, t}
\left(
  \frac{2 - 4 \, \mu_1 + \lambda}{ \lambda }
  +
  \frac{ \lambda }
  { \sqrt{ (2 - \lambda) (2 - 8 \, \mu_1 - \lambda) } }
\right).
\end{aligned}
$$
\note{The integral is evaluated by contour integration.
%
It is somewhat more convenient to change variable $p \to \frac{ \pi } { 2 } - p$,
and
$$
\begin{aligned}
\Err_A
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\int_0^{2 \, \pi}
\frac{ \left(1 - 4 \, \mu_1 \, \cos^2 p \right)^2 }
{ 2 - 8 \, \mu_1 \, \cos^2 p - \lambda }
\, dp
\\
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\oint
\frac{ \left[1 - \mu_1 \, \left(z+\frac{1}{z}\right)^2 \right]^2 }
{ 2 - 2 \, \mu_1 \, \left(z + \frac{1}{z}\right)^2 - \lambda }
\, \frac{dz}{z},
\end{aligned}
$$
where the contour is the unit circle
in the complex plane of $z$.
%
This integral has five poles, one at the origin,
which produces a residue $\frac{2 - 4 \, \mu_1 - \lambda}{4}$
(from series expansion of small $z$),
and the other four at
$$
z + \frac{1}{z} = \pm\sqrt\frac{2-\lambda}{2 \, \mu_1},
$$
with two of them inside the unit circle:
$$
z_\pm = \pm \frac{\sqrt{2-\lambda} -\sqrt{2 - 8 \, \mu_1 - \lambda}}
{\sqrt{8 \mu_1}}.
$$
Thus,
$$
\begin{aligned}
\Err_A
&=
\frac{n}{\lambda \, t}
\left(
 \frac{2 - 4 \, \mu_1 - \lambda}{4}
 +
 \sum_{z = z_{\pm} }
 \frac{ \left(z^2 - \mu_1 (1 + z^2)^2 \right)^2 }
 { 2 z^4 (2 - 4 \mu_1 - \lambda - 4 \mu_1 z^2) }
\right)
\\
&=
\frac{n}{\lambda \, t}
\left(
  \frac{2 - 4 \, \mu_1 - \lambda}{4}
 +
 \frac{ \left(1 - \mu_1 \left(z + \frac{1}{z} \right)^2 \right)^2 }
 { 2 - 4 \mu_1 - \lambda - 4 \mu_1 z_{\pm}^2 }
\right)
\\
&=
\frac{n}{\lambda \, t}
\left(
  \frac{2 - 4 \, \mu_1 - \lambda}{4}
 +
 \frac{ (\lambda/2)^2 }
 { \sqrt{(2-\lambda) (2 - 8 \mu_1 -\lambda)} }
\right).
\end{aligned}
$$
}
%
Minimizing the function yields
%
\begin{equation}
\lambda = \frac{1 - 4 \, \mu_1} { 1 - 2 \, \mu_1 },
\label{eq:lambda_nn_perfect}
\end{equation}
%
and the asymptotic error in the optimal case is given by
%
\begin{equation}
\Err_A
=
\frac{n}{t}
\left(
  1+ \frac{2 \, \mu_1^2}{1-4 \, \mu_1}
\right).
\label{eq:error_nn_prefect}
\end{equation}
%
%From Eq. \eqref{eq:error_nn}, it is clear that
Clearly, the error
increases with the magnitude of $\mu_1$
no matter its sign,
and the minimal value, obtained at $\mu_1 = 0$,
corresponds to the single-bin scheme.
%
Note that we have ignored the residual error
and assumed $\lambda < 2 \, \min \lambda_k = 2 - 8 \, \mu_1$
for all $k$ in the above solution.
%
Fortunately,
the $\lambda$ from Eq. \eqref{eq:lambda_nn_perfect}
satisfies this condition
by the stability condition,
$4 \, \mu_1 < 1$.



For the one-step sampling,
we find from Eq. \eqref{eq:Gamma_onestep}
that $\Gamma(p) = \cot^2 p$
is peaked around $p \approx 0$,
%
Then
$$
\begin{aligned}
\Err_A
&
\propto
\frac{   2 \, n }
     { \pi \, t }
\left.
\frac{            \left(1 - 4 \, \mu_1 \, \sin^2 p \right)^2         }
     { \lambda \, \left(2 - 8 \, \mu_1 \, \sin^2 p - \lambda \right) }
\right|_{ p \approx 0 }
\notag \\
&
\approx
\frac{   2 \, n }
     { \pi \, t }
\frac{             1             }
     {  \lambda \, (2 - \lambda) }
.
\end{aligned}
$$
%
The optimal $\lambda$ is therefore
%
\begin{equation}
\lambda \approx 1.
\label{eq:lambda_nn_onestep}
\end{equation}
%

\note{We can be a little more precise here.
%
From Eq. \eqref{eq:Gamma_onestep}, we get
%
$$
\Gamma(p) = \cot^2 p \approx \cos^2 p / (\sin^2 p + \delta^2),
$$
%
where we have introduced
$\delta \propto \sin[ \pi / (2 \, n) ] \sim 1/n$
as a small parameter to avoid the divergence
around the origin.
%
$$
\begin{aligned}
\Err_A
&=
\frac{   2 \, n }
     { \pi \, t }
\int_0^{\pi/2}
    \frac{            \left(1 - 4 \, \mu_1 \, \sin^2 p \right)^2         }
         { \lambda \, \left(2 - 8 \, \mu_1 \, \sin^2 p - \lambda \right) }
    \frac{ \cos^2 p }
         { \sin^2 p + \delta^2 }
\, dp
\notag \\
&
\stackrel{    \delta \ll 1     }
         { =\joinrel=\joinrel= }
\frac{   2 \, n }
     { \pi \, t }
\frac{             1             }
     {  \lambda \, (2 - \lambda) }
\frac{    1   }
     { \delta }
.
\end{aligned}
$$
The integral is evaluated as follows.
%
We again change variable $p \to \frac{ \pi } { 2 } - p$,
and
$$
\begin{aligned}
\Err_A
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\int_0^{2 \, \pi}
\frac{ \left(1 - 4 \, \mu_1 \, \cos^2 p \right)^2 }
     {       2 - 8 \, \mu_1 \, \cos^2 p - \lambda }
\cdot
\frac{ \sin^2 p            }
     { \cos^2 p + \delta^2 }
\, dp
\\
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\oint
\frac{ \left[1 -      \mu_1 \, \left(z + \frac{1}{z}\right)^2 \right]^2 }
     {       2 - 2 \, \mu_1 \, \left(z + \frac{1}{z}\right)^2 - \lambda }
\cdot
\frac{ 4 - \left( z + \frac 1 z \right)^2 }
     { \left( z + \frac 1 z \right)^2 + 4 \, \delta^2 }
\, \frac{dz}{z}
\\
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\oint
\frac{ \left[z^2 -      \mu_1 \, (z^2 + 1)^2 \right]^2   }
     {  (2 - \lambda) \, z^2 - 2 \, \mu_1 \, (z^2 + 1)^2 }
\cdot
\frac{ 4 \, z^2 - ( z^2 + 1 )^2 }
     { ( z^2 + 1 )^2 + 4 \, \delta^2 \, z^2 }
\, \frac{ dz }{ z^3 }
\\
&=
\frac{ n } { \lambda \, t }
\sum_l \operatorname{Res}_l
,
\end{aligned}
$$
where the contour is the unit circle
in the complex plane of $z$,
and the integral is reduced to a sum of residues
around the poles within.

The poles of the integral come from three sources,
namely,
% 1.
$z = 0$,
% 2.
the zeros of
$(2 - \lambda) \, z^2 - 2 \, \mu_1 \, (z^2 + 1)^2$,
% 3.
and the zeros of
$( z^2 + 1 )^2 + 4 \, \delta^2 \, z^2$.

For the first pole around the origin,
we need to expand the integrand as
$$
\frac{ A + B \, z^2 + \cdots }
     {          z^3          },
$$
then the residue is $B$.
%
Since
$$
\begin{aligned}
\left[ z^2 - \mu_1 (z^2 + 1)^2 \right]^2
&
\approx
[ - \mu_1 + (1 - 2 \, \mu_1) \, z^2]^2
\\
&
\approx
\mu_1^2
\left[
  1 + \left(4 - \tfrac { 2  } { \mu_1 } \right) z^2
\right],
\\
%
4 \, z^2 - ( z^2 + 1 )^2
&
\approx
-( 1 - 2 \, z^2 ),
\\
%
(2 - \lambda) \, z^2 - 2 \, \mu_1 \, (z^2 + 1)^2
&
\approx
-2 \, \mu_1 \,
\left[
  1 + \left(
        2 + \tfrac{ \lambda - 2 } { 2 \, \mu_1 }
      \right)
      \, z^2
\right],
\\
%
( z^2 + 1 )^2 + 4 \, \delta^2 \, z^2
&
\approx
1 + (2 + 4 \, \delta^2) \, z^2,
\end{aligned}
$$
the integrand is
$$
\begin{aligned}
\frac{ \mu_1 }
     {   2   }
\left[
  1 + \left(
        - \frac{ 2 + \lambda } { 2 \, \mu_1 }
        - 2 - 4 \, \delta^2
      \right) \, z^2
\right].
\end{aligned}
$$
and the residue is
$$
\operatorname{Res}_1
=
  - \frac{ 2 + \lambda } { 4 }
  - \mu_1 \, \left( 1 + 2 \, \delta^2 \right).
$$


For the second part,
we have four roots of
$$
(2 - \lambda) \, z^2 = 2 \, \mu_1 \, (z^2 + 1)^2,
$$
which are
\begin{equation}
z
=
\pm
\sqrt { \frac{ 2 - \lambda }
             { 8 \, \mu_1  } }
\pm
\sqrt { \frac{ 2 - \lambda }
             { 8 \, \mu_1  } - 1 }.
\label{eq:nnint_local_zeros_part2}
\end{equation}
We share distinguish two cases.
\textbf{Case A.}
If $\lambda < 2 - 8 \, \mu_1$,
two of the roots lie in the unit circle, namely
$$
z
=
\pm
\left(
\sqrt { \frac{ 2 - \lambda }
             { 8 \, \mu_1  } }
-
\sqrt { \frac{ 2 - \lambda }
             { 8 \, \mu_1  } - 1 }
\right),
$$
which satisfy
$$
4 \, \mu_1 \, ( z^2 + 1)
=
2 - \lambda
-
\sqrt{ ( 2 - \lambda ) ( 2 - \lambda - 8 \, \mu_1 ) }.
$$
Thus,
$$
\begin{aligned}
\left[ z^2 - \mu_1 \, \left( z^2 + 1 \right)^2 \right]^2
&=
\left( z^2 - \tfrac{ 2 - \lambda } { 2 } z^2 \right)^2
=
\frac{ \lambda^2 } { 4 } z^4.
\\
%
4 z^2 - \left( z^2 + 1 \right)^2
&=
-\frac{ 2 - \lambda - 8 \, \mu_1  } { 2 \, \mu_1 } z^2,
\\
%
\left( z^2 + 1 \right)^2 + 4 \, \delta^2 \, z^2
&=
\frac{ 2 - \lambda + 8 \, \mu_1 \, \delta^2 }
     { 2 \, \mu_1 }
     z^2,
\\
%
(2 - \lambda) \, 2 \, z
-
8 \, \mu_1 \, \left( z^2 + 1 \right) \, z
&=
2 \, z \, \left[2 - \lambda - 4 \,  \mu_1 \, (z^2 + 1) \right]
\\
&= 2 \, z \, \sqrt{ (2 - \lambda) ( 2 - \lambda - 8 \, \mu_1 ) },
\end{aligned}
$$
where we have used the L'H\^{o}pital's rule
and taken the derivative for the last factor.
%
Note that the two roots within the unit share the same
residue and the sum for this part is
%
$$
\operatorname{Res}_2
=
-\frac{                 \lambda^2                     }
      { 4 \, ( 2 + 8 \, \mu_1 \, \delta^2 - \lambda ) }
\sqrt{ 1 - \frac{ 8 \, \mu_1 } { 2 - \lambda } }.
$$
%
\textbf{Case B.}
If $2 - 8 \, \mu_1 \le \lambda < 2$,
the four zeros in Eq. \eqref{eq:nnint_local_zeros_part2}:
%
\begin{equation}
z
=
\pm
\sqrt { \frac{ 2 - \lambda }
             { 8 \, \mu_1  } }
\pm i \,
\sqrt { 1 - \frac{ 2 - \lambda }
                 { 8 \, \mu_1  } },
\label{eq:nnint_local_zeros_part2b}
\end{equation}
%
all satisfy $|z| = 1$, and hence
all lie on the border of the unit circle.
%
Thus, each contributes half of the residue there.
%
However, since the four zeros satisfy
$$
2 - \lambda - 4 \, \mu_1 \, ( z^2 + 1)
=
\pm i
\sqrt{ ( 2 - \lambda ) ( 8 \, \mu_1 - 2 + \lambda ) },
$$
the sum of the four residues gives zero in this case:
$$
\operatorname{Res}_2 = 0.
$$
In summary,
$$
\operatorname{Res}_2
=
\begin{dcases}
-\frac{                 \lambda^2                     }
      { 4 \, ( 2 + 8 \, \mu_1 \, \delta^2 - \lambda ) }
\sqrt{ 1 - \frac{ 8 \, \mu_1 } { 2 - \lambda } }
&
\mathrm{if \;}
\lambda < 2 - 8 \, \mu_1,
\\
0
&
\mathrm{otherwise}.
\end{dcases}
$$


For the last part,
we have four roots of
$(z^2 + 1)^2 = - 4 \, \delta^2 \, z^2$,
with two of them lying in the unit circle:
$$
z = \pm i \left( \sqrt{ 1 + \delta^2 } - \delta \right),
$$
satisfying
$$
z^2 + 1 + 2 \, \delta^2 = 2 \, \delta \, \sqrt{1 + \delta^2}.
$$
Then,
$$
\begin{aligned}
z^2 - \mu_1 \, \left( z^2 + 1 \right)^2
&= z^2 \, (1 + 4 \, \mu_1 \, \delta^2),
\\
4 \, z^2 - \left( z^2 + 1 \right)^2
&=
4 \, z^2 \, (1 + \delta^2)
\\
%
(2 - \lambda) \, z^2
- 2 \, \mu_1 \, \left( z^2 + 1 \right)^2
&=
\left(
  2 - \lambda + 8 \, \mu_1 \, \delta^2
\right) \, z^2
\\
%
4 \, z \, \left( z^2 + 1 \right)
+
8 \, \delta^2 \, z
&=
4 \, z \, (z^2 + 1 + \delta^2)
=
8 \, z \, \delta \, \sqrt{ 1 + \delta^2 }.
\end{aligned}
$$
Note that the above two roots share the same
residue and the sum for this part is
$$
\operatorname{Res}_3
=
\frac{ \left( 1 + 4 \, \mu_1 \, \delta^2 \right)^2 }
     {        2 + 8 \, \mu_1 \, \delta^2 - \lambda }
\frac{ \sqrt{ 1 + \delta^2 } }
     {        \delta         }.
$$

Now as $\delta \to 0$, the last contribution will dominate,
and we have
$$
\Err_A
\approx
\frac{ n } { t }
\frac{ (1 + 4 \, \mu_1 \, \delta^2)^2 }
{ \lambda \, (2 + 8 \, \mu_1 \, \delta^2 - \lambda) }
\frac{ 1 } { \delta }
\approx
\frac{ n } { t }
\frac{ 1 }
{ \lambda \, (2 - \lambda) }
\frac{ 1 } { \delta },
$$
and the minimum is reached at $\lambda \approx 1$.

The weakness of the above calculation is two fold.
%
First, it ignores the residual error.
%
Second, the simplified expression for the asymptotic error
$E_A$ is also misused.
%
Namely, with $\lambda \approx 1$,
we cannot assume $\lambda < 2 \, \lambda_k$ in general.
%
So it only works for a very small value of $\mu_1$.
}



Note that Eq. \eqref{eq:lambda_nn_onestep} is only good
for sufficiently short simulations.
%
For very longer simulations,
the optimal $\lambda$ tends to shift
toward the smallest eigenvalue,
$\min \lambda_k = 1 - 4 \, \mu_1$
according to the discussion in Sec. \ref{sec:optlambda}.
%



\subsubsection{Gaussian updating scheme}


The Gaussian updating kernel is commonly
adopted by metadynamics simulations.
%
Below we show that the resulting updating scheme
is stable, albeit less effective
in reducing short-wavelength noises.



We start by defining
a continuous limit
for $b = n - 1 \gg 1$:
$x = l \, \Delta x$,
and
$\mu_l = \mu(x) \, \Delta x$,
where
$\Delta x = \pi/n$
is the bin size.
%
Then
Eq. \eqref{eq:wband_eigenvalue}
can be approximated by an integral:
%
\begin{equation}
\lambda_k
=
2 \int_0^\pi
  \mu(x) \, \cos \bigl[ (k-1) \, x \bigr] \, dx,
\label{eq:lambda_int}
\end{equation}
%
with the normalization
%
\begin{equation}
1 = 2 \int_0^\pi \mu(x) \, dx.
\label{eq:mx_normalization}
\end{equation}



Particularly,
for the Gaussian updating kernel,
if $\Delta x \ll \sigma \ll 1$,
we can extended
the upper limit of the integrals
in Eqs. \eqref{eq:lambda_int}
and \eqref{eq:mx_normalization}
to infinity, and
%
\begin{equation}
\mu(x)
=
\frac{            1            }
     { \sqrt{ 2 \pi \sigma^2 } }
%
\exp\left(
      -\frac{       x^2     }
            { 2 \, \sigma^2 }
    \right),
\end{equation}
%
%
The eigenvalues are given by
%
\begin{align}
\lambda_k
&=
\exp\left[
      -\frac{ (k - 1)^2 \, \sigma^2 }
            {           2           }
    \right]
\notag
\\
&=
\exp\left[
      -\frac{ \pi^2 }{ 2 }
      \left(
        \frac{ k - 1 }
             {   n   }
      \right)^2
      \left(
        \frac{  \sigma }
             { \Delta x }
      \right)^2
    \right]
.
\label{eq:lambda_Gaussian}
\end{align}
%
The eigenvalues are all positive,
suggesting stability of the updating scheme.


%However,
%as $\sigma/\Delta x \gg 1$,
%the smallest eigenvalue
%%
%$$
%\lambda_n
%\approx
%\exp\left(
%      -\frac{ n^2 \, \sigma^2 }
%            {        2        }
%    \right)
%=
%\exp\left[
%      -\frac{ \pi^2 }{ 2 }
%      \left(
%        \frac{  \sigma }
%             { \Delta x }
%      \right)^2
%    \right]
%.
%$$
%%
%is exponentially small.
%%
%This means that
%the optimal value of $\lambda$
%given by Eq. \eqref{eq:optimal_lambda_approx}
%would be too small in practice,
%for it requires $\lambda < 2 \, \lambda_n$.
%%
%In other words,
%with a reasonable $\lambda$,
%the error of the last few fluctuation modes
%will always decay suboptimally,
%accordingly to Eq. \eqref{eq:error_asym_invt}.
%%
%However, since these modes
%represent short wavelength fluctuations,
%and these modes may be in
%the system under consideration.
%%
%Thus,
%we can truncate the sum of the error function
%in Eq. \eqref{eq:error_asym_invt}
%at some $k_{\max}$,
%which corresponds to a minimal length scale
%$l_{\min} = \Delta x \, (n /k_{\max}) = \pi/k_{\max}$
%greater than $\sigma$.
%%
%Then,
%$$
%\lambda_{ k_{\max} }
%\approx
%\exp\left(
%      -\frac{ k_{\max}^2 \, \sigma^2 }
%            {           2            }
%    \right)
%=
%\exp\left(
%      -\frac{  \pi^2 \, \sigma^2    }
%            {    2 \, l_{\min}^2  }
%    \right).
%$$




\section{\label{sec:results}
Numerical results}


\subsection{Test system and simulation protocol}


We tested the above theory on a one-dimensional system
of $n = 100$ bins,
with a flat target distribution
$p_i = 1/n$.
%
Since in the asymptotic regime,
the distribution converges to $p_i$
no matter the intrinsic distribution, $p^*_i$,
we also set the latter to be flat for convenience.
%
Our goal is to observe how the error
of the bias potential defined in Eq. \eqref{eq:error_sum}
depends on the schedule function $\alpha(t)$
at the end of the simulation.



Initially,
we set the bias potential to zero,
$v_i \equiv 0$,
%
and let the system undergo a period of equilibration
during which the updating magnitude is fixed at $\alpha_0$.
%
After the equilibration,
we reset the origin of the time, $t$,
and started the schedule specified by
Eq. \eqref{eq:alpha_invtlambda}.
%
The goal is then to observe how the final error
depends on the parameter, $\lambda$.
%
Further, we choose $t_0 = 1/(\lambda \, \alpha_0)$
to smooth the transition from equilibration.




Since the error depends on the underlying sampling scheme,
we have applied the above protocol to two
sampling schemes.
%
In both cases, we used the
Metropolis algorithm\cite{metropolis1953, newman, frenkel}
to sample the state space:
%
in each step, a new bin index $j$ was proposed
and then accepted with probability
%
$
A(i \to j) = \min\{ 1, \exp(v_i - v_j) \}.
$
The difference is that,
in the first sampling scheme,
we choose $j$ randomly out of the $n$ bins
to emulate the perfect sampling scheme
discussed in Sec. \ref{sec:Gamma}.
%
We refer to this as the global sampling scheme.
%
In the second scheme,
we limit $j$ to the two neighbors of $i$,
$i \pm 1$, with equal probability $1/2$,
to mimic the one-step sampling scheme
in Sec. \ref{sec:Gamma}.
%
This scheme models local sampling processes
typical in MD and straightforward MC simulations.
%
We refer to this as the local sampling scheme,



\subsection{Single-bin (WL) updating scheme}



We first tested the single-bin (WL) updating scheme
using the above protocol.
%
Here, we chose $\alpha_0 = 0.0001$
and ran the simulation for $t = 10^8$ steps.
%
As shown in Fig. \ref{fig:err_singlebin},
the numerical results of the terminal error agreed well with
the analytical prediction by Eq. \eqref{eq:error_invt}.
%
For both global and local sampling schemes,
the optimal $\lambda$ occurred around $1.0$,
the optimal value in the asymptotic regime.
%
Recall that $1/\lambda$ in Eq. \eqref{eq:alpha_invtlambda}
represents the relative updating magnitude.
%
Figure \ref{fig:err_singlebin} also shows that
a smaller than optimal value of $1/\lambda$
tends to increase the error more rapidly
than a larger than optimal value,
suggesting that the latter (over-updating)
is generally preferable to the latter (under-updating)
when the optimal value is unknown.


Our final observation is that the error functions
in the two cases shared the same shape,
differing only by a multiplicative constant.
%
This again agreed with the theory.
%
Since all eigenvalues of the single-bin scheme were
the same, $\lambda_2 = \cdots = \lambda_n = 1$,
the error in Eq. \eqref{eq:error_invt}
depends on the sampling scheme only through
the sum $\sum_{ k = 2 }^n \Gamma_k$,
which gives rise to the above multiplicative constant.


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig/errsbin.pdf}
  }
  \caption{
    \label{fig:err_singlebin}
    Error, $E$, versus the
    relative updating magnitude,
    %proportionality constant,
    $1/\lambda$,
    in Eq. \eqref{eq:alpha_invtlambda}
    for the single-bin (WL) scheme.
    %
    The results have been averaged over $1000$ independent runs.
  }
\end{center}
\end{figure}



\subsection{Nearest-neighbor updating scheme}


We now turn to the nearest-neighbor updating scheme.
%
We again find good agreement
between simulation and analytical results,
although there are subtle differences.
%
A new feature is that the optimal $\lambda$
depends on the simulation length, $t$.
%
As shown in Fig. \ref{fig:err_nnbr},
the normalized error, $(t + t_0) \, \Err$,
hence the optimal value of $1/\lambda$,
depended non-trivially on the simulation length
in the global sampling case.
%
The optimal $1/\lambda$ gradually increased
with simulation length, which is in agreement with
the discussion in Sec. \ref{sec:optlambda}.
%
For the local sampling case, however,
the normalized error roughly maintained the sample shape,
and the optimal $\lambda$ is around $1.0$,
as suggested by Eq. \eqref{eq:lambda_nn_onestep}.



To understand this dependence,
we observe that for shorter simulations,
long-wavelength modes
(with larger $\lambda_k$ and smaller $k$),
dominate the error function in Eq. \eqref{eq:error_invt}
because of the prefactor $\lambda_k \, \Gamma_k$,
which is inherited from
the equilibrium error under a fixed updating magnitude,
Eq. \eqref{eq:error_eql}.
%
For longer simulations,
short-wavelength modes
(with smaller $\lambda_k$ and larger $k$)
become more important,
for $\lambda_k$ also gives the decay rate
of the $k$th mode.
%and errors in smaller $\lambda_k$ modes
%are harder to damp out.
%
This drives the optimal $\lambda$ to shift toward
a smaller $\lambda_k$ value
as the simulation lengthens.
%
Note also that while increasing $\lambda$
also increases the error of the long-wavelength modes,
this rate of increase is much smaller,
as shown in the $\nu_k > 1$ branch
in Fig. \ref{fig:err_component}.
%
The shift of $\lambda$, however, is less noticeable
under the local sampling scheme,
because longer-wavelength modes more heavily
weighted by much larger values of $\Gamma_k$.


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig/errnnbr.pdf}
  }
  \caption{
    \label{fig:err_nnbr}
    Normalized error, $(t + t_0) \, E$,
    versus the %proportionality constant,
    relative updating magnitude,
    $1/\lambda$,
    in Eq. \eqref{eq:alpha_invtlambda}
    for the nearest-neighbor updating scheme with $\mu_1 = 0.24$.
    %
    The results have been averaged over $1000$ independent runs.
  }
\end{center}
\end{figure}





\subsection{Gaussian (metadynamics) updating scheme}


The error of Gaussian updating scheme is similar to
that of the nearest neighbor scheme,
as shown in Fig \ref{fig:err_sig5}.
%
The dependency of the simulation length is more prominent.
%
Even for local sampling,
there is now a visible shift of the optimal $\lambda$.


\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=0.95\linewidth]{fig/errsig5.pdf}
  }
  \caption{
    \label{fig:err_sig5}
    Normalized error, $(t + t_0) \, E$,
    versus the %proportionality constant,
    relative updating magnitude,
    $1/\lambda$,
    in Eq. \eqref{eq:alpha_invtlambda}
    for the Gaussian updating scheme with $\sigma = 5$ bins.
    %
    The results have been averaged over $1000$ independent runs.
  }
\end{center}
\end{figure}




\subsection{Optimality of the single-bin scheme}

Randomize the initial error?

Error vs. $\mu_1$.

Error vs. $\sigma$.


\section{\label{sec:conclusion}
Conclusions and Discussions}


We verified the optimality of the inverse-time formula
for the Wang-Landau algorithm.

We have shown that
without a priori information,
the single-bin scheme is asymptotically
the best among all updating scheme
in terms of convergence.

For metadynamics, and other multiple-bin updating schemes,
we found formulas to predict the error for finite length
simulations and the optimal relative updating magnitude,
$1/\lambda$.


\section{Acknowledgments}

We thanks Dr. Y. Mei and Dr. J. Ma for helpful discussions.



\bibliography{simul}
\end{document}
