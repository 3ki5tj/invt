\documentclass[reprint, superscriptaddress, floatfix]{revtex4-1}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
%\usepackage{tikz}
%\usetikzlibrary{calc}
\usepackage{hyperref}
\usepackage{setspace}

\setcitestyle{super}

\hypersetup{
  colorlinks,
  linkcolor={red!30!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{64,0,42}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}
\newcommand{\answer}[1]{{\color{DarkBlue}\footnotesize \textsc{Answer.} #1}}
\newcommand{\summary}[1]{{\color{DarkPurple}\footnotesize \textsc{Summary.} #1}}


\newcommand{\Err}{E}
\newcommand{\ii}{\mathrm{i}}
%bin average
\newcommand{\bav}[1]{#1_\mathrm{av}}


\begin{document}



\title{Optimal updating magnitude in adaptive flat-distribution-sampling simulations}

\author{Cheng Zhang}
\author{Justin A. Drake}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\author{Jianpeng Ma}
\affiliation{
Department of Biochemistry and Molecular Biology,
Baylor College of Medicine, Houston, Texas 77030, USA}
\affiliation{
Department of Bioengineering,
Rice University, Houston, Texas 77005, USA}
\author{B. Montgomery Pettitt}
\email{mpettitt@utmb.edu}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}



\begin{abstract}
  We present a method of computing the optimal schedule
  of the updating magnitude
  for a class of flat-distribution sampling methods
  for free energy calculation including
  the Wang-Landau (WL) algorithm and metadynamics.
  %
  These methods rely on adaptive construction of
  a bias potential that offsets
  the potential of mean force by histogram-based adaptive updates.
  %
  The convergence of the bias potential can be improved
  by decreasing the updating magnitude with an optimal schedule.
  %
  We show that
  the optimal schedule depends on the updating scheme.
  %
  While the asymptotically optimal schedule for
  the single-bin updating scheme (commonly used in the WL algorithm)
  is given by the known inverse-time formula,
  that for the Gaussian updating scheme (commonly used in metadynamics)
  is often more complex.
  %and the initial updating magnitude is ideally half of
  %the previous equilibrium value.
  %
  Further,
  we show that the single-bin updating scheme
  is optimal for very long simulations,
  and it can be generalized to a class of bandpass updating schemes
  that are similarly optimal.
  %
  These bandpass updating schemes aim at
  partially flattening a few long-range histogram modes
  and their optimal schedule
  is also given by the inverse-time formula.
  %
  Constructed from orthogonal polynomials,
  bandpass updating schemes serve as generalizations
  of the Langfeld-Lucini-Rago algorithm
  and can be used for parameter tuning for the Gaussian ensemble method
  and umbrella sampling.
  %
\end{abstract}

\maketitle



\section{Introduction}



Free energy calculation\cite{frenkel, newman} is a central theme
in computational physics and chemistry
that can provide insight into an array of phenomena not easily studied
with regular simulation methods.
\note{``Regular simulation methods'' means MC and MD,
and they are more pertinent to ``computational physics and chemistry''
than ``traditional experiments.''}%
%
Given a system,
often the task is to compute
a distribution, $p(z)$,
along a collective variable, $z$, by sampling via either
Monte Carlo (MC)\cite{frenkel, newman, landau_binder}
or molecular dynamics (MD)\cite{frenkel, karplus2002}
simulations.
%
The distribution is related to
the dimensionless free energy, or potential of mean force (PMF),
along the collective variable, $z$,
as $-\ln p(z)$.
%
However, reliable estimation of the PMF is often difficult
when the collective variable is energetically restricted to local regions
of the complicated, multimodal free energy surface.
%
Thus,
to overcome these energetic barriers in sampling,
%and to capture the global shape of the free energy surface
an effective strategy is to introduce
a bias potential\cite{torrie1974, *torrie1977}
that offsets the PMF
such that the distribution along $z$ is
flat.\cite{mezei1987, berg1992, *lee1993,
wang2001, wang2001pre,
huber1994,
*laio2002, *laio2008, *barducci2011, *sutto2012}
%
\note{The flat-distribution sampling
allows one to more accurately capture the
global shape of the free energy surface
by improving the sampling around the rarely visited barrier region
and accelerating transitions among free energy basins.}



Many efficient flat-distribution-sampling (FDS) techniques
based on adaptive construction of the bias potential
have been introduced,
including the Wang-Landau (WL) algorithm\cite{
  wang2001, wang2001pre}
and metadynamics\cite{huber1994,
  *laio2002, *laio2008, *barducci2011, *sutto2012, micheletti2004}
among others\cite{yan2004, kim2006, *kim2007, kim2010, junghans2014,
  langfeld2012, pellegrini2014,
  maragliano2006, *abrams2008,
  zheng2010}.
%
These techniques regularly update the bias potential
to discourage future visits to previously sampled configurations
by incremental local elevation of the bias potential.
%
A key difference lies
in the updating window function,
which specifies
a neighborhood around the current value of
$z$ on the free energy surface
%where the updating should occur
as well as the relative updating magnitude.
%
The window function often
takes the form of a discrete
$\delta$-function (a boxcar function one histogram bin wide)
in the WL algorithm,
or that of a Gaussian
in metadynamics.\cite{junghans2014}
%
We will refer to the two updating schemes
as the single-bin
and Gaussian updating schemes, respectively.
%
While the former is popular for MC simulations,\cite{wang2001,
  wang2001pre, kim2006, *kim2007}
the latter is commonly used for MD simulations
as it provides a smoother bias potential
that can be readily differentiated
to derive the bias force.\cite{huber1994,
  *laio2002, *laio2008, *barducci2011, *sutto2012, junghans2014}
%
In both cases,
regular updates to the bias potential
disrupts the underlying equilibrium
sampling,\cite{zhou2005, morozov2007, zhou2008}
and the convergence of the bias potential
can often benefit from a gradually decreasing
updating magnitude of the bias potential.



The schedule of reducing the updating magnitude
affects the precision and accuracy
of the final bias potential,
and hence that of the PMF.\cite{
belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016,
liang2007,
morozov2007, morozov2009, zhou2008,
komura2012, *caparica2012, *caparica2014,
min2007,
barducci2008, dickson2011, dama2014, poulain2006}
%
While an overly slow reduction of the updating magnitude
can be ineffective in restricting the variance of the bias potential,
a too rapid reduction often
converges the bias potential to a wrong value.
%
For the single-bin case, it is known that
the optimal schedule is given by the
inverse time\cite{
belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016,
liang2007,
morozov2007, morozov2009, zhou2008}
or
the inverse number of updating steps.


In this study,
we first present a method of computing
the optimal schedule
for an adaptive FDS simulation
under a general updating scheme,
including the single-bin and Gaussian versions.
%
We will map the optimization problem to a mechanical one,
in which the schedule plays the role of the velocity of
a free particle with a position-dependent mass,
and the final error becomes the action.
%
This mapping allows the optimal schedule
to be derived from the equation of motion
that minimizes the action.
%
%The resulting optimal schedule
%depends on the updating window function
%through the mass of the particle.
%
We will show that
while the optimal schedule in the single-bin case
is given by the known inverse-time formula,
that for Gaussian case is more complex.
%and sensitive to the simulation length
%and the width of the Gaussian.

We will then show by comparing the errors
of simulations under different updating schemes
that the single-bin updating scheme
is optimal for very long simulations.
%without a priori assumption of the smoothness of the PMF.
%
A similar but partial asymptotic optimality applies to
a class of bandpass updating schemes
that flatten only
a few long-range distribution modes.
%
Compared with the Gaussian updating scheme,
bandpass updating schemes have the advantage of allowing
simpler control of the updating magnitude
for optimal convergence.

Finally, we present an application of
the bandpass updating schemes
by a construction from
a sequence of orthogonal polynomials.
%
The bias potential in this case assumes
a polynomial form,
and coefficients are adjusted on-the-fly
to approximate the target distribution.
%
This method serves
as a generalization of the Langfeld-Lucini-Rago (LLR)
algorithm\cite{langfeld2012, pellegrini2014},
and we show that
a similar construction with
a quadratic bias potential\cite{neuhaus2006, *neuhaus2007, zhu2012}
allows one to sample an array of Gaussian distributions
with predefined means and variances.

The article is organized as follows.
%
We present the analytical results in Sec. \ref{sec:methods},
provide numerical examples
in Sec. \ref{sec:results},
and conclude the article
in Sec. \ref{sec:conclusion}.




\section{\label{sec:methods}
Methods}



We present the methods
in the following order.
%
In Sec. \ref{sec:background},
we briefly review the basics of FDS
and some known aspects of the optimal schedule.
%
Then, in Sec. \ref{sec:single-bin},
we derive the method of
computing the optimal schedule
in the simple case of the
single-bin updating scheme
(as in the WL algorithm)
by proving the optimality
of the known inverse-time formula.
%
We extend the method to the general case
of a multiple-bin updating scheme
that encompasses the Gaussian updating scheme used in metadynamics
in Sec. \ref{sec:multiple-bin}.
%
Then, we compare different updating schemes
and show that the single-bin updating scheme
and a class of generalized bandpass updating schemes
are optimal for convergence
in the long time limit
in Sec. \ref{sec:cmpschemes}.
%
Finally,
we use the bandpass updating schemes
to build a class of adaptive FDS algorithms
with polynomial bias potentials
in Sec. \ref{sec:bandpass_poly}.



\subsection{\label{sec:background}
Flat-distribution sampling}



%\subsubsection{\label{sec:FDS}
%Bias potential}



Consider the problem of computing
the distribution, $p_i$,
along a discrete collective variable, $i$,
for a given system.
%
%For example, $i$ can be the energy, $E$,
%in a lattice spin model or the temperature index
%in a simulated tempering simulation\cite{
%marinari1992, *lyubartsev1992}.
%
For a continuous collective variable, $z$,
%which is often the case in a (classical) molecular system,
we can use the equivalent integer $i$ to represent
the index of a small interval, or a bin,
$(z, z + \delta z)$.
%
%The distribution is normalized as
%$\sum_{i = 1}^n p_i = 1$.


Sampling a multimodal distribution with deep basins
is often challenging in regular MC and MD simulations.
%
To promote transitions among the basins,
we can apply a bias potential to alter
the intrinsic distribution to a smoother one, $\rho_i$.
%
We will broadly define
a flat-distribution-sampling (FDS) simulation
as a biased simulation
targeting a flat $\rho_i$ (i.e. $\rho_i \equiv 1/n$ for $n$ bins)
or a nearly flat one\cite{
dayal2004, *trebst2004, barducci2008, singh2011}.
%
Since the equilibrium distribution
under a bias potential, $u_i$, is given by
%
\begin{equation}
  \pi_i
  %=
  %C_i \, p_i \, e^{-u_i}
  =
  \frac{                p_i \, e^{-u_i} }
       { \sum_{j = 1}^n p_j \, e^{-u_j} }
  \propto
  p_i \, e^{-u_i}
  ,
  \notag
  %\label{eq:pi_p_V}
\end{equation}
%
we wish to find a bias potential that allows
the resulting $\pi_i$ to coincide
with the desired $\rho_i$.
%
Upon convergence, the bias potential would satisfy
%
\begin{equation}
  u_i \to \ln \frac{p_i}{\rho_i} + \mathrm{const.}
  ,
  \label{eq:Vi_target}
\end{equation}
%
and the PMF, $-\ln p_i$, can be readily
deduced from the bias potential. % $u_i$.



Many methods\cite{mezei1987, berg1992, *lee1993,
wang2001, wang2001pre, huber1994,
*laio2002, *laio2008, *barducci2011, *sutto2012}
have been developed to find the desired bias potential.
%
In an earlier class of equilibrium FDS methods\cite{
  mezei1987, berg1992, *lee1993, marinari1992, *lyubartsev1992},
the bias potential, $u_i$, is
estimated beforehand and fixed
during simulation,
%
and a correction to the bias potential
is derived from the normalized histogram, $H_i$,
accumulated from the simulation as
%
\begin{equation}
  \hat u_i
  =
  u_i
  +
  \ln \frac{ H_i }
           { \rho_i }.
  \label{eq:vcorr_equil}
\end{equation}
%
\note{This follows from
  \begin{align*}
    H_i \approx \pi_i
    &\propto p_i \, e^{-u_i},
    \\
    \rho_i
    &\propto p_i \, e^{-\hat u_i}.
  \end{align*}
}
However, this formula often requires a long
accumulation period for a precise histogram,
and thus disallows
continuous improvement of the bias potential
%(hence that of the PMF)
as the simulation lengthens.

Below we will focus on a more recent class of adaptive FDS methods,
including the WL algorithm\cite{wang2001, wang2001pre}
and metadynamics\cite{huber1994, laio2002, *laio2008, *barducci2011, *sutto2012},
which
continuously and incrementally update the bias potential.
%
%\subsubsection{Updating schemes}
%
In the WL algorithm\cite{wang2001, wang2001pre},
the bias potential, $u_i(t)$, is updated
at each time step $t$ as
%
\begin{equation}
  u_i(t+1)
  =
  u_i(t)
  +
  \delta_{i, \, i(t)}
  \frac{ \alpha(t) } { \rho_{i(t)} }
  ,
\label{eq:wl_update}
\end{equation}
%
where $i(t)$ is the bin at step $t$,
$\delta_{i, \, i(t)}$ is the Kronecker delta function,
which is $1$ when $i = i(t)$ or $0$ otherwise,
and $\alpha(t)$ is the updating magnitude.
%
We refer to this scheme as
the \emph{single-bin} (updating) scheme,
for it applies only to the bias potential
at the current bin, $i(t)$.
%
In a more general
\emph{multiple-bin} (updating) scheme,
%including the Gaussian updating scheme used in metadynamics,
neighboring bins are also updated:
%
\begin{equation}
  u_i(t+1)
  =
  u_i(t)
  +
  w_{i, \, i(t)}
  \frac{ \alpha(t) }
       { \rho_{i(t)} }
  .
  \label{eq:mbin_update}
\end{equation}
%
A common example is the Gaussian updating scheme used in metadynamics,
with $w_{ij}$ being roughly a Gaussian centered on bin $j = i(t)$.
%
Note that if the updating occurs
only every few sampling steps,
then $t$ denotes the number of updating steps.

%\subsubsection{Updating magnitude and the inverse-time formula}



While adaptive FDS simulations
can often sample the desired distribution,
frequent updates to the bias potential
disrupt the underlying equilibrium sampling
and introduce artificial errors that need to be
reduced by gradually decreasing the updating magnitude.\cite{
  belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016,
  zhou2005, morozov2007, morozov2009, zhou2008, min2007,
  poulain2006, liang2007,
  crespo2010, *atchade2011, *fort2015}


In the original WL algorithm\cite{
wang2001, wang2001pre},
the updating magnitude, $\alpha(t)$,
(or $\ln f$ therein)
is controlled in stages.
%
In each stage, $\alpha(t)$
is kept constant,
and the histogram along $i$
is collected and monitored.
%
Once the histogram is sufficiently flat,
we reset the histogram and start a new stage
with $\alpha(t)$ reduced by a factor of $1/2$.\cite{
  wang2001, wang2001pre}
%
While this scheme works well for early stages,
it tends to reduce the magnitude
too quickly in late stages, making the asymptotic error
saturate\cite{
belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016}.


%For the single-bin updating scheme
%used in WL algorithm,
A more effective way
of controlling the updating magnitude in late stages
is to follow the inverse-time schedule,\cite{
  belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016,
  morozov2009, zhou2008,
  komura2012, *caparica2012, *caparica2014}
%
\begin{equation}
  \alpha(t) = \frac{1}{t},
  \label{eq:alpha_invt}
\end{equation}
%
where $t$, referred to as the ``time'' below,
is the number of updating steps
from the beginning of the simulation.
%
This is a general result,\cite{robbins1951, pellegrini2014}
and it has an intuitive explanation:\cite{
  marsili2006, barducci2008}
the bias potential undergoing the single-bin updating scheme
can be mapped to a cumulative moving average over all data points collected so far;
upon the addition of a new data point,
the average absorbs its contribution by the due weight,
the inverse of the current sample size, $t$
(cf. Appendix \ref{sec:equilerr}).




\subsection{\label{sec:single-bin}
Single-bin updating scheme}



In this section,
we derive the optimal $\alpha(t)$
for the single-bin updating scheme,
Eq. \eqref{eq:wl_update},
as a preparation
for the more general multiple-bin scheme.
%
We will first express the error of
the bias potential, $u_i(t)$,
as a functional of $\alpha(t)$,
and then minimize it by variation.
%



\subsubsection{Error}



We define the net error in the bias potential
by deducting two contributions.
%
First, we deduct the target value
[Eq. \eqref{eq:Vi_target}]
by introducing a shifted bias potential
%
\begin{equation}
  v_i(t)
  \equiv
  u_i(t)
  -
  \ln \frac { p_i }
            { \rho_i }
  ,
  \label{eq:v_def}
\end{equation}
%
which should tend to a constant of $i$
upon convergence.
%Note that $v_i(t)$ undergoes the same updating equation
%as $u_i(t)$ for the shift remains a constant during simulation.
%%
%In terms of $v_i$'s, Eq. \eqref{eq:pi_p_V}
%becomes
%%
%\begin{equation}
%  \pi_i
%  =
%  \frac{                \rho_i \, e^{-v_i} }
%       { \sum_{j = 1}^n \rho_j \, e^{-v_j} }
%  \propto
%  \rho_i \, e^{-v_i}.
%  \label{eq:pi_p_v}
%\end{equation}
%%
%According to Eq. \eqref{eq:pi_p_V},


The bias potential following Eq. \eqref{eq:wl_update}
generally grows over time.
%
Since a uniform increase of $u_i(t)$, or, equivalently, that of $v_i(t)$,
does not affect the resulting distribution, $\pi_i(t)$,
the real deviation of the bias potential
depends only on the difference between $v_i(t)$
and a baseline value, $\bav{v}(t)$,
for the average growth.\cite{
dama2014}
%
In the second deduction, we define
the bin average as $\bav{v}(t) \equiv \sum_{i=1}^n \rho_i \, v_i(t)$,
and the deviation of the bias potential as
%
\begin{equation}
  v_{*i}(t) \equiv v_i(t) - \bav{v}(t)
  .
\label{eq:x_def}
\end{equation}
%
\note{Alternatively,
  we may define $v_{*i}(t)$ such that
  $\rho_i \, e^{-v_{*i}(t)}$ is a normalized distribution,
  i.e.
  $$
  v_{*i}(t) \to -\ln\left(
    \frac{ (p_i/\rho_i) \, e^{ -u_i(t) } }
    { \sum_{j=1}^n p_j \, e^{ -u_j(t) } }
  \right).
  $$
  We can readily show that the two definitions are equivalent
  for small $v_{*i}$.}%
%
The total error is then given by
%
\begin{equation}
  \Err(t)
  =
  \sum_{i = 1}^n \rho_i \,
  \left\langle v_{*i}^2(t) \right\rangle
  .
\label{eq:error_def}
\end{equation}
%
Below we will use the notations ``$\bav{}$'' and ``$_{*i}$''
for other variables.




\subsubsection{\label{sec:sbin_diffeq}
Differential equation}



To proceed, we
approximate the finite difference equation, Eq. \eqref{eq:wl_update},
by a differential equation\footnote{In
  switching to the continuous-time setup,
  we find it convenient to shift the origin of time by $-1$,
  e.g. the sum $\sum_{i=1}^T$ is mapped to the integral $\int_0^T dt$.}
%
\begin{equation}
  \dot u_i(t)
  \equiv
  \frac{ d u_i(t) } { dt }
  \approx
  \alpha(t) \, \frac{ h_i(t) } { \rho_i }
  \equiv
  \alpha(t) \, f_i(t)
  ,
  \label{eq:ut_diffeq}
\end{equation}
%
with
%
$h_i(t) = \delta_{i, i(t)}$
%\begin{equation}
%  h_i(t) = \delta_{i, i(t)}
%  ,
%  \label{eq:h_def}
%\end{equation}
%
being the instantaneous histogram,
which is equal to $1$
for the bin $i(t)$ at time $t$
or zero otherwise,
and we have defined
$f_i(t) \equiv h_i(t) /\rho_i$.
%
The shifted bias potential, $v_i(t)$,
follows the same differential equation
for the shift is a constant:
$\dot v_i(t) \approx \alpha(t) \, f_i(t)$.
%
Deducting the bin average [cf. Eq. \eqref{eq:x_def}],
we get
%
\begin{equation}
  \dot v_{*i}(t)
  \approx
  \alpha(t) \, f_{*i}(t)
  .
  \label{eq:vt_diffeq}
\end{equation}

We can split the histogram into
a deterministic averaged part, $\langle f_i(t) \rangle$,
and a random fluctuating part, $\xi_i(t)$,
%
\begin{equation}
  f_i(t) =
  \langle f_i(t) \rangle
  +
  \xi_i(t)
  .
  \notag
  %\label{eq:h_split}
\end{equation}
%
The deterministic part can be related
to an average in an ensemble consisting of
many copies of similar simulations
sharing the same schedule $\alpha(t)$
and the same bias potential at time $t$.
%
The initial states and the stochastic forces
during the process may differ, however.
%
For a sufficiently small $\alpha(t)$,
the bias potential remains roughly the same for a short period,
and we may assume a quasi-equilibrium sampling process
such that
%with the deterministic part specified by Eq. \eqref{eq:pi_p_V}:
%
\begin{align}
  \langle f_i(t) \rangle
  %&=
  %\frac{ \langle h_i(t) } { \rho_i } \rangle
  &\approx
  \frac{ \pi_i(t) } { \rho_i }
  =
  \frac{                          e^{-v_{*i}(t)} }
       { \sum_{j = 1}^n \rho_j \, e^{-v_{*j}(t)} }
  %\notag
  %\\
  %&
  \approx
  1 - v_{*i}(t)
  ,
  \notag
  %\label{eq:h_ave}
\end{align}
%
where we have assumed the smallness
of $v_{*i}(t)$ in the linear expansion.
%
\note{
The second step follows from
$$
\frac{ \langle h_i(t) \rangle }
     { \rho_i }
\approx
\frac{                       1 - v_{*i}  }
     { \sum_{ r = 1 }^n \rho_j (1 - v_{*j}) }
=
\frac{                       1 - v_{*i}  }
     { 1 - \sum_{ j = 1 }^n \rho_j \, v_{*j} }
.
$$
}%
%
Deducting the bin average
and noticing that
$\bav{f}(t) = \sum_{i=1}^n \rho_i [h_i(t)/\rho_i] = \sum_{i=1}^n \delta_{i, i(t)} = 1$,
we get
$\langle f_{*i}(t) \rangle = \langle f_i(t) \rangle - 1 \approx - v_{*i}(t)$,
%
%\begin{align*}
%  \langle f_{*i}(t) \rangle
%  \equiv
%  \langle f_i(t) \rangle -
%  \bav{f}(t)
%  =
%  \langle f_i(t) \rangle - 1
%  \approx
%  - v_{*i}(t)
%  ,
%\end{align*}
%
and
%
\begin{equation}
  f_{*i}(t) \approx - v_{*i}(t) + \xi_{*i}(t)
  .
  \label{eq:sh_ave}
\end{equation}


We will approximate the histogram fluctuation part, $\xi_{*i}(t)$,
%and hence its deviation from the bin average, $\xi_{*i}(t)$,
as white noise such that
%
\begin{equation}
  \langle \xi_{*i}(t) \, \xi_{*i}(t') \rangle
  = G_i \, \delta(t - t')
  ,
  \label{eq:Gi_def}
\end{equation}
%
for some constant $G_i$,
and $\delta(t)$ is Dirac's delta function.
%
We expect the approximation to hold
for simulations much longer than the autocorrelation time.

From Eqs.
\eqref{eq:vt_diffeq} and \eqref{eq:sh_ave},
we get % a set of decoupled equations
%
\begin{equation}
  \dot v_{*i}(t)
  %=
  %\alpha(t) \, \left[ \langle f_{*i}(t) \rangle + \xi_{*i}(t) \right]
  \approx
  -\alpha(t) \, \left[ v_{*i}(t) - \xi_{*i}(t) \right]
  .
  %\notag
  \label{eq:dxdt_singlebin}
\end{equation}
%
Without the random part, $\xi_{*i}(t)$,
the deviation of the bias potential, $v_{*i}(t)$,
would tend to $0$ at long times, reaching convergence.
%
The formal solution of Eq. \eqref{eq:dxdt_singlebin} is
%
\begin{equation}
  v_{*i}(T)
  =
  v_{*i}(0) \, e^{-q(T)}
  +
  \int_0^T
    \dot{\theta}\bigl( q(t) \bigr) \, \xi_{*i}(t) \, dt,
  \label{eq:vi_solution}
\end{equation}
%
where
%
$q(t) \equiv \int_0^t \alpha(t') \, dt'$,
%
and
%
\begin{align}
  \theta(q')
  &\equiv
  e^{q' - q(T)}.
  \label{eq:theta_def}
\end{align}



\subsubsection{Optimal schedule}



From Eqs. \eqref{eq:error_def},
\eqref{eq:vi_solution}, and
\eqref{eq:Gi_def},
we can write the total error at the end of period $T$
as a sum of a residual error, $E_R(T)$ (from the decay of the initial error)
and an asymptotic error, $E_A(T)$:
%
\begin{align}
  \Err(T)
  =
  \Err_R(T) + \Err_A(T)
  ,
  \label{eq:error_tot}
\end{align}
%
where
\begin{align}
  \Err_R(T)
  &= \Err(0) \, e^{-2 \, q(T)}
  ,
  \label{eq:ER_sbin}
  \\
  \Err_A(T)
  &=
  \sum_{i=1}^n \rho_i
    \int_0^T\!\!\!\int_0^T
    \!\!
    \langle
      \xi_{*i}(t) \xi_{*i}(t')
    \rangle
    \dot \theta\bigl( q(t) \bigr)
    \dot \theta\bigl( q(t') \bigr)
    \, dt dt'
  \notag \\
  &= \Gamma \, \int_0^T \dot \theta^2\bigl( q(t) \bigr) \, dt
  ,
  \label{eq:EA_sbin}
\end{align}
with
%$\Gamma \equiv \sum_{i=1}^n \rho_i \, G_i$.
$\Gamma \equiv \sum_{i=1}^n \rho_i \, G_i
\approx \sum_{i=1}^n \rho_i \int_{-\infty}^{\infty} \langle \xi_{*i}(t) \xi_{*i}(0) \rangle \, dt$.
%\begin{equation}
%  \Gamma \equiv \sum_{i=1}^n \rho_i \, G_i
%  =
%  \sum_{i=1}^n \rho_i \int_0^T
%    \langle \xi_{*i}(t) \xi_{*i}(0) \rangle \, dt
%  .
%  \notag%\label{eq:Gamma_from_G}
%\end{equation}

The error depends implicitly on the curve, $\alpha(t)$,
or, equivalently, the curve, $q(t)$.
%via $v_{*i}(T)$.
%
%If $\alpha(t)$ is a constant, $a_0$,
%so that $q(t) = a_0 \, t$,
%then after a long period,
%the error approach an equilibrium value
%%
%\begin{equation}
%  \Err_\mathrm{equil.}
%  = \Err_A = \Gamma \int_0^\infty e^{-2\,q'} \, a_0 \, d q'
%  = \frac{1}{2} a_0 \Gamma
%  .
%  \label{eq:error_equil_sbin}
%\end{equation}
%
We wish to find the $q(t)$
that minimizes this error.
%
It is convenient to
vary the curve, $q(t)$ ($0 < t < T$),
under a fixed value of $q(T)$,
since it allows us to focus on
the asymptotic error, $E_A(T)$.
%
The contribution of the residual error
will be considered later
by adjusting the value of $q(T)$.
%
Note that the asymptotic error
has the form of an action of a free particle
at $\theta\bigl( q(t) \bigr)$,
with the endpoints fixed at
$\theta\bigl( q(0) \bigr)  = e^{- q(T)}$
and
$\theta\bigl( q(T) \bigr) = 1$.
%
Thus, the velocity should be a constant
%Since the residual error
%is fixed during the process,
%the asymptotic error,
%resembling the action of a free particle,
%demands a constant velocity
%
\begin{equation}
  \frac{d}{dt} \theta\bigl( q(t) \bigr) = c
  .
\label{eq:dthetadt_const}
\end{equation}
%
Then, by using Eq. \eqref{eq:theta_def}, we have
%
\begin{equation}
  e^{ q(t) - q(T) }
  =
  c \, (t + t_0)
  =
  \frac{ t + t_0 }
       { T + t_0 }
  ,
\label{eq:expqt}
\end{equation}
%
where $t_0$ is some constant,
and the previous constant, $c$, has been determined
from the $t = T$ case
in the second step.
%
Taking the logarithm,
and differentiating both sides
with respect to $t$,
we get the inverse-time schedule
%
\begin{equation}
  \alpha(t) = \frac{ 1 }{ t + t_0 }
  ,
\label{eq:alpha_invt1}
\end{equation}
%
with Eq. \eqref{eq:alpha_invt} being
the special case of $t_0 = 0$.
%

We now determine the optimal value of $q(T)$,
which was previously held fixed.
%
This is equivalent to choosing a value of $t_0$
because $q(T) = \ln\bigl(1 + \frac{T}{t_0}\bigr)$
from the $t = 0$ case of Eq. \eqref{eq:expqt}.
%
Using Eq. \eqref{eq:alpha_invt1} in
Eqs. \eqref{eq:ER_sbin} and \eqref{eq:EA_sbin}
yields
\begin{align}
  \Err_R(T)
  &= \frac{ \Err(0) \, t_0^2 } { (T + t_0)^2 }
  ,
  &
  %\notag
  %\label{eq:ER_sbin1}
  \\
  \Err_A(T)
  &= \frac{ \Gamma \, T } { (T + t_0)^2 }
  ,
  %\label{eq:EA_sbin1}
\end{align}
%
and the total has a minimum of
\begin{equation}
  \Err(T)
  =
  \frac{ \Gamma } { T + t_0 }
  ,
  \label{eq:Emin_sbin}
\end{equation}
%
reachable at $t_0 = \Gamma /\Err(0)$.
%
%If we assume that at $t = 0$,
%the system has just completed a long simulation
%under a constant updating magnitude, $a_0$,
%then the initial error can be computed from Eq. \eqref{eq:EA_sbin}
%as
%\begin{equation}
%  \Err(0)
%  = \Gamma \int_0^\infty e^{-2\,q'} \, a_0 \, d q'
%  = \frac{1}{2} a_0 \Gamma
%  ,
%  \label{eq:error_equil_sbin}
%\end{equation}
%and
%%
%\begin{equation}
%  t_0 = \frac{ 2 } { a_0 } = \frac{1}{ \alpha(t = 0) }
%  .
%  \label{eq:t0_sbin}
%\end{equation}
%
%The minimum error can be related to
%the expected histogram fluctuation\cite{zhou2005, zhou2008}
%(cf. Appendix \ref{sec:hfluc}).



\subsection{\label{sec:multiple-bin}
Multiple-bin updating scheme}



We now turn to the general multiple-bin updating scheme.
%
By definition, Eq. \eqref{eq:mbin_update},
a visit to bin $j = i(t)$ in this case results in updates of the bias potential
not only at bin $j$, but also at a few neighboring bins $i$'s.
%
Similar to Eq. \eqref{eq:ut_diffeq},
we can approximate the updating scheme
as a differential equation,
%
\begin{equation}
  \dot u_i(t)
  \approx
  \alpha(t) \,
  \sum_{j=1}^n w_{ij} \, f_j(t)
  =
  \alpha(t) \,
  \sum_{j=1}^n w_{ij} \, \frac{ h_j(t) } { \rho_j }
  .
  \label{eq:ut_diffeq_mbin}
\end{equation}
%
The optimization procedure is similar to
the single-bin case.
%
The key simplification comes from the
projection of the bias potential to the eigenmodes
of the updating matrix, $\mathbf w$,
formed by the relative magnitudes, $w_{ij}$'s.



\subsubsection{\label{sec:updating-matrix}
Updating matrix and spectral decomposition}



We will first review the spectral decomposition
of the updating matrix, $\mathbf w$,
which helps establish the convergence criterion
and the decomposition of error into
independent eigenmodes.
%
The matrix, $\mathbf w$, is subject to several conditions.
%
First, we have a fixed-point condition\cite{bussi2006, dama2014}.
%
To sample the desired distribution,
$\pmb\rho = (\rho_1, \dots, \rho_n)$,
the growth rate of the bias potential,
${\dot u}_i(t)$
in Eq. \eqref{eq:ut_diffeq_mbin},
should be a constant of $i$
if $h_j(t)$ were the same as $\rho_j$,
i.e.
$\sum_{j=1}^n w_{ij}$ should be a constant
to allow the bias potential to grow uniformly
in the asymptotic regime.
%
If the growth rate is positive,\footnote{Although
  in general the growth rate is not necessarily positive,
  it is often the largest eigenvalue
  for ``natural'' updating matrices considered
  in this study.}
we can, by a proper overall scaling of $\mathbf w$,
write this condition as
%
\begin{equation}
  \sum_{j = 1}^n w_{ij} = 1
  .
\label{eq:w_sumj}
\end{equation}
%
In other words, $(1, \dots, 1)^T$
is a right eigenvector of $\mathbf w$
with eigenvalue $1$.
%
Thus, the transpose $\mathbf w^T$
resembles a transition matrix,
except that some elements can be negative
for cases considered here.
%
Below we will %take advantage of the analogy and
borrow techniques used
in studying transition matrices\cite{vankampen},
but limit ourselves to properties that are unaffected
by the admission of negative elements into $\mathbf w$.


%To simplify the ensuing discussion,
Second, we limit ourselves to matrices % $\mathbf w$'s
that satisfy
the detailed balance condition for $\pmb\rho$,
%
\begin{equation}
  \rho_i \, w_{ij} = \rho_j \, w_{ji}
  .
  \notag
  %\label{eq:w_detailedbalance}
\end{equation}
%
This requires the scaled updating matrix,
$\sqrt{ \rho_i/\rho_j } \, w_{ij}$,
to be symmetric,
and the symmetry allows the diagonalization
of $\mathbf w$ with a set of
eigenvectors, $\phi_{ki}$,
%
\begin{equation}
  \sum_{i = 1}^n \phi_{ki} \, w_{ij}
  =
  \lambda_k \, \phi_{kj}
  ,
\label{eq:eig_w}
\end{equation}
%
satisfying the orthonormal conditions\cite{vankampen}:
%
\begin{align}
  \sum_{k = 0}^{n - 1}
    \phi_{ki} \, \phi_{kj}
  &=
  \delta_{ij} \, \rho_i,
  \label{eq:eig_orthonormal_cols}
  \\
  \sum_{i = 1}^n
    \frac{ \phi_{ki} \, \phi_{li} }
         { \rho_i }
  &=
  \delta_{kl}
  ,
  \label{eq:eig_orthonormal_rows}
\end{align}
%
where we have started the index of the eigenmodes at $0$
instead of $1$ for later convenience.

The above symmetry implies the existence of
a left eigenvector, $\pmb \rho$,
corresponding to the uniform right eigenvector
associated with Eq. \eqref{eq:w_sumj},
with eigenvalue $1$:
%
\begin{equation}
  \sum_{i = 1}^n \rho_i \, w_{ij}
  =
  \sum_{i = 1}^n \rho_j \, w_{ji}
  =
  \rho_j
  .
  \notag
  %\label{eq:w_balance}
\end{equation}
%
We will label this eigenvector by index $k = 0$,
and
%
\begin{equation}
  \phi_{0i} = \rho_i,
\label{eq:eigenmode0}
\end{equation}
%
with $\lambda_0 = 1$.
%
Clearly, Eq. \eqref{eq:eigenmode0}
satisfies the normalization condition
given by Eq. \eqref{eq:eig_orthonormal_rows}
for $k = l = 0$,
while the orthogonality demands
%
\begin{equation}
  \sum_{ i = 1 }^n \phi_{ki}
  =
  \delta_{k0}
  .
\label{eq:ortho0}
\end{equation}

We can further define
a generalized Fourier transform, $\mathcal{F}$,
for variable, $x_i$, as
%
\begin{equation}
  {\tilde x}_k
  \equiv \mathcal{F}[x_i]_k
  \equiv \sum_{i = 1}^n \phi_{ki} \, x_i
  ,
  %\notag
  \label{eq:gft_def}
\end{equation}
%
and the inverse transform as
%$x_i = \sum_{k = 0}^{n-1} \phi_{ki} \, \tilde{x}_k / \rho_i$.
%
\begin{equation}
  x_i = \frac{1}{\rho_i} \sum_{k = 0}^{n-1} \phi_{ki} \, \tilde{x}_k
  .
  %\notag
  \label{eq:gft_inv}
\end{equation}
%
Then,
we can rewrite Eq. \eqref{eq:ortho0} as
$\mathcal F[1]_k = \delta_{k0}$,
%
and Eq. \eqref{eq:eigenmode0} shows that
the bin average
%of any quantity, $x_i$, is the first mode of the Fourier transform:
$\bav{x}= \mathcal F[x_i]_0 = \tilde x_0$,
%
and
\begin{equation}
  \tilde x_{*k}
  = \tilde x_k - \bav{x} \, \delta_{k0}
  =
  \begin{dcases}
    0           & k = 0
    ,
    \\
    \tilde x_k  & k > 0
    .
  \end{dcases}
  \label{eq:xtstar}
\end{equation}
%
Thus, the only effect
of the bin-average deduction on $\tilde x_k$ is to
annihilate the $k=0$ mode.
%
From Eqs. \eqref{eq:gft_def} and \eqref{eq:gft_inv},
we have Parseval's theorem,
\begin{equation}
  \sum_{k=1}^{n-1} \tilde x_k \, \tilde y_k
  =
  \sum_{k=0}^{n-1} \tilde x_{*k} \, \tilde y_{*k}
  =
  \sum_{i=1}^n \rho_i \, x_{*i} \, y_{*i}
  .
  \label{eq:parseval}
\end{equation}
\note{Proof:
\begin{align*}
  \sum_{k=0}^{n-1} \tilde x_{*k} \, \tilde y_{*k}
  &=
  \sum_{k=0}^{n-1} \tilde x_{*k} \sum_{i=1}^n \phi_{ki} \, y_{*i}
  \\
  &=
  \sum_{i=1}^n \left( \sum_{k=0}^{n-1} \tilde x_{*k} \, \phi_{ki} \right) \, y_{*i}
  =
  \sum_{i=1}^n \rho_i \, x_{*i} \, y_{*i}.
\end{align*}
}

We can now use Eq. \eqref{eq:eig_w} to diagonalize
the multiple-bin updating scheme, Eq. \eqref{eq:mbin_update},
as
%
\begin{equation}
  {\tilde v}_k(t + 1) =
  {\tilde v}_k(t) + \alpha(t) \, \lambda_k \,
  {\tilde f}_k(t)
  ,
  \label{eq:vkupdate}
\end{equation}
%
or in continuous time, we have, for $k > 0$,
%
\begin{equation}
  \dot{\tilde v}_k(t)
  =
  \alpha(t) \, \lambda_k \, {\tilde f}_k(t)
  \approx
  -\alpha(t) \, \lambda_k \,
  \bigl[ {\tilde v}_k(t) - {\tilde \xi}_k(t) \bigr]
  ,
  \label{eq:vt_diffeq_mbin}
\end{equation}
%
where
we have used Eq. \eqref{eq:xtstar} and
the transformed version of Eq. \eqref{eq:sh_ave}
in the second step.

Equation \eqref{eq:vt_diffeq_mbin} shows that
in the absence of the fluctuating term, $\tilde \xi_k(t)$,
the deviation of the bias potential would tend to zero
at a rate of $\alpha(t) \, \lambda_k$.
%
Thus, the updating scheme is convergent
only if no eigenvalue, $\lambda_k$, is negative
for $k > 0$.
%
%With a set of heterogeneous eigenvalues,
%the errors of the eigenmodes are reduced at different rates.
%Thus, the optimization problem of the final error
%becomes more complicated and it often
%involves a compromise among eigenmodes.
%
The formal solution of Eq. \eqref{eq:vt_diffeq_mbin}
is given by
\begin{equation}
  \tilde v_k(T)
  =
  \tilde v_k(0) \, e^{-\lambda_k \, q(T)}
  +
  \int_0^T
    \dot{\theta}_k\bigl( q(t) \bigr) \, \tilde\xi_k(t) \, dt
  ,
  \label{eq:vk_solution}
\end{equation}
where
\begin{equation}
  \theta_k(q') \equiv e^{\lambda_k \, [q' - q(T)]}
  .
  \label{eq:uk_def}
\end{equation}





\subsubsection{Error}



By Eq. \eqref{eq:parseval},
we can rewrite the error defined in Eq. \eqref{eq:error_def} as
a sum of the errors in individual modes\footnote{If
the error weighting distribution differs from $\rho_i$
in Eq. \eqref{eq:error_def},
the error would contain quadratic cross terms among different eigenmodes.
%
While the solution for the optimal schedule retains the same form,
the mass function defined in Eq. \eqref{eq:mass_func}
will include the corresponding cross terms.
}
%$\Err(T) = \sum_{k = 1}^{n - 1} \left\langle {\tilde v}_k^2(T) \right\rangle$.
%
\begin{align}
  \Err(T)
  =
  \sum_{k = 1}^{n - 1}
    \left\langle
      {\tilde v}_k^2(T)
    \right\rangle
  .
  \notag
\end{align}
%
As in Eq. \eqref{eq:error_tot},
we can decompose the error as the sum
of the residual error and the asymptotic error, with
\begin{align}
  \Err_R(T)
  &=
  \sum_{k = 1}^{n-1}
    \left\langle
      {\tilde v}_k^2(0)
    \right\rangle \,
    e^{ - 2 \, \lambda_k  \, q(T) }
  ,
  \label{eq:error_res}
  \\
  \Err_A(T)
  &=
  \sum_{k=1}^{n-1}
  \int_0^T\!\!\!\int_0^T
  \!\!
  \langle
    \tilde\xi_k(t)
    \tilde\xi_k(t')
  \rangle
  \theta_k\bigl(q(t)\bigr)
  \theta_k\bigl(q(t')\bigr)
  \, dtdt'
  \notag \\
  &=
  \int_0^T
  \sum_{k = 1}^{n-1}
  \Gamma_k \, \dot \theta_k^2\bigl( q(t) \bigr) \, dt
  ,
  \label{eq:error_asym}
\end{align}
%
where
%
%$\Gamma_k$ is computed from
%the autocorrelation function
%of the histogram fluctuation,
%$\bigl\langle {\tilde \xi}_k(0)
%\, {\tilde \xi}_k(t) \bigr\rangle$,
%as:
%
\begin{equation}
  \Gamma_k
  \approx
  \int_{-\infty}^\infty
  \bigl\langle
    \tilde\xi_k(t) \, \tilde\xi_k(0)
  \bigr\rangle
  \, dt
  \to
  \sum_{t = -\infty}^\infty
  \bigl\langle
    \tilde\xi_k(t) \, \tilde\xi_k(0)
  \bigr\rangle
  ,
  \label{eq:Gamma_sum}
\end{equation}
%
with the second expression being
the counterpart in the discrete-time setting.
%
Equation \eqref{eq:Gamma_sum} represents
a first approximation for modeling
a general fluctuation, ${\tilde \xi}_k(t)$,
as an equivalent white noise.
In this way,
the underlying sampling process
affects the error only through the
few numbers, $\Gamma_k$'s.
%
Note that by Eq. \eqref{eq:parseval}, we have
$\sum_{k=1}^{n-1} \bigl\langle {\tilde \xi}_k(t) \, {\tilde \xi}_k(0) \bigr\rangle
= \sum_{i=1}^n \rho_i \bigl\langle \xi_{*i}(t) \, \xi_{*i}(0) \bigr\rangle,$
and summing $t$ from $-\infty$ to $\infty$ yields,
\begin{equation}
  \sum_{k=1}^{n-1} \Gamma_k = \sum_{i=1}^n \rho_i \, G_i = \Gamma
  .
  \label{eq:Gammak_sum}
\end{equation}
This shows the equivalence of
Eq. \eqref{eq:EA_sbin} and \eqref{eq:error_asym}
in the case of the single-bin updating scheme, with $\lambda_k \equiv 1$.
%In Appendix \ref{sec:Gamma_measure},
%we discuss a way of estimating $\Gamma_k$
%from an FDS simulation under a constant magnitude.
%



\subsubsection{\label{sec:optschedule}
Optimal schedule}



As in the single-bin case,
by varying the schedule, $\alpha(t)$, under a fixed value of
$q(T) = \int_0^T \alpha(t) \, dt$,
we can focus on the asymptotic error,
$\Err_A(T)$.
%
The value of $q(T)$ will be adjusted later
to minimize the total error. % in Sec. \ref{sec:optinitalpha}.
%
We can rewrite Eq. \eqref{eq:error_asym} much like an action
of a particle whose position is given by $q(t)$:
%
\begin{equation}
  \Err_A(T)
  =
  \int_0^T
    {\mathcal L} \bigl[ q(t)\bigr]
    \, dt
  ,
  \notag
  %\label{eq:error_asym_Lagrangian}
\end{equation}
%
where the Lagrangian is
%
\begin{align}
  {\mathcal L} \bigl[ q(t) \bigr]
  &=
  \sum_{ k = 1 }^{n - 1}
    \Gamma_k \, {\dot \theta}_k^2\bigl[ q(t) \bigr]
  %\notag
  %\\
  %&=
  =
  %\sum_{ k = 1 }^{n - 1}
  %  \Gamma_k \, \lambda_k^2 \, \theta_k^2 \bigl[ q(t) \bigr]
  %\; \dot q^2( t )
  %\notag
  %\\
  %&=
  M^2\bigl(q(T) - q(t) \bigr)
  \; \dot q^2( t )
  .
\notag
\end{align}
%
where we have
used Eq. \eqref{eq:uk_def} and
defined the (square-root) mass function as
%
\begin{equation}
  M(\bar q)
  \equiv
  \sqrt{
    \textstyle\sum_{ k = 1 }^{n - 1}
    \Gamma_k \, \lambda_k^2 \, e^{-2 \, \lambda_k \, \bar q}
  }
  .
  %\notag
  \label{eq:mass_func}
\end{equation}
%
In this mechanical analogy,
the schedule, $\dot q(t) = \alpha(t)$,
corresponds to the velocity of the particle,
%
and the above Lagrangian, containing only the kinetic energy,
describes a free particle
with a position-dependent mass.
%
Here, the Hamiltonian coincides with the Lagrangian:
%
\begin{equation}
  \mathcal H
  =
  \frac{ \partial \mathcal L }
       { \partial \dot q     }
  \, \dot q
  -
  \mathcal L
  =
  2 \, \mathcal L
  - \mathcal L
  =
  \mathcal L
  .
  \notag
  %\label{eq:error_asym_Hamiltonian}
\end{equation}
%
Since the Lagrangian
does not explicitly depend on time, $t$,
the Hamiltonian is conserved,\cite{goldstein, *landau_mechanics, *arnold, *jose}
which means that the asymptotic error grows
linearly with time at a rate of $\mathcal L$.
We may set
%
\begin{equation}
  \sqrt{ \mathcal H }
  =
  \sqrt{ \mathcal L }
  =
  M\bigl( q(T) - q(t) \bigr)
  \;
  \dot q(t)
  =
  \frac{C_M}{T}
  ,
  \label{eq:Lagrangian_const}
\end{equation}
%
for some positive $C_M$.
%
Equation \eqref{eq:Lagrangian_const} serves
as a generalization of Eq. \eqref{eq:dthetadt_const}.
%
By integrating this equation of motion, we get
%
\begin{equation}
  \int_{ q(T) - q(t) }^{ q(T) }
    M(\bar q)
    \;
    d \bar q
  =
  C_M \, \frac t T
  ,
  \label{eq:q_opt}
\end{equation}
%
and $C_M$ can be determined from
the $t = T$ case as
%
\begin{equation}
  C_M =
  \int_{ 0 }^{ q(T) }
    M( \bar q )
    \;
    d \bar q
  .
  \label{eq:mint}
\end{equation}
%
Thus, Eq. \eqref{eq:Lagrangian_const}
gives an implicit equation for $q(t)$,
and hence the optimal schedule,
$\alpha(t) = \dot q(t)$.
%
This solution can be characterized geometrically,
as shown in Appendix \ref{sec:schedule_geometry}.
%
The minimal asymptotic error is
%
\begin{equation}
  \Err_A(T)
  =
  \mathcal L \, T
  =
  \frac { C_M^2 } { T }
  =
  \frac 1 T
  \left(
    \int_0^{ q(T) } M(\bar q) \, d \bar q
  \right)^{\!\!2}
  .
\label{eq:error_asym2}
\end{equation}



%\subsubsection{\label{sec:eqlerr}
%Initial error
%%from a simulation under a constant updating magnitude
%}



\subsubsection{\label{sec:optinitalpha}
  Optimal initial updating magnitude
}


We now determine the optimal value of $q(T)$
that minimizes the total error defined in Eq. \eqref{eq:error_tot}.
%
To model the initial error for Eq. \eqref{eq:error_res},
we will assume that at $t = 0$
the system has undergone a preliminary FDS simulation
under a constant updating magnitude, $a_0$,
for $T_0$ steps.
%
If the initial bias potential
before the preliminary run is zero,
we have, from Eq. \eqref{eq:vk_solution} [with $T\to T_0, q(t) \to a_0 \, t$],
%
\begin{align}
  \left\langle {\tilde v}_k(0) \right\rangle
  &=
  \tilde u_k \, e^{-\lambda_k \, a_0 \, T_0}
  ,
  \label{eq:xt_eql}
  \\
  %\operatorname{var} \tilde v_k(0)
  %&=
  %\frac 1 2 \, \Gamma_k \, \lambda_k \, a_0
  %\left( 1 - e^{-2 \, \lambda_k \, a_0 \, T_0} \right)
  %,
  %\label{eq:xtvar}
  %\\
  \left\langle \tilde v_k^2(0) \right\rangle
  &=
  %\operatorname{var} \tilde v_k(0) + \zeta_k^2
  %=
  \frac 1 2 \, \Gamma_k \, \lambda_k \, a_0
  +
  \epsilon_k \, e^{-2 \, \lambda_k \, a_0 \, T_0}
  ,
  \label{eq:xt2_eql}
\end{align}
%
where
$\epsilon_k \equiv \left(\tilde u_k^2 - \frac{1}{2} \Gamma_k \, \lambda_k \, a_0\right) \, e^{-2\,\lambda_k a_0 T_0}$.



%
From Eqs. \eqref{eq:error_res},
\eqref{eq:xt2_eql},
and \eqref{eq:error_asym2},
we find that
the optimal $q(T)$ that minimizes $\Err_R(T) + \Err_A(T)$
satisfies
%
\begin{align}
  \Delta\bigl( q(T) \bigr)
  &\equiv
  \frac{ M\bigl( q(T) \bigr) } { T }
    \int_0^{q(T)} M(q') \, d q'
  -\frac{a_0}{2} \, M^2\bigl( q(T) \bigr)
  \notag \\
  &\quad
  -\sum_{k = 1}^{n-1}
  \lambda_k \, \epsilon_k \, e^{-2\lambda_k \, q(T)}
  =0
  ,
\label{eq:opt_qT}
\end{align}
where
%
%\begin{align}
%  m\bigl( q(T) \bigr)
%  +
%  \frac{2}{a_0 \, C_M \, M\bigl( q(T) \bigr)}
%  \sum_{k = 1}^{n-1}
%  \lambda_k \, \tilde u_k^2 \,
%  e^{-2 \, \lambda_k \, [a_0 \, T_0 + q(T)] }
%  \\=
%  \frac{1} { T \, (a_0 / 2) }
%  .
%\label{eq:opt_qT}
%\end{align}
%
\note{We can write
\begin{align*}
  \left\langle
    {\tilde v}_k^2(0)
  \right\rangle
  &=
  \frac { a_0 \, \Gamma_k \, \lambda_k } { 2 }
  +
  \Delta_k
  e^{-2\, \lambda_k a_0 T_0}
  ,
\end{align*}
%
where $\Delta_k \equiv \tilde u_k^2 - \frac{a_0 \lambda_k \Gamma_k}{2}$.
%
Let $q_T \equiv q(T)$,
\begin{align*}
  \frac{
    \partial \Err_R(T)
  }
  {
    \partial q_T
  }
  &=
  -2\sum_{k=1}^{n-1} \lambda_k
  \left\langle
    \tilde v_k^2(0)
  \right\rangle \,
  e^{-2 \, \lambda_k \, q_T}
  \\
  &=
  -a_0 \, M^2(q_T)
  -2
  \sum_{k=1}^{n-1} \lambda_k \,
  \Delta_k e^{-2 \, \lambda_k \, (a_0 \, T_0 + q_T)}
  ,
  \\
  \frac{
    \partial \Err_A(T)
  }
  {
    \partial q_T
  }
  &=
  \frac 2 T \,
  M(q_T) \,
  \int_0^{ q_T } M(q) \, dq
  =
  \frac{ 2 \, C_M } { T } \, M(q_T)
  .
\end{align*}
Then solve $\partial (\Err_R + \Err_A) / \partial q_T = 0$,
\begin{align*}
  \frac{ M\bigl( q_T \bigr) } { T } C_M
  -\frac{a_0}{2} \, M^2\bigl( q_T \bigr)
  -\sum_{k = 1}^{n-1}
  \lambda_k \, \Delta_k \, e^{-2\lambda_k \, (q_T + a_0 T_0)}
  =0
  .
\end{align*}
}
%
We can rewrite the condition in terms of
the initial updating magnitude
by Eq. \eqref{eq:Lagrangian_const}, %\eqref{eq:mQ_invTa},
\begin{align}
  \alpha(t = 0)
  %=
  %\frac{1}{T \, m\bigl( q(T) \bigr) }
  =
  \frac{ a_0 } { 2 }
  +
  \sum_{k = 1}^{n-1}
  \frac{
    \lambda_k \, \epsilon_k \, e^{-2\lambda_k \, q(T)}
  }{M^2\bigl( q(T) \bigr)}
  \stackrel{\epsilon_k \to 0}{=\joinrel=\joinrel=}
  \frac{ a_0 }
       { 2 }
  .
  %\notag
  \label{eq:half_alpha0}
\end{align}
%
%
The equality in the last step is achievable
if the initial bias, $\epsilon_k$, is negligible,
%which is often true if no eigenvalue is close to $0$,
and in this limit, the optimal initial updating magnitude
is half of the value
in the preliminary run.\note{This factor, $1/2$,
  happens to coincide with the
  recommended reduction factor of the updating magnitude
  at stage transitions
  in the original WL algorithm\cite{
  wang2001, wang2001pre}.  }%
%[cf. Eq. \eqref{eq:t0_sbin}].



\subsubsection{\label{sec:procedure}
Procedure for computing the optimal schedule and error
}



Based on the above results,
we give a procedure for computing
the optimal schedule and the error
for a given adaptive FDS simulation.
%
Basically, we need to extract
$\lambda_k$'s, $\Gamma_k$'s, and $\epsilon_k$'s
from the given system and updating scheme (input),
and then apply them to Eq. \eqref{eq:q_opt}
for the schedule,
as shown in Fig. \ref{fig:vardep}.
%
While the updating scheme alone
determines $\lambda_k$'s,
$\Gamma_k$'s depend also on the system
and the underlying sampling method.
%
The procedure is given below.

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/vardep.pdf}
  }
  \caption{
    \label{fig:vardep}
    Computation of the optimal schedule and the error.
  }
\end{figure}




\begin{enumerate}

\item
Compute the eigenvalues, $\lambda_k$'s,
from the updating matrix, $\mathbf w$.
%
Note that
no eigenvalue can be negative in a convergent updating scheme.

\item \label{step:prerun}
Run a preliminary adaptive FDS simulation
under a constant updating magnitude, $a_0$.
%
\note{The updating magnitude should be sufficiently small
  such that the resulting error is no greater than
  multiple of $1.0$.}

\item \label{step:Gamma}
Estimate $\Gamma_k$'s
(cf. Appendix \ref{sec:Gamma_measure})
and the initial bias $\epsilon_k$.\footnote{The
  value of $\tilde u_k$ can be estimated
  from the histogram-corrected bias potential
  by Eq. \eqref{eq:uhatav},
  although it often leads to overestimated
  $\epsilon_k$'s for large-$k$ modes.
}

\item \label{step:qT}
Compute the optimal value of $q(T)$
by solving Eq. \eqref{eq:opt_qT}.\cite{*[{We
  used a variant of the Newton-Raphson method
  [see e.g. }]
  [{] with the bisection method as a fallback.
  We solved the equation for $\ln q(T)$
  to avoid negative values of $q(T)$,
  and imposed a lower bound,
  $\bigl|\Delta\bigl(q(T)\bigr)/q(T)\bigr|$, for
  $\Delta'\bigl(q(T)\bigr)$
  in computing its change.
  The initial value was given by
  $q(T) = \ln\left(1+\frac{1}{2}T\,a_0\right)$.}]
  press3rd}
%
%Since $m(\bar q)$ is a decreasing function of $\bar q$,
%we can start from an interval $[Q_L, Q_R]$
%that satisfies $m(Q_L) > 2/(T\,a_0) > m(Q_R)$
%(e.g. we can choose $Q_L = 0$,
%  and repeatedly double $Q_R$ until
%  the latter inequality is satisfied).
%%
%We can then iteratively refine the interval
%by substituting $Q_M = (Q_L + Q_R)/2$
%for either $Q_L$ and $Q_R$,
%depending on the sign of $m(Q_M) - 2/(T \, a_0)$,
%until the width of the interval is less than
%the desired precision,
%%
%and set $q(T)$ to $Q_M$.

\item \label{step:alpha}
Compute the optimal schedule by
numerically integrating
Eqs. \eqref{eq:q_opt} and \eqref{eq:mint}.\footnote{We
  used the trapezoidal rule for the integration.
  The grid points were
  $q_{[i]} = \bigl(q(T) + 1\bigr) - \bigl(q(T)+1\bigr)^{1-i/N_g}$,
  where $i = 0, \dots, N_g$
  with $N_g = 20000$.
  %
  The grid points were designed for long simulations
  under the Gaussian updating scheme:
  for a mass function, $M(\bar q)$, that is roughly
  inversely proportional to $\bar q$
  (cf. Appendix \ref{sec:schedule_geometry}),
  this grid makes the integral,
  $t_{[i+1]} - t_{[i]}$,
  approximately a constant.
  %
  The resulting values, $\bigl(q_{[i]}, t_{[i]} \bigr)$,
  were used to compute the updating magnitude
  as (cf. Appendix \ref{sec:ave_ub}):
  $$%\begin{equation}
    \alpha_{[i]}
    =
    \frac{1}{\lambda_1}
    \left[
      1
      -
      \exp\left(
      -\lambda_1
      \frac{ q_{[i+1]} - q_{[i-1]} }
           { t_{[i+1]} - t_{[i-1]} }
      \right)
    \right]
    ,
    %\notag
    %\label{eq:alphai}
  $$%\end{equation}
  with
  $q_{[ -1]} \equiv t_{[-1]} \equiv 0$,
  $q_{[N_g+1]} \equiv q(T)$,
  and
  $t_{[N_g+1]} \equiv T$.
  %
  The values of $(t_{[i]}, \alpha_{[i]})$
  were linearly interpolated
  to get the updating magnitude, $\alpha(t)$, at any $t$.
  }
%


\item
Compute the total error $\Err(T)$ from
Eqs.
\eqref{eq:error_tot},
\eqref{eq:error_res},
\eqref{eq:xt2_eql},
and
\eqref{eq:error_asym2}.

\end{enumerate}

%The above procedure is not always necessary.
%%
%As we will show below, for a class of bandpass updating schemes,
%  the asymptotically optimal schedule
%  is simply given by the inverse-time schedule, Eq. \eqref{eq:alpha_invt1}.
%
%For these updating schemes,
%  the preliminary run with a constant updating magnitude
%  can be replaced by the WL-style stagewise reduction
%  until the updating magnitude falls under
%  the value given the inverse-time prescription\cite{
%    belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016}.



\subsubsection{\label{sec:Gaussian_scheme}
Gaussian updating scheme}



A commonly-used multiple-bin updating scheme in metadynamics
is the Gaussian updating scheme,
whose updating matrix can be characterized by
a rigid Gaussian function centered on the current bin
(with some boundary adjustment).
%to satisfy the constraints discussed in Sec. \ref{sec:updating-matrix},
%see Fig. \ref{fig:mat}(a) for an example.
%
In terms of the continuous collective variable, $z = i \, \delta z$,
the eigenvalues of this scheme are given by\cite{bussi2006}
\begin{align}
  \lambda_k
  =
  \exp\left(
        -
        \frac{ \pi^2 \, \sigma_z^2 \, k^2 }
             { 2 \, \Delta z^2 }
      \right)
  ,
  %\notag
  \label{eq:lambda_Gaussian}
\end{align}
where $\sigma_z$ is the width of the Gaussian
and $\Delta z = n \, \delta z \, g/2$
is the effective span
with $g = 1$ for a periodic variable or $2$ otherwise
(cf. Appendix \ref{sec:Gaussian_math}).

The optimal schedule of the Gaussian updating scheme
generally has no simple closed-form expression.
Nonetheless, we can show that it always lies above
the inverse-time schedule,
and may assume a roughly exponentially-decaying form
initially (cf. Appendix \ref{sec:schedule_geometry}).

%\begin{figure}[h]\centering
%  \makebox[\linewidth][c]{
%    \includegraphics[angle=0, width=1.0\linewidth]{fig/mat.pdf}
%  }
%  \caption{
%    \label{fig:mat}
%    Updating matrices for two
%    homogeneous updating schemes of
%    a non-periodic variable ($n = 500$).
%    Here $j$ is the current bin,
%    $i$ is the bin where updating occurs,
%    and $w_{ij}$ is the relative updating magnitude.
%    %
%    (a) The Gaussian updating scheme of width $\sigma = 20$ bins.
%    (b) The homogeneous bandpass updating scheme of $K = 20$.
%    %
%    Both updating schemes
%    can be characterized by a rigid window
%    centered on the current bin, $j$,
%    with some boundary adjustments.
%  }
%\end{figure}



\subsubsection{\label{sec:homo_scheme}
Homogeneous updating schemes}


More generally, we can replace the rigid Gaussian function
in the Gaussian updating scheme
by a window function of different shape.
%
We call the resulting updating scheme
\emph{homogeneous} or
translationally-invariant.
%see Fig. \ref{fig:mat}(b) for an example.

By morphing the window function
but preserving the translational invariance,
we can design a homogeneous updating scheme
that is more convenient in error control
than the Gaussian updating scheme.
%
%whose updating matrix can be
%characterized by a rigid window function with a floating center,
%
As shown in Appendix \ref{sec:homo},
the homogeneous updating schemes share
the same set of orthonormal eigenvectors, $\phi_{ki}$,
which are cosine and sine functions,
and differ only by the eigenvalues.
%
Thus, the window function is completely determined
by the eigenvalues, and the updating matrix
is given by\cite{bussi2006}
%
\begin{equation}
  w_{ij}
  =
  \frac{1}{\rho_i} \sum_{k=0}^{n - 1}
  \lambda_k \, \phi_{ki} \, \phi_{kj}
  .
  \label{eq:w_from_phi}
\end{equation}
%
%We discuss this strategy in detail below.
As shown below,
the optimized updating scheme
allows the optimal schedule to coincide with
the inverse-time formula,
and thus spare the need to
go through the general procedure in Sec. \ref{sec:procedure}.




\subsection{\label{sec:cmpschemes}
Optimization of updating schemes}


Now that we can compute the optimal schedule
for a given updating scheme,
we turn to the problem of optimizing
the updating scheme.


\subsubsection{\label{sec:optWL}
Asymptotic optimality of the single-bin updating scheme}



We first show
that the single-bin updating scheme is asymptotically optimal.
%
We wish to find a lower bound of the asymptotic error,
$\Err_A(T)$ [given by Eq. \eqref{eq:error_asym}],
for a class of updating matrices that
share the same set of eigenvectors,
hence the same $\Gamma_k$'s,
but not the eigenvalues,
$\lambda_k$'s.
%
If all eigenvalues are positive,
we can use the Cauchy-Schwarz inequality,\footnote{This
  inequality follows from the nonnegativity of
  the quadratic polynomial of $y$,
  \begin{align*}
  \int_0^T \!\!dt \sum_{k = 1}^{n-1} \Gamma_k \,
      ( y - {\dot \theta}_k )^2
    \equiv
    A_2 \, y^2 + A_1 \, y + A_0
    ,
  \end{align*}
  which implies that the discriminant is nonpositive,
  $A_1^2 - 4 \, A_0 \, A_2 \le 0$.}
%
%
\begin{align}
  &
  \left(
    \int_0^T \!\!dt
      \sum_{k = 1}^{n-1}
        \Gamma_k \, {\dot \theta}_k^2\bigl( q(t) \bigr)
  \right)
  %
  \left(
    \int_0^T \!\!dt
      \sum_{k = 1}^{n-1}
        \Gamma_k
  \right)
  %
  \notag \\
  & %\qquad \qquad \qquad
  \ge
  \Biggl[
    \int_0^T \!\!dt
      \sum_{k = 1}^{n-1}
        \Gamma_k \, {\dot \theta}_k \, \bigl( q(t) \bigr)
  \Biggr]^{\!2}
  %\notag \\
  %& \qquad \qquad \qquad
  \!\!=
  \Biggl[
    \sum_{k = 1}^{n-1} \Gamma_k
      \bigl(
        1 - e^{ -\lambda_k \, q(T) }
      \bigr)
  \Biggr]^{\!2}
  \!\!
  .
  \notag
  %\label{eq:CSineq}
\end{align}
%
In the long time limit, $q(T) \to \infty$,
the factor $e^{-\lambda_k \, q(T)}$ is negligible,
and the lower bound is independent of the $\lambda_k$'s:
$\Err_A(T) \ge \sum_{k=1}^{n-1} \Gamma_k/T$.
%Note that the last expression %of Eq. \eqref{eq:CSineq}
%depends on the curve, $q(t)$ ($0 < t < T$),
%only through the endpoint value, $q(T)$,
%which is fixed in the variational process.
%
This inequality applies to any schedule
and any updating scheme in the class,
and the equality is achieved
if $\dot \theta_k\bigl( q(t) \bigr) = \mathrm{constant}$
for all $k > 0$ at any $t$.
%
Solving these equations yields
a set of identical eigenvalues,
$\lambda_1 = \cdots = \lambda_{n-1}$.

We can show that the above optimal solution
represents the single-bin updating scheme.
%
Since the $k = 0$ eigenmode represents
a uniform shift of the bias potential,
we can set $\lambda_0 = \lambda_1$
without changing the nature of the updating scheme.
%
By Eqs. \eqref{eq:w_from_phi} and
\eqref{eq:eig_orthonormal_cols}, we have
\begin{align*}
  w_{ij}
  = \frac{1}{\rho_i} \sum_{k=0}^{n-1} \lambda_1 \, \phi_{ki} \, \phi_{kj}
  = \lambda_1 \, \delta_{ij}
  ,
\end{align*}
which is a multiple of the identity updating matrix
that defines the single-bin updating scheme.
%
%This demonstrates the asymptotic optimality of
%the single-bin updating scheme.
%
%We can further verify that
%$\theta_k = (t+t_0)/(T+t_0)$
%in this case,
%and the optimal schedule coincides with
%Eq. \eqref{eq:alpha_invt1}.
%
%The asymptotic error can be written as
%$\Err_A(T) = \sum_{ k = 1 }^{n-1} \Gamma_k \, T / (T + t_0)^2$,
%which is also identical to Eq. \eqref{eq:EA_sbin1}.
%
%\begin{align}
%  \Err_A(T)
%  =
%  \frac{       T     }
%       { (T + t_0)^2 }
%  \sum_{ k = 1 }^{n-1}
%    \Gamma_k
%  ,
%\end{align}
%




\subsubsection{\label{sec:bandpass}
Bandpass updating schemes}


We can generalize
the single-bin updating scheme to a class of
perfect (brick-wall) \emph{bandpass} updating schemes
that discard certain noise modes
of short wavelength.
%
If the eigenmodes are sorted in descending order
of the typical wavelength,
and the PMF is sufficiently smooth
to be represented by the first $K$
eigenmodes,
we may assign the eigenvalues of the first $K$ eigenmodes to $1$
and the remaining eigenvalues to $0$:
%
\begin{equation}
  \lambda_0 = \cdots = \lambda_{K-1} = 1
  ,
  \mathrm{\; and \;}
  \lambda_K = \cdots = \lambda_{n-1} = 0
  .
  \label{eq:lambda_bandpass}
\end{equation}
%
The updating matrix can then be reconstructed
from the eigenvalues by Eq. \eqref{eq:w_from_phi}.
%(cf. Appendix \ref{sec:homo_bandpass} for the case of homogeneous updating schemes).
%
%Then, the updating matrix is, according to
%Eq. \eqref{eq:w_from_phi},
%%
%$$w_{ij} = \frac{1}{\rho_i} \sum_{k=0}^{K-1} \phi_{ki} \, \phi_{kj}.$$
%
In effect, each updating step of the bandpass updating scheme
is equivalent to a single-bin updating step,
Eq. \eqref{eq:wl_update},
followed by a filtering step
that eliminates the components of the noise modes of $k \ge K$
from the bias potential.
%
%The asymptotic error is then
%$\Err_A(T) = \sum_{ k = 1 }^{ K - 1 } \Gamma_k T/(T + t_0)^2$,
%which is less than that from Eq. \eqref{eq:EA_sbin1}.
%
Like the single-bin updating scheme,
the bandpass updating scheme is also
asymptotically optimal,
but only up to the first $K$ fluctuation modes.

A distinct advantage of the bandpass updating schemes
is that they share with the single-bin updating scheme
the asymptotically optimal schedule,
the inverse-time schedule, Eq. \eqref{eq:alpha_invt1}.
%
Thus, it is unnecessary to go through the general procedure
described in Sec. \ref{sec:procedure}
for these updating schemes.

We will consider two special classes of bandpass updating schemes,
homogeneous and polynomial bandpass updating schemes.
%
%A summary of updating schemes is given in Fig. \ref{fig:scheme}.
%
First,
by applying this construction to the basis of cosine and sine functions,
we get a homogeneous bandpass updating scheme
(cf. Appendix \ref{sec:homo_bandpass}),
which resembles the Gaussian updating scheme,
but with long-range wiggles.
%as shown in Fig. \ref{fig:mat}(b).
%
In Sec. \ref{sec:lj},
we will further compare
the homogeneous bandpass updating scheme
with the Gaussian updating scheme
in an example.
%
%While both updating scheme are able to deliver
%a smooth and differentiable potential of mean force,
%the bandpass updating scheme is superior in asymptotic convergence,
%because of the set of clear-cut spectrum of eigenvalues.
%Eq. \eqref{eq:lambda_bandpass},
%instead of a Gaussian one, Eq. \eqref{eq:lambda_Gaussian},
%
%\footnote{As an estimate of error,
%  we note that truncating a linear or quadratic function
%  at mode $K$ for a non-periodic variable
%  results in an error roughly proportional to
%  $|\max_i u_i - \min_i u_i|^2/K^3$.}
%
The other class of polynomial bandpass updating schemes will be discussed below.

%\begin{figure}[h]\centering
%  \makebox[\linewidth][c]{
%    \includegraphics[angle=0, width=1.0\linewidth]{fig/scheme.pdf}
%  }
%  \caption{
%    \label{fig:scheme}
%    Summary of updating schemes.
%    %
%    Existing algorithms are shaded.
%    %
%    Only the Gaussian updating scheme
%    require explicit calculation for the optimal schedule,
%    because the optimal schedule of bandpass updating schemes
%    is always given by inverse-time formula.
%    %
%    The target distributions of all updating schemes are flat except
%    that for the adaptive Gaussian ensemble method,
%    which is a Gaussian distribution
%    or a mixture of Gaussian distributions.
%    \note{Do we need these descriptions, or the figure at all?}%
%  }
%\end{figure}






\subsection{\label{sec:bandpass_poly}
Polynomial bandpass updating scheme}


Here we consider a class of bandpass updating schemes
constructed from a sequence of orthogonal polynomials.
%
The resulting polynomial bandpass updating schemes are more intuitive
than the homogeneous bandpass updating scheme
in that the former express the bias potential as
a simple polynomial of the collective variable.

\subsubsection{Orthogonal polynomial basis}

For convenience, we
replace the discrete bin index, $i$,
by the continuous variable, $z$,
%
$\rho_i$ by $\rho(z) \, \delta z$,
$\phi_{ki}$ by $\phi_k(z) \, \delta z$,
and
$w_{ij}$ by $w(z, z') \, \delta z$, etc.
%
We define the eigenvectors by
$\phi_k(z) = R_k(z) \, \rho(z)$,
where $R_k(z)$ is the $k$th member of a sequence of orthogonal polynomials
that satisfy
\begin{align*}
  \int R_k(z) \, R_{k'}(z) \, \rho(z) \, d z
  = \overline{ R_k(z) \, R_{k'}(z) }
  = \delta_{k, k'}
  ,
\end{align*}
%
where the overline denotes the average for the distribution, $\rho(z)$.
%
The first few members are\footnote{The
  orthogonal polynomials share the same form
  for different target distributions,
  but the parameters,
  $\overline z$,
  $\sigma_z$,
  $\kappa_z$, etc.,
  are determined by the target distribution, $\rho(z)$.}
\begin{align*}
  R_0(z) = 1, \;
  R_1(z) = \frac{z - \overline{z}}{\sigma_z}, \;
  R_2(z) = \frac{ (z - \overline{z})^2 - \sigma_z^2 } {\kappa_z},
\end{align*}
%
where $\sigma_z \equiv \sqrt{\overline{\left(z - \overline z\right)^2}}$,
and
$\kappa_z \equiv \sqrt{\overline{\left[  (z - \overline z)^2 - \sigma_z^2  \right]^2 }}$.
%
We can readily verify that the eigenvectors satisfy
Eqs. \eqref{eq:eig_orthonormal_cols}-\eqref{eq:ortho0}.
%
\note{For $k = 0$, $R_0(z)$ must be a constant,
  and by the orthonormal condition, we have $R_0(z) = 1$.
  Thus, Eq. \eqref{eq:ortho0} is satisfied.}%
%
The eigenvectors allow us to construct the updating matrix
from Eq. \eqref{eq:w_from_phi}
with the eigenvalues given by \eqref{eq:lambda_bandpass}:
\begin{align*}
  w(z, z')
  =
  \sum_{k=0}^{K-1} \frac{ \phi_k(z) \, \phi_k(z') } { \rho(z) }
  =
  \sum_{k=0}^{K-1} R_k(z) \, R_k(z') \, \rho(z')
  .
\end{align*}
%
%where we have dropped the $k=0$ mode
%that corresponds to a uniform shift
%of the bias potential.
%
Then, by Eq. \eqref{eq:mbin_update},
the bias potential is updated as
%
$$
u(z, t+1) = u(z, t)
+ \sum_{k=0}^{K-1} R_k(z) R_k\bigl( z(t) \bigr) \, \alpha(t).
$$
%
We can write the bias potential
as a superposition of the orthogonal polynomials
%
\begin{equation}
  u(z, t) = \sum_{k=0}^{K-1} c_k(t) \, R_k(z),
  \label{eq:uz_decomp}
\end{equation}
%
with the variable coefficients updated as
%
\begin{equation}
  c_k(t+1) = c_k(t) + \alpha(t) \, R_k\bigl( z(t) \bigr)
  .
  \label{eq:ckupdate}
\end{equation}

Note that although the target distribution, $\rho(z)$,
does not explicitly appear in the bias potential, Eq. \eqref{eq:uz_decomp},
or the updating scheme, Eq. \eqref{eq:ckupdate},
it affects both via the parameters,
$\overline z$,
$\sigma_z$,
$\kappa_z$, etc.
%
For $k > 0$, $R_k(z)$ is a polynomial of degree $k$ with zero expectation,
%
and $R_k\bigl(z(t)\bigr)$ roughly
represents the deviation of the $k$th statistical moment
of the instantaneous distribution
from its expectation.
Because high-order moments are generally unreliable,
this method works better with a small $K$ for a narrow distribution.
%
However, by constructing
a mixture or expanded ensemble\cite{swendsen1986,
  *geyer1991, *hukushima1996, *hansmann1997, *sugita1999,
  *earl2005, *zuckerman2011, *rauscher2009,
  neuhaus2006, *neuhaus2007, kim2010,
  marinari1992,
  *lyubartsev1992, li2007,
  park2007, *nguyen2013, *zhang2015st, shirts2017},
we can simultaneously sample multiple target distributions
to cover a wider range, as exemplified below.


\subsubsection{Langfeld-Lucini-Rago (LLR) algorithm}

For example,
to sample a flat distribution, $\rho(z) = 1/\Delta z$,
over $\left(z_c - \frac 1 2 \Delta z, z_c + \frac 1 2 \Delta z\right)$,
we can use the Legendre polynomials\cite{arfken}.
%
If the interval is sufficiently small,
we can use $K = 2$ to limit the basis to the first two modes,
$R_0(z) = 1$ and $R_1(z) = 2 \sqrt{3} \, (z - z_c)/\Delta z$.
%
%The bias potential is a linear function of $z$,
If we drop the $k = 0$ mode
that corresponds to a uniform shift of the bias potential,
the updating scheme is essentially the LLR algorithm\cite{langfeld2012}
in the single-interval case,
and the inverse-time schedule
was recommended for the optimal convergence of $c_1$.\cite{pellegrini2014}
%
In practical applications,
the LLR algorithm is often applied simultaneously
to an array of adjacent intervals
to cover a wide range of the collective variable, $z$.\cite{langfeld2012, pellegrini2014}
%
%In this way, attempts to move out of the current interval
%are not necessarily rejected,
%but can be interpreted as transitions to
%another internal and accepted with proper probability.



\subsubsection{\label{sec:age}
Adaptive Gaussian ensemble}

In analogy to the LLR algorithm,
we will consider the case of sampling
a \emph{non-flat} Gaussian distribution
%$\rho(z) \propto \exp[-(z-z_c)^2/(2\sigma_z^2)]$,
\begin{equation}
  \rho(z)
  =
  \frac{1}{\sqrt{2\pi} \sigma_z}
  \exp\left[ - \frac{(z-z_c)^2}{2\,\sigma_z^2} \right]
  ,
  \label{eq:rho_Gaussian}
\end{equation}
using a basis of the Hermite polynomials\cite{arfken}.
%
As discussed above,
the method is effective only for
a local region of a multimodal distribution,
and thus we will assume a small width, $\sigma_z$.
%
Because the Gaussian distribution has no hard boundaries
as in the previous case,
we need to use,
in addition to the linear mode,
$R_1(z) = (z - z_c)/\sigma_z$,
a quadratic one,
$R_2(z) = \bigl[(z - z_c)^2 - \sigma_z^2\bigr] /\left(\sqrt 2 \, \sigma_z^2\right)$,
to restrict the sampling region.
%
The bias potential is thus quadratic,
which is common in umbrella sampling\cite{maragliano2006, *abrams2008, zhu2012}
and the Gaussian ensemble method\cite{hetherington1987,
*challa1988, *costeniuc2006, neuhaus2006, *neuhaus2007}.



\subsubsection{\label{sec:st}
Expanded ensemble}



We can expand the sampling range
by constructing an expanded ensemble
from a superposition of $S$ Gaussian umbrellas
centered at different places.
%
The expanded ensemble treats
the umbrella index, $s = 1, \dots, S$,
also as a random variable,
and the biased sampling targets a joint distribution
of the umbrella index
and the continuous variable, $z$,
$\rho(s, z) = \rho_s \, \rho^{(s)}(z)$.
%
Here,
the marginal inter-umbrella distribution is flat,
$\rho_s = 1/S$,
and the conditional intra-umbrella distribution, $\rho^{(s)}(z)$,
is the Gaussian given in Eq. \eqref{eq:rho_Gaussian}.
\footnote{This method
  should not be confused with the Gaussian updating scheme.
  %
  In the former case, the bias potential, $u^{(s)}(z)$,
  is quadratic
  and the conditional intra-umbrella target distribution, $\rho^{(s)}(z)$,
  is Gaussian,
  although the marginal inter-umbrella distribution, $\rho_s$, is flat.
  %
  In the latter case, the bias potential is a sum of
  Gaussian functions, and the target distribution
  is flat.}
%
To sample this distribution, we give each umbrella
an independent bias potential of the form of Eq. \eqref{eq:uz_decomp},
%
\begin{equation}
  u^{(s)}(z)
  %u(s, z)
  = \hat c_0^{(s)}
  + \frac{c_1^{(s)}}{\sigma_z^{(s)}} \bigl(z - z_c^{(s)}\bigr)
  + \sqrt 2 \, c_2^{(s)} \frac{\bigl(z-z_c^{(s)}\bigr)^2}
  {2 \, \bigl( \sigma_z^{(s)} \bigr)^2}
  ,
  \label{eq:u_age}
\end{equation}
%
with $\hat c_0^{(s)} \equiv c_0^{(s)} - c_2^{(s)}/\sqrt 2$,
and update the coefficients as
%
\begin{align}
  c_k^{(s)}(t+1) = c_k^{(s)}(t)
  + \frac{ \alpha(t) } { \rho_s } \, R_k^{(s)}\bigl( z(t) \bigr) \,
    \delta_{s, s(t)}
  ,
  \label{eq:ck_age_update}
\end{align}
%
where $s(t)$ is the umbrella at time $t$.
%

In the expanded ensemble,
inter-umbrella transitions can be handled via the frameworks of
parallel\cite{swendsen1986,
  *geyer1991, *hukushima1996, *hansmann1997, *sugita1999,
  *earl2005, *zuckerman2011, *rauscher2009,
  neuhaus2006, *neuhaus2007, kim2010}
and/or simulated tempering\cite{marinari1992,
  *lyubartsev1992, li2007,
  park2007, *nguyen2013, *zhang2015st}.
%
Using the latter framework,
the transition probability from
the $s$th umbrella to the $s'$th umbrella is given by
%
\begin{align}
A(s \to s') =
\min\left\{1,
  \frac{
    \exp\bigl[ - u^{(s')}\bigl( z(t), t \bigr) \bigr]
  }
  {
    \exp\bigl[ - u^{(s)}\bigl( z(t), t \bigr) \bigr]
  }
  %e^{
  %    u^{(s)}\bigl( z(t), t \bigr)
  %  - u^{(s')}\bigl( z(t), t \bigr)
  %}
  \right\}
  ,
  \label{eq:transprob_st}
\end{align}
where $z(t)$ is the current value of $z$.
%
In this case,
the parameter, $c_0^{(s)}$,
serves as the bias potential
for inter-umbrella transitions,
and it is updated by Eq. \eqref{eq:ck_age_update},
much like in the single-bin updating scheme.\cite{li2007}

Since the bandpass updating schemes are a straightforward extension
of the single-bin updating scheme,
we can borrow the existing protocol\cite{belardinelli2007,
  *belardinelli2007jcp, *belardinelli2008, *belardinelli2016}
to optimally control the updating magnitude.
%
We can adopt the WL stage-by-stage strategy early in simulation,
and switch to the inverse-time schedule
once the updating magnitude from the WL strategy
falls under the value given by the inverse-time formula.\footnote{We
  slightly modified the transition criterion
  from the WL to the inverse-time schedule.
  We allowed such a switch only
  upon the completion of a WL stage,
  and interpreted $t$ as the number of updating steps
  in this just completed WL stage.
  If the switch was successful,
  the above $t$ was used as the $t_0$
  in the inverse-time regime.}
  %
However, we need to redefine the measure of histogram fluctuation
used in the WL strategy for stage transitions
by Eq. \eqref{eq:F2st}
to include the intra-umbrella fluctuations.
%
%In the asymptotic regime, the optimal schedule
%is simply given by the inverse-time schedule, Eq. \eqref{eq:alpha_invt1}.
%%
%In initial stages, when the inverse-time schedule
%is still inapplicable,
%we can instead use the WL style stage-wise strategy
%to control the updating magnitude
%for the bandpass updating scheme,
%since it is a straightforward extension of the single-bin updating scheme.
%%
%However, the histogram fluctuation used for stage transitions
%should be computed from only the first $K$ modes,
%where the histogram flattening was performed

Upon convergence,
the parameters, $c_k^{(s)}$,
can be related to the PMF.
By Eqs. \eqref{eq:Vi_target} and \eqref{eq:rho_Gaussian}, we have
\begin{align*}
  u^{(s)}(z)
  %u(s, z)
  \approx
  \ln p(z)
  +
  \frac{ \bigl(z - z_c^{(s)} \bigr)^2 }
  { 2 \, \bigl( \sigma_z^{(s)} \bigr)^2 }
  +
  \mathrm{const.}
\end{align*}
%
Comparing this to Eq. \eqref{eq:u_age}, we get
\begin{subequations}
\begin{align}
  \hat c_0^{(s)}
  &\approx \ln p\bigl(z_c^{(s)}\bigr) + \mathrm{const.}
  ,
  %\label{eq:c0hat_limit}
  \\
  c_1^{(s)}
  &\approx \left. \frac{d \ln p(z) } { d z } \right|_{z = z_c^{(s)}}
  ,
  %\label{eq:c1_limit}
  \\
  \frac{
    \sqrt 2 \, c_2^{(s)} - 1
  } { \bigl( \sigma_c^{(s)} \bigr)^2 }
  &\approx \left. \frac{d^2 \ln p(z) } { d z^2 } \right|_{z = z_c^{(s)}}
  .
  %\label{eq:c2_limit}
\end{align}
\label{eq:ck_limit}
\end{subequations}
%
Thus,
$\hat c_0^{(s)}$ as a function of $z_c^{(s)}$
approximates the negative PMF, $\ln p(z)$;\cite{maragliano2006, *abrams2008}
$c_1^{(s)}/\sigma_z^{(s)}$
approximates the mean force at $z = z_c^{(s)}$.
%
and $\sqrt{2} \, c_2^{(s)}$,
serving as the relative ``pressure''
to restrain the width of the distribution,
would exceed $1$ in a barrier region
where the PMF is locally concave,
i.e. $-d^2\ln p(z)/dz^2 < 0$.
%
%
We also have approximately\cite{park2007, *nguyen2013, *zhang2015st}
\begin{equation}
  \hat c_0^{(s+1)} - \hat c_0^{(s)}
  \approx
  \frac{1}{2}\left(
    \frac{ c_1^{(s+1)} } { \sigma_z^{(s+1)} }
    +
    \frac{ c_1^{(s)} } { \sigma_z^{(s)} }
  \right)
  \left(
  z_c^{(s+1)} - z_c^{(s)}
  \right)
  .
  \label{eq:c0_est}
\end{equation}
%
%and upon convergence, $\exp c_1^{(s)}$ should be proportional to
%the partition function,
%$\int e^{-\beta \, \Phi(\mathbf x)-u^{(s)}(z(\mathbf x),t)} \, d\mathbf x$,
%as in simulated tempering,
%where
%$\beta = 1/(k_B T)$ is the inverse temperature,
%$\mathbf x$ denotes the internal degrees of freedoms,
%and
%$\Phi(\mathbf x)$ is the unbiased potential energy.
%


We summarize the method as the following procedure.

\begin{enumerate}

\item
  Set up an array of Gaussian umbrellas covering the desired range,
    e.g. we may set, for $s = 1, \dots, S$,
    the mean,
    $z_c^{(s)} = z_{\min} + \frac{s - 1}{S - 1} (z_{\max} - z_{\min})$,
    and
    the width, $\sigma_z^{(s)}$, to be a constant, $\sigma_z$.
    For the polynomial coefficients, we can set
    $c_1^{(s)}/\sigma_z^{(s)}$ to some estimated mean force, $\eta_z$;
    $c_2^{(s)} = 1/\sqrt 2$;
    and
    $c_0^{(s)} = \eta_z \bigl[ z_c^{(s)} - \frac{1}{2}(z_{\min} + z_{\max}) \bigr]$
    by Eq. \eqref{eq:c0_est}.

\item
  Equilibrate the system at umbrella $s = 1$.
  Repeat the following steps until the simulation ends.

\item
  Perform a regular MC or MD step
  in the current umbrella $s$ under
  the bias potential $u^{(s)}(z, t)$.

\item
  Attempt a transition to another umbrella
  with the acceptance probability given
    by Eq. \eqref{eq:transprob_st}.\cite{*[{The
    intra-umbrella sampling step and
    the inter-umbrella transition step
    can be executed at different frequencies
    without affecting the correctness of the algorithm.
    Although some prefers mixing the two steps
    in a random order,
    the sequential execution adopted here
    is also valid,
    as discussed in }] [{}]
    manousiouthakis1999}

\item
  Update the $c_k^{(s)}$'s
  according to Eq. \eqref{eq:ckupdate},
  and increase $t$ by $1$.

\item
  Change the updating magnitude, $\alpha(t)$.

\end{enumerate}

After simulation,
we can use the multiple histogram method\cite{ferrenberg1988, *ferrenberg1989,
*kumar1992, *roux1995, *bartels1997, *souaille2001,
*kobrak2003, *gallicchio2005, *chodera2007,
*fenwick2008, *kim2011, *shirts2008,
zhu2012, newman, frenkel}
to recover the unbiased distribution,
%
\begin{equation}
  p(z) \, \delta z
  \propto
  \frac{
    \sum_{s=1}^S n^{(s)}(z)
  }
  {
    \sum_{s=1}^S N^{(s)} \, e^{-u^{(s)}(z)}
  }
  ,
  \label{eq:WHAM}
\end{equation}
%
where $n^{(s)}(z)$
the number of visits to the bin
$\left(z-\frac{1}{2} \delta z, z + \frac{1}{2} \delta z\right)$
in umbrella $s$,
and the $N^{(s)}$ is the total number of visits
to the umbrella $s$.





\section{\label{sec:results}
Numerical results}



\subsection{\label{sec:lj}
Comparison of homogeneous updating schemes}

In this example, we will study the optimal schedule
and compare the single-bin,
Gaussian (Sec. \ref{sec:Gaussian_scheme}),
and homogeneous bandpass (Sec. \ref{sec:bandpass})
updating schemes.

\subsubsection{System and simulation details}

Our test system is
a Lennard-Jones (LJ) fluid of $N = 100$ particles,
and we wish to compute the PMF along
the distance, $r$, between two particular particles.
%
We will assume reduced units in the calculation.
%
The two particular particles, labeled as $1$ and $2$,
are mutually non-interacting
but do interact with other particles
via the pair potential $\varphi(r) = 4 \, \left(r^{-12} - r^{-6}\right)$
(truncated at half of the box size).
%
The PMF is the negative logarithm of
the intrinsic distribution of the distance
between the two tagged particles, $p(r)$,
which is proportional to the cavity distribution function\cite{hansen}.
%
%Formally, we have
%\begin{align*}
%  p(r) = \langle \delta(\mathbf r_{12} - \mathbf r) \rangle
%  =
%  \frac{V}{Z}
%  \int e^{-\beta \, \Phi(\mathbf r^N)}
%  d\mathbf r_3 \cdots d\mathbf r_N
%  ,
%\end{align*}
%where
%$V$ is the reduced volume,
%$\Phi(\mathbf r^{N}) = \sum_{j=3}^N\sum_{i=1}^{j-1} \varphi(r_{ij})$,
%and
%$Z = \int e^{\beta \Phi(\mathbf r^N)} d\mathbf r_1 \cdots d\mathbf r_N$.
%%
%This distribution is proportional to the cavity distribution function\cite{hansen}.
%as $y(r) = \left(1 - \frac{1}{N}\right) \, V \, p(r)$\cite{hansen}.

We used biased sampling to calculate
the PMF from $r = 0$ to half of the box size
on a grid of size $\delta r = 0.01$ in reduced units.
%
The reduced density was $\varrho = 0.1$.
The numbers of bins was $n = 500$.
The reduced temperature was $T = 3$.
%
The bias potential, $u(r)$,
was initially set to zero and
underwent the test updating scheme and schedule.
%
Each simulation started with an ``equilibration'' phase
of $10^7$ steps under a constant updating magnitude,
$a_0 = 10^{-4}$
(corresponding to $\ln f = n \, a_0 = 0.05$ in the WL algorithm),
followed by a production phase of
$T = 10^8$ steps
with the updating magnitude given by
either the inverse-time or optimal schedule.
%
The deviation of the bias potential was calculated from
Eqs. \eqref{eq:v_def} and \eqref{eq:x_def},
and the error from Eq. \eqref{eq:error_def},
with the reference bias potential
pre-calculated from a longer simulation
under the single-bin updating scheme.
%
We repeated each simulation multiple times
and report the averages.
%
To ensure the same schedule was used for multiple runs,
the optimal schedule was pre-computed using a set of fixed
$\Gamma_k$'s and $\epsilon_k$'s derived
from a longer simulation under
a smaller updating magnitude, $a_0 = 10^{-5}$.
\note{Maybe using a simulation with constant updating magnitude
using the block histogram method?}

We used the Metropolis algorithm\cite{frenkel, metropolis1953}
for configuration sampling.
%
Each step consisted of
one MC trial for displacing one of the two tagged particles,
and another four trials for displacing the other particles.
%
In each trial,
a random particle was displaced in each spatial dimension
according to a uniform distribution over $(-\varepsilon, \varepsilon)$.
%
The two tagged particles were fixed on the $x$-axis
and only displaced in this direction.
%
We used $\varepsilon = 0.1/\varrho$
to ensure a reasonable acceptance ratio.



\subsubsection{Results}




Both the optimal and potentially sub-optimal inverse-time schedules
reduced the error significantly,
as shown in Table \ref{tab:lj_error}.
%
The errors roughly matched the predicted values
calculated using the pre-computed
$\Gamma_k$'s and $\epsilon_k$'s.
%
The initial errors
at a relatively large updating magnitude
were generally underestimated,
suggesting that
the regular updating might have caused the sampling
to deviate from the quasi-equilibrium one
as assumed in our theory.


\begin{table}[h]
  \caption{\label{tab:lj_error}
    Errors of the bias potential under
    the single-bin,
    Gaussian (with width $\sigma = 0.28$),
    and homogeneous bandpass (with mode cutoff $K = 26$)
    updating schemes
    on the LJ system.
    %
    The numbers have been averaged over multiple runs,
    and the predicted values are shown in parentheses for comparison;
    $\Err_\mathrm{init.}$ and $\Err_\mathrm{final}$
    denote the errors at the beginning and end of the production phase, respectively.
    %
    \note{The reference values can be found
    on the second line of \texttt{verr.log}.
    To check use \texttt{getav.py} under \texttt{prog/lj},
    which will show the simulation and prediction values.}%
  }
  \setlength{\tabcolsep}{2pt}
  \renewcommand\arraystretch{1.4}
  \begin{tabular} { l c c c c }
    \hline
    Scheme & Schedule &
    $\Err_\mathrm{init.}$ &
    $\Err_\mathrm{final}$
    \\
    %\hline
    %\multicolumn{4}{c}{
    %  $\varrho = 0.1$,
    %  $500$ bins,
    %  $\Gamma_1 \approx 111$,
    %  $\Gamma \approx 1.1\times10^3$.
    %} \\
    \hline
    Single-bin & $1/t$
    & $5.7(4.5)\times10^{-2}$
    & $9.1(9.1)\times10^{-6}$
    \\
    %\hline
    Gaussian & $1/t$
    & $6.5(6.2)\times10^{-3}$
    & $1.4(1.4)\times10^{-5}$
    \\
    %\hline
    Gaussian & optimal
    & $6.0(6.2)\times10^{-3}$
    & $2.7(2.7)\times10^{-6}$
    \\
    %\hline
    Bandpass & $1/t$
    & $8.0(7.9)\times10^{-3}$
    & $1.7(1.7)\times10^{-6}$
    \\
    \hline
  \end{tabular}
\end{table}

As shown in Fig. \ref{fig:lj_alpha},
the optimal schedule for the Gaussian updating scheme
with width $\sigma = 0.28$
was more complex than the inverse-time one.
%
We chose the Gaussian updating scheme here because
the optimal and inverse-time schedules were identical
in the single-bin and bandpass updating schemes.
%
The optimal schedule lay above the inverse-time schedule,
and its initial decay was roughly exponential,
in agreement with the discussion in Appendix \ref{sec:schedule_geometry}.
%
Since our equilibration phase was sufficiently long,
the initial updating magnitude, $\alpha(0)$,
was around the ideal value of $a_0/2 = 5\times10^{-5}$
[cf. Eq. \eqref{eq:half_alpha0}].

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/lj_alpha.pdf}
  }
  \caption{
    \label{fig:lj_alpha}
    Optimal schedule of a Gaussian updating scheme.
  }
\end{figure}

The optimal schedule more effectively reduced
the error than the inverse-time schedule
for the Gaussian updating scheme,
as shown in Table \ref{tab:lj_error}.
%
By decomposing the error
into the components of the eigenmodes,
$\langle \tilde v_k^2 \rangle$'s,
by Eqs. \eqref{eq:gft_def} and \eqref{eq:wband_eigenvector_refl}
(Fig. \ref{fig:lj_xerr}),
we found that the inverse-time schedule
failed to reduce to the errors in the $k = 7$ to $21$ modes adequately.
%
By a delayed reduction,
the optimal schedule removed
more error in these modes,
and produced a flatter profile.
%(The remaining unevenness of the profile
%might be due to errors of the estimated values of
%$\Gamma_k$'s and $\epsilon_k$'s.)
%
However, the gain of using the optimal schedule
diminished for shorter simulations,
e.g. when the production phase was shortened to $T = 10^7$ steps,
the final errors under the optimal and inverse-time schedules
became $2.1\times 10^{-5}$ and $3.3\times 10^{-5}$, respectively,
which were more similar.

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/lj_xerr.pdf}
  }
  \caption{
    \label{fig:lj_xerr}
    Error components,
    $\langle \tilde v_k^2 \rangle$'s,
    of the PMF after $10^8$ production steps.
    %
    The optimal schedule more effectively
    reduced the errors of middle-range eigenmodes
    than the inverse-time schedule ($1/t$).
    The ``corrected'' values were obtained from
    Eq. \eqref{eq:uhatav}.
    The lines are a guide to the eyes.
  }
\end{figure}


For the Gaussian updating scheme,
the error depended not only on the schedule
but also on the width of the Gaussian.
%
Although widening the Gaussian
can suppress local noise,
it can also mask finer features of the PMF.\cite{laio2005}
%it also diminishes the eigenvalues
%[by Eq. \eqref{eq:lambda_Gaussian}]
%of more middle-range modes
%(which often represent finer features of the PMF)
%and hence slows down the error reduction there
%[by Eq. \eqref{eq:vt_diffeq_mbin}].
%
Thus, there is an optimal width that balances the two factors.
%
According to the analytical prediction,
the optimal width (assuming the optimal schedule)
was $\sigma = 0.28$, which we used in the simulations,
and it would decrease with the simulation length
and approach the limit of the single-bin updating scheme,
$\sigma \to 0$, asymptotically.

The homogeneous bandpass updating scheme
had a similar behavior.
%
The mode cutoff, $K$, also has an optimal value,
which was $26$ according to the prediction.
%
The minimal error of the bandpass case
was slightly less than the corresponding value
in the Gaussian case, as shown in Table \ref{tab:lj_error}.
%
The optimal cutoff would increase with the simulation length
and approach the limit of the single-bin updating scheme, $K \to n$,
asymptotically.


Although the Gaussian and homogeneous bandpass updating schemes
helped reduce the error in this case,
it was only because
the sampling along the collective variable
was free from major barriers
such that the histogram fluctuation was mainly local in nature.
%
For similar simulations performed at a higher density, $\varrho = 0.8$,
we found the gain of using the above two updating schemes
over the single-bin updating scheme, if any, to be negligible.
%
However, one may still prefer the multiple-bin updating schemes
in MD simulations
when a smooth bias potential is necessary
for deriving the bias force.

For simulations using sub-optimal
updating schemes and schedules,
we can use the histogram-based correction formula,
Eq. \eqref{eq:uhatav},
to improve the estimated PMF.
%
In Fig. \ref{fig:lj_xerr},
we show that this correction produced
the flatter profile seen in the single-bin case
for a simulation with a sub-optimal schedule.
%
While this correction also introduces
some local noise in the large $k$ modes,
the noise is likely harmless
for a post-simulation analysis, as
the bias potential no longer needs to be differentiated
for force calculation.
%
%For example, for the simulation in the $\varrho = 0.8$ case
%with $\sigma = 0.5$ and the inverse-time schedule,
%the correction was able to lower
%the error from $9.8\times10^{-3}$ to $3.3\times10^{-3}$.





\subsection{\label{sec:potts}
Adaptive Gaussian ensemble}


As another proof of principle,
we tested the adaptive Gaussian ensemble method
discussed in Secs. \ref{sec:age} and \ref{sec:st}
on the two-dimensional $L\times L$ ten-state
Potts model\cite{wu1982, newman, wang2001, wang2001pre}
in the energy space.
%
In this case, % $\Phi(\mathbf x) = 0$ and
the intrinsic distribution is
the density of states, $g(E)$.
%
We spread the Gaussian umbrellas
of the same width $\sigma_E^{(s)} = L$ over $[-1.8L^2, -0.8L^2]$
with an even spacing of $L$.
%
For simplicity, only transitions between neighboring
umbrellas were performed.\footnote{For
  wider umbrellas,
  one may need to mix the nearest-neighbor inter-umbrella transitions
  with some longer-range transitions
  to improve the convergence.}
%In attempting inter-umbrella transitions,
%the target umbrella is randomly chosen from
%either one of the nearest neighbors ($50\%$ chance),
%or any other umbrella.
%
%\note{Here, we need to rerun simulations.}%
%
Initially, we set
$c_1^{(s)} = \tilde \beta_c \, \sigma_E^{(s)}$
for all umbrellas
with $\tilde \beta_c = 1.4$
being the approximate inverse critical temperature.
%
For configuration sampling in each umbrella,
we used the Wolff algorithm\cite{wolff1989, newman}
(which was more efficient than the Metropolis algorithm)
at the inverse temperature, $\beta = c_1^{(s)}/\sigma_E^{(s)}$,
followed by a Metropolis step with
acceptance probability
$\min\bigl\{1, e^{c_2^{(s)} [R_2(E) - R_2(E')]} \bigr\}$,
where $E$ and $E'$ are the energies of the old and new configurations.
%
The acceptance ratio was no less than 91\% in any umbrella.
%
In the initial WL stages,
the initial updating magnitude was $0.01$;
the fluctuation threshold for stage transitions
was $F = 0.2$ [using the definition given by Eq. \eqref{eq:F2st}];
and the reduction factor was $1/2$.

The results for the $L = 64$ case
after $10^5 L^2$ steps are shown
in Fig. \ref{fig:pt_hist}.
%
The Gaussian histograms were evenly spaced with similar widths as intended,
%and the overall energy histogram was roughly flat,
as shown in Fig. \ref{fig:pt_hist}(a).
%
We computed the density of states by Eq. \eqref{eq:WHAM},
and found the critical temperature to be $T_c = 0.70161$,
in agreement with a previous study\cite{wang2001pre}.
%
Since Gaussian umbrellas were sufficiently narrow,
we could readily verify
Eqs. \eqref{eq:ck_limit}:
the logarithmic density of states, $\ln g(E)$,
was roughly captured
by $\hat c_0^{(s)}$ as a function of $E_c^{(s)}$,
as shown Fig. \ref{fig:pt_hist}(b);
%
$c_1^{(s)}/\sigma_E^{(s)}$, roughly representing
the local or statistical temperature, $\beta(E) = d\ln g(E)/dE$,
manifested the backbending behavior\cite{kim2006, *kim2007, kim2010}
typical in a first-order phase transition,
as shown in Fig. \ref{fig:pt_hist}(c);
%
$\sqrt 2 \, c_2^{(s)}$,
was indeed above $1.0$
in the unstable region,
where $\beta(E)$ is increasing or, equivalently,
$\ln g(E)$ is locally convex,
as shown in Fig. \ref{fig:pt_hist}(d).


\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/pt_hist.pdf}
  }
  \caption{
    \label{fig:pt_hist}
    Sampling using adaptive Gaussian ensembles
    on the $64 \times 64$ ten-state Potts model.
    %
    (a) Normalized histograms.
    %
    (b) $\hat c_0^{(s)} = c_0^{(s)} - c_2^{(s)}/\sqrt 2$ 
    against $E_c^{(s)}$ (circles)
    roughly agreed with the logarithmic density of states,
    $\ln g(E)$ (dashed line).
    In either case,
    we have deducted the linear contribution, $\beta_c \, E$,
    and applied a proper shift so as to normalize
    the distribution at the critical temperature, $\beta_c$.
    %
    (c) $c_1^{(s)}/\sigma_E$ (circles), roughly corresponding to
    the statistical temperature, $\beta(E) = d\ln g(E)/dE$;
    $\beta_c$ (dashed line).
    %
    (d) $\sqrt 2 \, c_2^{(s)}$
    exceeded $1.0$ in the unstable region,
    where $\beta(E)$ is increasing.
    \note{When the new data come,
    run the \texttt{update.sh},
    copy the new value of the inverse critical temperature,
    and the value in \texttt{lns for normalization \dots [value] (lnz)}
    in the output.
    Use these values to update \texttt{bc} and \texttt{shiftlnz}
    in \texttt{pt\_hist.gp}, and run \texttt{gnuplot} again.  }%
  }
\end{figure}


\section{\label{sec:conclusion}
Conclusions and Discussions}



In conclusion,
we have proposed a method for computing
the optimal schedule of the updating magnitude
for a general class of free energy calculation methods,
which we refer to as adaptive flat-distribution-sampling (FDS) methods
that include the Wang-Landau (WL) algorithm and metadynamics.
%
Adaptive FDS methods
incrementally update a bias potential
along a collective variable
to offset the potential of mean force (PMF)
and thereby sample a flat distribution.
%
The optimal schedule can improve the convergence
of the bias potential,
and thus reduce the effort
of computing the PMF.
%They differ by the updating schemes,
%which specify how the bias potential is modified
%in each updating step.
%%
%In the single-bin or WL case,
%the update is limited to the current bin;
%while metadynamics,
%the bias potential of a neighborhood of the current one
%is updated with relative weight specified by a Gaussian window.


The optimal schedule depends on the updating scheme,
which is mathematically characterized by an updating matrix,
with the element, $w_{ij}$,
representing the relative updating magnitude
at bin $i$ when bin $j$ is visited.
%
Our method computes the optimal schedule from
the eigenvalues of the updating matrix,
which reflect the relative rates of error reduction,
and the intrinsic histogram fluctuations
of the eigenmodes.
%
For the single-bin updating scheme (as often used in the WL case),
whose updating matrix is the identity matrix with identical eigenvalues,
we confirmed that the optimal schedule
is given by the known inverse-time formula,
Eq. \eqref{eq:alpha_invt1}.
%
For a general multiple-bin scheme,
including the Gaussian one often used in metadynamics,
the optimal schedule, having to balance
the different rates of error reduction of the eigenmodes,
is usually not given in a simple closed form,
but is implicitly given by an equation of motion
of a free particle with a position-dependent mass,
Eq. \eqref{eq:q_opt}.
%
%In this case,
%the optimal schedule and error
%can be sensitive to the simulation length
%and other parameters.

%In particular,
%the Gaussian updating scheme has a nontrivial optimal schedule
%and its width needs to be chosen carefully.
%%
%With a set of rapidly diminishing eigenvalues,
%the Gaussian updating scheme suppresses
%the accumulation of local noise in the bias potential
%%maintain the smoothness of the bias potential,
%at the cost of slowing down the reduction
%of the error in the middle-range eigenmodes.
%%
%Fortunately, the efficiency loss in
%reducing the error of the long- and middle-range eigenmodes
%can often be compensated
%by the histogram-based correction formula, Eq. \eqref{eq:uhatav},
%which ideally would remove the persistent error
%in these eigenmodes.
%%and recover a bias potential
%%similar to that from a simulation
%%using the single-bin updating scheme.


In comparing different updating schemes,
we found
the single-bin updating scheme to be optimal
for the error control in the long time limit.
%without a priori assumptions
%on the smoothness of the PMF.
%
The single-bin updating scheme can be generalized to
a class of bandpass updating schemes,
which discard a few noise modes
and are similarly optimal for the non-noise modes.
%
The bandpass updating schemes allow convenient control
for the updating magnitude
as the optimal schedule
is simply given by an inverse-time formula.

As an application,
we studied a particular class
of the bandpass updating schemes
constructed from a family of orthogonal polynomials,
which allows the bias potential
to be conveniently expressed as a low-order polynomial.
%with the coefficients adjusted on-the-fly.
%
This construction leads to a generalization
of the LLR algorithm\cite{langfeld2012, pellegrini2014},
and hopefully it could be used for
adaptive parameter adjustment
in umbrella sampling with
quadratic\cite{neuhaus2006, *neuhaus2007, zhu2012}
or similar\cite{martin-mayor2007, *sergio2011, *persson2013, kim2010}
bias potentials.



This study has several limitations.
%
First, we have ignored the \emph{systematic}
error\cite{zhou2005, morozov2007, zhou2008}
caused by the frequent updates to the bias potential.
%
While this is justified in the asymptotic regime,
where the updating magnitude is small
and the sampling is a quasi-equilibrium one,\cite{
  zhou2005, morozov2007, zhou2008, barducci2008, dama2014}
it is inadequate in
early stages of a simulation
(This is why we still have to resort to the WL stage-by-stage
technique even for the single-bin and bandpass updating schemes.)
%the random error,
%which is roughly proportional to
%the square root of the updating magnitude\cite{
%  zhou2005, morozov2007, zhou2008, bussi2006},
%can easily outweigh
%the systematic error,
%which is roughly proportional to
%the magnitude itself\cite{morozov2007}.
%
The white noise approximation
can also be inaccurate for relatively short simulations.
%
For the Gaussian updating scheme,
the calculation of the optimal schedule requires
the estimation of a few parameters
in a sufficiently long run
under a constant updating magnitude,
whereas the gain of using the optimal schedule
can sometimes be rather modest
for relatively short simulations,
even if the histogram fluctuation is largely
local noise.
%
Finally, the adaptive Gaussian ensemble method
introduced in Sec. \ref{sec:age} may still need refinement
to improve its general applicability and stability.
%e.g. to prevent the updating scheme from
%overly updating the parameters.
%Our immediate interest is to apply these relations to
%problems containing aqueous mixtures of proteins and nucleic acids.


\section{Acknowledgments}

We thank Dr. Y. Mei, Dr. W. Tang, J. Wang,
O. Nassar, Dr. C. Lai, Dr. S. Ou, D. Stuhlsatz,
Dr. C. Myers, and Dr. O. Samoylova
for helpful discussions,
and the anonymous reviewers for critical reading
of earlier versions of the manuscript.
%
We gratefully acknowledge the Robert A. Welch Foundation (H-0037),
the National Science Foundation (CHE-1152876)
and the National Institutes of Health (GM-037657)
for partial support of this work.
%
JM thanks support from the National Institutes of Health
(R01-GM067801, R01-GM116280),
the Welch Foundation (Q-1512),
and National Science Foundation (Grant PHY-1427654).
%
This research used computing resources of
the National Science Foundation XSEDE grid.
%


\appendix




\section{\label{sec:equilerr}
Moving average interpretation
}



We can understand
the optimality of the inverse-time schedule
for the single-bin updating scheme
by a moving average interpretation.
%
Here, the bias potential undergoing
the recurrence updating relation,
Eq. \eqref{eq:wl_update},
is mapped to an equivalent moving average
with a dynamic weight
determined by the updating schedule.
%
For the single-bin updating scheme,
the inverse-time schedule is optimal
because it corresponds to an equally-weighted average.


\subsection{Moving average and recurrence relation}

Consider the weighted cumulative moving average (CMA)
%
\begin{equation}
  X_\omega(t)
  =
  \frac{
    \omega(0) \, x(0) + \cdots + \omega(t) \, x(t)
  }
  {
    \Omega(t)
  }
  ,
  \label{eq:X_ave}
\end{equation}
%
where
$\Omega(t) \equiv \omega(0) + \cdots + \omega(t)$
is the cumulative weight.
%
We can write the average as a recurrence relation
%
\begin{equation}
  X_\omega(t)
  = X_\omega(t-1)
  +
  \alpha(t)
  \, [ x(t) - X_\omega(t-1) ]
  ,
  \label{eq:X_recur}
\end{equation}
%
where the updating magnitude is defined as
%
%The relative weight, $\omega(t)/\Omega(t)$,
%can be identified as the updating magnitude,
\begin{equation}
  \alpha(t) \equiv
  \frac{ \omega(t) } {\Omega(t)}
  =
  1 - \frac{ \Omega(t-1) } {\Omega(t)}
  ,
  \label{eq:alpha_from_Omega}
\end{equation}
%
and $x(t) - X_\omega(t-1)$
is the proposed change from the previous average
from the data point at step $t$.

Conversely, we can revert a recurrence relation
in the form of Eq. \eqref{eq:X_recur}
with a given $\alpha(t)$
to an average in the form of Eq. \eqref{eq:X_ave}
with the weight given by
\begin{align}
  \frac{ \omega(t) } { \omega(0) }
  =
  \left(
    \prod_{\tau=1}^{t-1} \frac{1}{1- \alpha(\tau)}
  \right)
  \frac{ \alpha(t) } { 1 - \alpha(t) }
  .
  \label{eq:omega_from_alpha}
\end{align}

%The inverse-time schedule, Eq. \eqref{eq:alpha_invt1},
For example,
the equally-weighted average, $\omega(t) = \omega(0)/t_0$ (for $t > 0$),
corresponds to the inverse-time updating magnitude,
Eq. \eqref{eq:alpha_invt1}.
%
Updating magnitudes above and below this value
are typical in CMAs of
growing and diminishing weights, respectively.


\subsection{\label{sec:ave_us}
Application to updating schemes}

To apply the above formalism to the updating schemes,
we need a correction formula for the bias potential.
%
We will use a linearized version of Eq. \eqref{eq:vcorr_equil},
%$$\hat u_i = u_i + \frac{ H_i } { \rho_i } - 1.$$
%
\begin{equation}
  \hat u_i = u_i + \frac{ H_i } { \rho_i } -1
  .
  \notag
\end{equation}
%
Using this formula on
the instantaneous histogram, $h_i(t)$,
yields an independent estimate of the ideal bias potential
from a single data point at step $t$,
%
\begin{equation}
  \hat u_i(t)
  \approx u_i(t) + \frac{ h_i(t) } { \rho_i } - 1
  = u_i(t) + f_i(t) - 1
  .
  \label{eq:uhatt}
\end{equation}

For the single-bin updating scheme, Eq. \eqref{eq:wl_update},
we have
\begin{align}
  u_i(t+1)
  &=
  u_i(t) + \alpha(t) \, f_i(t)
  \notag \\
  &\approx
  u_i(t) + \alpha(t) \, [\hat u_i(t) - u_i(t)] + \mathrm{const.}
  ,
  \notag
  %\label{eq:u_uhat}
\end{align}
where we have used Eq. \eqref{eq:uhatt}
and ignored a uniform shift in the second step.
%
Comparing this to Eq. \eqref{eq:X_recur},
we find that the time evolution of $u_i(t+1)$
is the same as that of the CMA $X_\omega(t)$
of $x(t) = \hat u_i(t)$.
%
Thus, the bias potential can be thought as
a CMA with the weight given by Eq. \eqref{eq:omega_from_alpha}.
%
The inverse-time schedule is optimal
because it corresponds to the equally-weighted average,
which is statistically most efficient.

Similarly, for the multiple-bin updating scheme,
we have, from Eqs. \eqref{eq:v_def} and \eqref{eq:vkupdate},
${\tilde u}_k(t + 1) = {\tilde u}_k(t) + \lambda_k \, \alpha(t) \, {\tilde f}_k(t)$,
and from Eq. \eqref{eq:uhatt},
$\hat{\tilde u}_k(t) \approx {\tilde u}_k(t) + {\tilde f}_k(t)$,
for $k > 0$.
Thus,
\begin{align*}
  \tilde u_k(t+1) = \tilde u_k(t) + \lambda_k \, \alpha(t) \,
  \left[
    \hat{\tilde u}_k(t) - \tilde u_k(t)
  \right]
  ,
\end{align*}
%
which means that
$\tilde u_k(t+1)$ is equivalent to
a CMA of $\hat{\tilde u}_k(t)$,
and the weight is given by Eq. \eqref{eq:omega_from_alpha},
with $\alpha(t)$ replaced by the effective updating magnitude,
$\lambda_k \, \alpha(t)$.



\subsection{\label{sec:ave_ub}
Upper bound of the updating magnitude}

The moving average interpretation imposes
an upper bound on the updating magnitude.
%
By Eq. \eqref{eq:alpha_from_Omega},
the effective updating magnitude, $\lambda_k \, \alpha(t)$,
of any mode $k > 0$
should not exceed $1.0$.
But this is not necessarily satisfied
for the updating schedule
derived from the continuous-time formalism.
%
An expedient fix,
which we used in Sec. \ref{sec:procedure},
is to derive the effective updating magnitude
from Eq. \eqref{eq:alpha_from_Omega}
for the $k = 1$ mode with the largest eigenvalue,
i.e.
$\alpha(t) = \lambda_1^{-1}[1-\Omega(t-1)/\Omega(t)]$,
with
\begin{align*}
  \ln \frac{ \Omega(t-1) } { \Omega(t) }
  &\approx
  \frac{1}{t_+ - t_-} \ln \frac{ \Omega(t_-) } { \Omega(t_+) }
  =
  \!\!\sum_{\tau=t_- + 1}^{t_+}
  \frac{ \ln[1- \lambda_k \, \alpha(\tau)] } { t_+ - t_- }
  \\
  &\approx
  -\frac{ \lambda_k \int_{t_-}^{t_+} \alpha(\tau) \, d\tau}{t_+ - t_-}
  =
  -\lambda_k \, \frac{q(t_+) - q(t_-)}{t_+ - t_-}
  ,
\end{align*}
where $t_-$ and $t_+$ are two neighboring grid points
around $t$.
%
This guarantees $\lambda_k \, \alpha(t) \le 1$
for all nontrivial modes.



\subsection{Histogram correction}



For a general multiple-bin updating scheme,
each mode of bias potential
corresponds to a CMA with a dynamic weight,
which is not necessarily the optimal equal weight.
%
Below we give a simple correction
to recover the equally-weighted average.
%
We will use capital letters to denote equally-weighted averages.
%e.g. for variable $y$,
%$Y(t) \equiv \frac{1}{t} \sum_{\tau=1}^t y(\tau)$.
%\begin{equation}
%  Y(t) \equiv \frac{1}{t} \sum_{\tau=1}^t x(\tau)
%  .
%  \notag
%  %\label{eq:timeav}
%\end{equation}
%
Then, by averaging both sides of Eq. \eqref{eq:uhatt}, we get
\begin{align}
  \hat U_i(t)
  &\approx
  U_i (t)
  + F_i(t) - 1
  \approx
  U_i(t)
  + \ln \frac{ H_i(t) } { \rho_i }
  .
  \label{eq:uhatav}
\end{align}
%
This correction serves as
a generalization of Eq. \eqref{eq:vcorr_equil}
for FDS simulations under
a variable bias potential.\footnote{We can also
  derive Eq. \eqref{eq:uhatav}
  by forming an independent estimator
  of the unbiased distribution\cite{marsili2006},
  which is, by Eq. \eqref{eq:Vi_target},
  $\hat p_i(t) \approx h_i(t) \, e^{ u_i(t) }$,
  assuming a proper shift of $u_i(t)$.
  Time averaging the equation yields
  $\rho_i \, e^{\hat U_i(t)} = \hat P_i(t) \approx H_i \, e^{ U_i(t) }$.}
%
However, it does not remove the systematic bias
caused by the adaptive updates,
and thus still requires a sufficiently small updating magnitude.







\section{\label{sec:schedule_geometry}
Characterization of optimal schedules}



Here we give a geometric characterization
of the optimal schedule.
%
First, we define a normalized mass distribution as
%
\begin{equation}
  m(\bar q)
  =
  \frac{
    M(\bar q)
  }
  {
    \int_0^{ q(T) } M(q') \, d q'
  }
  =
  \frac{
    M(\bar q)
  }
  {
    C_M
  }
  ,
  %\notag
  \label{eq:mass_distr}
\end{equation}
%
such that
$\int_0^{q(T)} m(\bar q) \, d\bar q = 1$.
%
Then from Eqs. \eqref{eq:Lagrangian_const} and \eqref{eq:q_opt},
we find that for $\bar q(t) \equiv q(T) - q(t)$,
%
\begin{align}
  m\bigl( \bar q(t) \bigr)
  &=
  \frac{ 1 }
       { T \, \alpha(t) }
  ,
  \label{eq:mQ_invTa}
  \\
  \int_{\bar q(t)}^{ q(T) }
    m(\bar q) \, d \bar q
  &=
  \frac t T
  .
  \label{eq:intmQ_tT}
\end{align}
%
This allows each point on the schedule,
$\bigl(t, \alpha(t)\bigr)$,
to be mapped to a point,
$\Bigl(\bar q(t), m\bigl(\bar q(t)\bigr)\Bigr)$,
on the mass distribution,
such that the ordinate of the latter curve
is $1/(T\alpha)$,
and the area under the curve in the domain, $[\bar q(t), q(T)]$,
or the complementary cumulative distribution function of $m(\bar q)$,
is equal to $t/T$,
as shown in Fig. \ref{fig:massq}(a).

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=\linewidth]{fig/massq.pdf}
  }
  \caption{
    \label{fig:massq}
    (a) Geometric construction of the optimal schedule,
    $\alpha(t)$,
    from the normalized mass distribution,
    $m(\bar q)$.
    %
    (b) Inverse construction of $m(\bar q)$
    from the optimal schedule $\alpha(t)$.
  }
\end{figure}

We can now show that
for simulations with equal lengths and initial updating magnitudes,
the optimal schedule for the Gaussian updating scheme
always lies above the inverse-time schedule,
which is optimal for the single-bin updating scheme.
%
From Eqs. \eqref{eq:mQ_invTa}, \eqref{eq:intmQ_tT},
\eqref{eq:mass_distr}, and \eqref{eq:mass_func},
we have
%
\begin{align*}
  %\frac{d\alpha^{-1}}{dt}
  \frac{d}{dt}\left(\frac{1}{\alpha(t)}\right)
  &=
  \frac{ dm } { d(t/T) }
  =
  -\frac{dm}{m\,d\bar q}
  =
  -\frac{1}{2} \frac{ dM^2(\bar q)/d\bar q }
  { M^2(\bar q) }
  %\notag
  \\
  &
  =
  \frac{ \sum_{k=1}^{n-1} \Gamma_k \, \lambda_k^3 \, e^{-2\lambda_k \bar q} }
  { \sum_{k=1}^{n-1} \Gamma_k \, \lambda_k^2 \, e^{-2\lambda_k \bar q} }
  \equiv
  \bar\lambda(\bar q)
  ,
\end{align*}
%
which is a weighted average of $\lambda_k$'s
with the weight being $\Gamma_k \, \lambda_k^2 \, e^{-2\lambda_k \bar q}$.
%
For the Gaussian updating scheme,
we have $\bar \lambda(\bar q) < 1$ for $\bar q > 0$
by Eq. \eqref{eq:lambda_Gaussian}.
%
Thus the inverse updating magnitude
increases more slowly
in the Gaussian updating scheme
than in the single-bin updating scheme,
resulting in a larger updating magnitude
in the former case.

The schedule, $\alpha(t)$,
also defines a variable transformation from
the mass distribution, $m(\bar q)$.
%much like the Legendre transform.
%
If we define
$\bar q^* \equiv \bar q/q(T)$
and $t^* \equiv t/T$
such that both are limited to the interval $[0, 1]$,
as well as
\begin{align*}
  m^*       &\equiv q(T) \, m(\bar q) = -\frac{ dt^* } { d\bar q^* },
  \\
  \alpha^*  &\equiv \frac{T}{q(T)} \, \alpha(t) = -\frac{ d\bar q^* } { dt^* },
\end{align*}
then it is clear that
both $m^*(\bar q^*)$ and $\alpha^*(t^*)$
can describe the differential relation
between $\bar q^*$ and $t^*$,
but with the natural variables
being $\bar q^*$ and $t^*$, respectively.
%
Further, the transformation is an involution:
if we apply the same transform on $\alpha^*(t^*)$
we would recover $m^*(\bar q^*)$,
as shown in Fig. \ref{fig:massq}(b).
%
A few examples are shown in Table \ref{tab:m_and_a}.
%
For example, in the single-bin updating scheme,
the mass distribution decays exponentially,
and we would have the inverse-time schedule.
%
On the other hand,
the eigenvalues of the Gaussian updating scheme
can differ greatly in magnitude
[cf. Eq. \eqref{eq:lambda_Gaussian}].
%
Then, at a large $\bar q$,
the sum in Eq. \eqref{eq:mass_func} will be dominated by
the largest term, where $\lambda_k \approx 1/\bar q$,
and $m(\bar q) \propto M(\bar q) \propto 1/\bar q$,
and we would expect a roughly exponentially-decaying schedule.

\begin{table}[h]
  \caption{\label{tab:m_and_a}
    Examples of the variable transformation between the
    reduced schedule, $\alpha^*(t^*)$,
    and the reduced mass distribution, $m^*(\bar q^*)$,
    as defined in Eqs. \eqref{eq:mQ_invTa}
    and \eqref{eq:intmQ_tT}.
  }
  \setlength{\tabcolsep}{2pt} % spacing between columns
  \renewcommand\arraystretch{2.0} % spacing between rows
  \begin{tabular} { c c c }
    \hline
    $m^*(\bar q^*)$ &
    $\alpha^*(t^*)$ &
    $\bar q^*(t^*)$
    \\
    \hline
    $(1+ s_\gamma) \, \gamma \, e^{-\gamma \, {\bar q}^*}$ &
    $\dfrac{1}{ \gamma \, (t^* + s_\gamma) }$ &
    $\dfrac{1}{\gamma} \ln\left(\dfrac{1 + s_\gamma}{t^* + s_\gamma} \right)$
    \\
    $\dfrac{1}{\gamma \, ({\bar q}^* + s_\gamma) }$ &
    $(1 + s_\gamma) \, \gamma \, e^{-\gamma \, t^*}$ &
    $(1 + s_\gamma) \, e^{-\gamma \, t^*} - s_\gamma$
    \\
    $\dfrac{c \, a}{\left( {\bar q}^* + q^*_0 \right)^{a+1}}$ &
    $\dfrac{c^{\frac 1 a} / a}{\left( t^* + t^*_0 \right)^{\frac{1}{a}+1}}$ &
    %$\left( \dfrac{c}{t^* + t^*_0} \right)^{1/a} - q^*_0$
    $q^*_0 \left[\left( \dfrac{1+t^*_0}{t^* + t^*_0} \right)^{\!\frac{1}{a}} \!- 1\right]$
    %$\,\begin{aligned}
    %  &a, q^*_0 > 0, \\
    %  &\frac{1}{t^*_0} \equiv \left(1+\frac{1}{q^*_0}\right)^a - 1, \\
    %  &c \equiv {q^*_0}^a(t^*_0 + 1)
    %\end{aligned}$
    \\
    \hline
    \multicolumn{3}{p{8cm}}{
    Here,
    $\gamma > 0$,
    and we have defined
    $s_\gamma \equiv 1/(e^\gamma - 1)$,
    ${t^*_0}^{-1} \equiv \left(1+{q^*_0}^{-1}\right)^a - 1$,
    and
    $c \equiv {q^*_0}^a(1 + t^*_0)$.
    } \\
    \hline
  \end{tabular}
\end{table}




\section{\label{sec:Gamma_measure}
Integrals of the autocorrelation functions of the eigenmodes
}



Here we give two methods of estimating
the integrals of the autocorrelation functions, $\Gamma_k$'s,
in an adaptive FDS simulation
under a constant updating magnitude.
%
They have the advantage of not requiring
explicit computation of
the autocorrelation functions of the eigenmodes.

First, because the difference between $u_i$ and $v_i$,
is a constant of time [cf. Eq. \eqref{eq:v_def}],
${\tilde u}_{k}$ and ${\tilde v}_{k}$
share the same variance.
%
%$\operatorname{var} {\tilde u}_{k} =
% \operatorname{var} {\tilde v}_{k} =
% \left\langle {\tilde v}_{k}^2 \right\rangle$,
%\begin{equation*}
%  \operatorname{var} {\tilde u}_{k}
%  =
%  .
%\end{equation*}
%
Then, from Eqs. \eqref{eq:xt_eql} and \eqref{eq:xt2_eql}, we have
%
\begin{equation}
  \operatorname{var} {\tilde u}_k
  =
  \operatorname{var} {\tilde v}_{k}
  \approx
  \frac{1}{2} \, \Gamma_k \, \lambda_k \, a_0
  %\left( 1 - e^{-2 \, \lambda_k \, a_0 \, T_0} \right)
  ,
\label{eq:varXt}
\end{equation}
%
for a sufficiently long period, $T_0$.
%
We can compute the variance, $\operatorname{var} \tilde u_k$,
by trajectory averaging in a long simulation,
and use Eq. \eqref{eq:varXt} to estimate $\Gamma_k$.
%
However, this method would fail for a near zero, $\lambda_k$,
and it is best used for the single-bin updating scheme,
where $\lambda_k \equiv 1$.

To use the method in a simulation under a multiple-bin updating scheme,
we can construct a virtual bias potential, $u_i^\mathrm{vir}$,
that emulates the bias potential in a simulation
under the single-bin updating scheme.
%
This virtual bias potential can be constructed as (cf. Appendix \ref{sec:ave_us}),
\begin{equation}
  u_i^\mathrm{vir}(t+1)
  =
  u_i^\mathrm{vir}(t)
  +
  a_0 \, [\hat u_i(t) - u_i^\mathrm{vir}(t)]
  ,
  \notag
\end{equation}
%
where $\hat u_i(t)$ is given by Eq. \eqref{eq:uhatt}.
%
We can then apply Eq. \eqref{eq:varXt} to $u_i^\mathrm{vir}$
with the effective $\lambda_k \equiv 1$.

The above method
requires a sufficiently small
the updating magnitude,
$a_0 \ll 2/\left(\max_k \Gamma_k \right)$,
and it tends to overestimate the value of $\Gamma_k$,
since Eq. \eqref{eq:varXt}
does not take into account
the non-equilibrium sampling effect
caused by frequent updating.


Alternatively, we can estimate $\Gamma_k$ using Eq. \eqref{eq:Fk2_Gammak}.
%
We can split the trajectory into blocks of $T_\mathrm{blk}$ steps,
compute the histogram fluctuation, $\tilde F_k(T_\mathrm{blk})$,
and estimate $\Gamma_k$ from the average of
$T_\mathrm{blk} \, \tilde F_k^2(T_\mathrm{blk})$
over the blocks.
%
For this method to work,
the trajectory block should be large enough
to cover at least a few autocorrelation times (which is roughly $\Gamma_k/2$),
but also sufficiently small
to avoid the artificial histogram flattening
caused by the frequent non-equilibrium updates.
%
Thus, we require $\max_k \Gamma_k/2 \ll T_\mathrm{blk} \ll 1/a_0$.




\section{\label{sec:homo}
Homogeneous updating schemes
}


Here, we discuss some details
on the homogeneous updating schemes.
%
We can characterize
the updating matrix, $w_{ij}$,
of a homogeneous updating scheme
by a rigid window function
that shifts with the current bin $j$
(ignoring the boundary adjustment).
%
The unadjusted window function is defined by
the relative updating magnitude, $\mu_l$,
on the $l$th bin from the central bin.
%
%The translational invariance requires $\rho_i$ to be flat,
%and the updating matrix, $\mathbf w$,
%must be symmetric to satisfy detailed balance,
%Eq. \eqref{eq:w_detailedbalance}.
%
By the translational invariance,
the updating matrices of homogeneous updating schemes
share the same set of eigenvectors,
and the eigenvalues
are related to the window function as a cosine transform.



\subsection{\label{sec:wband_eig}
Eigenvalues and window function}



For a periodic variable\cite{dama2014},
the element of the updating matrix, $w_{ij}$,
can only be a function of the difference of indices, $i-j$, modulo $n$,
and we write it as
%
\begin{equation}
  w_{ij}
  =
  \mu_{i-j}
  +
  \mu_{i-j+n}
  +
  \mu_{i-j-n}
  ,
  \notag
  %\label{eq:w_band_pbc}
\end{equation}
%
for some numbers, $\mu_l$'s, which
define the unadjusted window function.
%
For simplicity, we will assume that
the $\mu_l$'s are symmetric, $\mu_l = \mu_{-l}$,
and they can be cutoff at $l = b$
(i.e. $\mu_l = 0$ for $l > b$),
where $b$ is the integral part of $(n-1)/2$.
%
\note{The cutoff $b$ is a bit tricky to define
in order to encompass the cases of even and odd $n$.}%
%
To satisfy Eq. \eqref{eq:w_sumj}, we further impose
the normalization, $\sum_{l=-b}^b \mu_l = 1$.
%
%\begin{equation}
%  \mu_{-b} + \cdots + \mu_b = 1
%  .
%  \notag
%  %\label{eq:mu_normalization}
%\end{equation}



To find the eigenvectors,
we define for a periodic variable, $\phi_i$,
the out-of-boundary values by
$\phi_i = \phi_{i \pm n}$,
%
such that $i \pm n$ lies in between $1$ and $n$.
%
Then, the multiplication of the matrix, $\mathbf w$,
is equivalent to a convolution with the window function:
%
\begin{equation}
  \sum_{ j = 1 }^n
    w_{ij} \, \phi_j
  =
  \sum_{ j = 1 - n }^{ 2 \, n }
    \mu_{i - j} \, \phi_j
  =
  \sum_{ l = -b }^{ b }
    \mu_l \, \phi_{ i - l}
  .
  \notag
  %\label{eq:wmul_to_convol}
\end{equation}
%
The characteristic equation,
$\mathbf w \, \pmb\phi = \lambda \, \pmb\phi$
can then be solved by discrete Fourier transform,
with the orthogonal eigenvectors,
$\pmb\phi^{(1)}, \dots, \pmb\phi^{(n)}$,
given by
%
\begin{equation}
  \phi^{(k)}_i
  =
  \phi_{ki}
  =
  \frac{ \sqrt{ 2 - \delta_{k,0} } } { n }
  %\overset{ \cos } { \sin }
  \,
  {\cos \atop \sin}
  \left(
    \frac{ k \, i \, 2 \, \pi }
         {      n             }
  \right)
  ,
  \notag
\end{equation}
%
where the function takes the cosine form for $k \le n/2$,
or the sine form otherwise.
%
%Then one can readily verify that
The eigenvalues are given by
\begin{equation}
  \lambda_k
  =
  \mu_0
  +
  2 \,
  \sum_{ l = 1 }^b
  \mu_l
  \cos\left(
  \frac{ k \, l \, 2 \, \pi }
       {      g \, n        }
  \right)
  ,
  \label{eq:wband_eigenvalue}
\end{equation}
%
with $g = 1$.
Note also the two-fold degeneracy,
%There is a two-fold degeneracy,
  $\lambda_{n - k} = \lambda_k$.


For a non-periodic variable,
we will use the reflective boundary condition\cite{bussi2006},
which sets up two reflective mirrors at
$i = 1/2$ and $i = n + 1/2$.
%
Then the variable and the images in the two mirrors
can be mapped to an equivalent periodic variable of period $2 \, n$.
%
Thus, we can borrow the eigenvalues from
Eq. \eqref{eq:wband_eigenvalue}
with a doubled period, i.e. $g = 2$,
and an increased cutoff $b = n - 1$
(so $\mu_n = 0$).
%
The eigenvectors sharing the same periodicity
and being even about $i = 1/2$ are given by
%
\begin{equation}
  \phi^{(k)}_i
  =
  \phi_{k i}
  =
  \frac{ \sqrt{ 2 - \delta_{k, 0} } }
       {             n              }
  \cos \left[
       \frac{ k \, \left( i - \frac 1 2 \right) \, 2 \, \pi}
            {             2 \, n                           }
       \right]
  .
  %\notag
  \label{eq:wband_eigenvector_refl}
\end{equation}
%
The updating matrix can be reconstructed
by Eq. \eqref{eq:w_from_phi},
%
%
\begin{equation}
  w_{ij}
  =
  \mu_{ i - j }
  +
  \mu_{ i + j - 1 }
  +
  \mu_{ i + j - 2 n - 1 }
  .
  \notag
  %\label{eq:w_band_refl}
\end{equation}
%
%where the last two terms on the right hand side
%help prevent unintended distortion\cite{dickson2011, mcgovern2013}
%to the equilibrium distribution\cite{bussi2006}.


%\subsection{\label{sec:invert_wband}
%Inversion}

We can recover the window function from the eigenvalues
by inversely transforming Eq. \eqref{eq:wband_eigenvalue},
%
\begin{equation}
  \mu_i
  =
  \frac { 1 } { g \, n }
  \sum_{ k = 0 }^{ g \, n - 1 }
  \lambda_k
  \cos \left(
       \frac{ k \, i \, 2 \, \pi }
            {      g \, n        }
  \right)
  .
\label{eq:mu_from_lambda}
\end{equation}
%
In the non-periodic case,
we have defined
$\lambda_k \equiv \lambda_{2 \, n - k}$
for $k = n + 1, \dots, 2 \, n - 1$,
as well as
%
\begin{align}
  \lambda_n
  =
  (-1)^{ n - 1 }
  \lambda_0
  +
  2 \, \sum_{ k = 1 }^{ n - 1 }
      (-1)^{n - k - 1} \, \lambda_k
  ,
\label{eq:lambdan}
\end{align}
to satisfy the constraint, $\mu_n = 0$.
%





\subsection{\label{sec:Gaussian_math}
Gaussian updating scheme}



The Gaussian updating scheme (cf. in Sec. \ref{sec:Gaussian_scheme})
is associated with a Gaussian-shaped window function, $\mu_l$.
%
%This updating scheme is commonly
%used in metadynamics.
%
Below we show its convergence
in the continuous limit,
and how to maintain the convergence
on a finite grid.



We recall that
the bin index, $i$, can be related to
the continuous variable $z$ as
$z = i \, \delta z$,
where
$\delta z$ is the bin size.
%
We further define
$\mu(z) \equiv \mu_i/\delta z$
and
$\Delta z \equiv (n \, \delta z) \, g / 2$.
%
For $n \gg 1$,
we can approximate Eq. \eqref{eq:wband_eigenvalue}
as an integral:
%
\begin{equation}
  \lambda_k
  =
  \int_{-\Delta z}^{\Delta z}
    \mu(z) \, \cos\left( \frac{ \pi \, k \, z } { \Delta z} \right)
    \, d z,
  \notag
  %\label{eq:lambda_int}
\end{equation}
%
with the normalization,
$\int_{-\Delta z}^{\Delta z} \mu(z) \, dz = 1$.

If the width of the Gaussian, $\sigma_z$,
is much less than $\Delta z$,
then we can extend the limits of the integrals
to infinity, and normalize $\mu(z)$ as
%
\begin{equation}
  \mu(z)
  \approx
  \frac{            1            }
       { \sqrt{ 2 \pi } \sigma_z }
  %
  \exp\left(
        -\frac{   z^2   }
              { 2 \, \sigma_z^2 }
      \right)
  .
\notag
\end{equation}
%
Then, from Eq. \eqref{eq:wband_eigenvalue},
we get Eq. \eqref{eq:lambda_Gaussian},
which shows that all eigenvalues are positive.
%
However,
when discretized on a finite grid,
some eigenvalues can turn negative.
%
To keep the eigenvalues positive,
we can redefine the Gaussian updating scheme
from the eigenvalues given by Eq. \eqref{eq:lambda_Gaussian},
and then inversely computing the window function
using Eq. \eqref{eq:mu_from_lambda},
or the updating matrix using Eq. \eqref{eq:w_from_phi}.




\subsection{\label{sec:homo_bandpass}
Homogeneous bandpass updating schemes}



In a homogeneous bandpass updating scheme,
the eigenvalues are either $0$ or $1$.
%and it is convenient to set $\lambda_0 = 1$.
%
We can use Eq. \eqref{eq:mu_from_lambda}
to find the window function.
%
For a periodic variable,
we will take into account the two-fold degeneracy
and modify Eq. \eqref{eq:lambda_bandpass}
by setting $\lambda_{n-K+1}, \dots, \lambda_{n-1}$
also to $1$.
%
Then, $\mu_l = \operatorname{sinr}(K, l, n)$ for $l \le b$
and $w_{ij} = \operatorname{sinr}(K, i-j, n)$,
where we have defined
\begin{equation}
  \operatorname{sinr}(K, l, n)
  \equiv
  \begin{dcases}
    \frac{
      \sin
      \frac{ (2 K - 1) \, l \, \pi }
           {              n        }
    }
    {
      n \, \sin \frac{ l \, \pi } { n }
    }
    & l \ne 0, \\
    \frac{2K-1}{n} & l = 0.
  \end{dcases}
\notag
%\label{eq:mu_sinc_pbc}
\end{equation}
%

For a non-periodic variable,
we have $\lambda_n = (-1)^{n-K}$
from Eqs. \eqref{eq:lambda_bandpass} and \eqref{eq:lambdan},
and $\mu_l = (-1)^{n-K+l}/(2 \, n) + \operatorname{sinr}(K, l, 2 \, n)$
for $l \le b$.
%\begin{equation}
%  \mu_l
%  =
%  (-1)^{n-K+l} / ( 2 \, n )
%    +
%  \operatorname{sinr}(K, l, 2n)
%  .
%  \notag
%  %\label{eq:mu_sinc_refl}
%\end{equation}
%
The updating matrix is given by
$w_{ij} = \operatorname{sinr}(K, i-j, 2 \, n) + \operatorname{sinr}(K, i+j-1, 2 \, n)$.
%
\note{$w_{ij}$
  is free from the oscillatory term, $(-1)^{n-K+1}$,
  that appeared in the window function, since
  its contribution to $\mu_{i-j}$ to the updating matrix
  is canceled by its contribution to either $\mu_{i+j-1}$
  or $\mu_{i+j-2n-1}$,
  whichever exists, of the opposite sign.}
%



\section{\label{sec:hfluc}
Histogram fluctuation}


Here we first relate the error
in a long FDS simulation under the single-bin updating scheme
to a measure of histogram fluctuation,
which can be used in the WL criterion of stage transitions.
%
We then modify the measure
such that it applies to
the polynomial bandpass updating schemes.

%\subsection{Histogram fluctuation and error}

Consider an FDS simulation
under a nearly exact bias potential.
%
By Eq. \eqref{eq:sh_ave},
the fluctuation of the instantaneous histogram
is then dominated by the random part,
%
\begin{equation}
  f_{*i}(t) \approx \xi_{*i}(t)
  ,
  \label{eq:f_xi}
\end{equation}
and
%the same holds for the Fourier transform,
$\tilde f_k(t) \approx \tilde \xi_k(t).$
%
If we define the time average,
%
\begin{align*}
  \tilde F_k(T) \equiv \frac 1 T \sum_{t = 1}^T \tilde f_k(t)
  =\mathcal F\left[ \frac{ H_i(T) } { \rho_i } \right]_k
  ,
\end{align*}
%
for the histogram fluctuation of mode $k$,
then the total fluctuation is
%
\begin{align}
  F^2(T)
  &\equiv
  \sum_{k=1}^{n-1} \tilde F_k^2(T)
  =
  \sum_{i=1}^{n} \rho_i \, F_{*i}^2(T)
  =
  \sum_{i=1}^n
  \frac{ [ H_i(T) -\rho_i]^2 }{\rho_i}
  ,
  \notag
  %\label{eq:hfluc_def}
\end{align}
%
where we have used Eq. \eqref{eq:parseval} in the last step.
%
From Eq. \eqref{eq:f_xi}, we have
%$$\bigl\langle \tilde F_k^2(T) \bigr\rangle
%=
%\sum_{t=-\infty}^\infty
%\frac{
%\bigl\langle
%  \tilde f_k(t) \, \tilde f_k(0)
%\bigr\rangle } { T }
%\approx
%\sum_{t=-\infty}^\infty
%\frac{
%\bigl\langle
%  \tilde \xi_k(t) \, \tilde \xi_k(0)
%\bigr\rangle } { T }
%=
%\frac{ \Gamma_k } { T },$$
%
\begin{align}
  \bigl\langle \tilde F_k^2(T) \bigr\rangle
  &=
  \frac{1}{T}
  \sum_{t=-\infty}^\infty
  \bigl\langle
    \tilde f_k(t) \, \tilde f_k(0)
  \bigr\rangle
  \notag \\
  &\approx
  \frac{1}{T}
  \sum_{t=-\infty}^\infty
  \bigl\langle
    \tilde \xi_k(t) \, \tilde \xi_k(0)
  \bigr\rangle
  =
  \frac{ \Gamma_k } { T }
  ,
  %\notag
  \label{eq:Fk2_Gammak}
\end{align}
%
and the total histogram fluctuation is
\begin{equation}
  \bigl\langle F^2(T) \bigr\rangle
  =
  \sum_{k=1}^{n-1}
  \bigl\langle \tilde F_k^2(T) \bigr\rangle
  =
  \sum_{k=1}^{n-1}
  \frac{ \Gamma_k } { T }
  =
  \frac{ \Gamma } { T }
  ,
  \label{eq:F2sum}
\end{equation}
where we have used Eq. \eqref{eq:Gammak_sum}
in the last step.
%
We may relate the histogram fluctuation
to the minimal error under the single-bin updating scheme,
Eq. \eqref{eq:Emin_sbin}, as
  $\Err(T) \approx \bigl\langle F^2(T) \bigr\rangle$.
%we may rephrase the optimality of the inverse-time schedule
%as the inequality,
%\begin{equation}
%  \Err(T)
%  \approx
%  \bigl\langle F^2(T) \bigr\rangle
%  .
%  \notag
%  %\label{eq:fluc}
%\end{equation}
%and the histogram fluctuation
%serves as a good estimate of the final error.
%

%\subsection{Overall histogram fluctuation}

As an estimate of the error,
the total histogram fluctuation given by Eq. \eqref{eq:F2sum}
can be used in the WL criterion for stage transitions.
%
However, for a bandpass updating scheme,
we should restrict the sum to the first $K$ modes,
where the histogram flattening applies, i.e.
$F^2(T) \equiv \sum_{k = 1}^{K-1} \tilde F_k^2(T)$.
%
%\begin{align}
%  .
%  \label{eq:maxFk}
%\end{align}

In particular,
in sampling of the bivariate distribution,
$\rho(s, z)$, in Sec. \ref{sec:st},
the above definition becomes
\begin{align}
  F^2
  &=
  \sum_{\nu = 0}^{S - 1}
  \sideset{}{'}\sum_{k = 0}^{K - 1}
  \tilde F_{\nu, k}^2
  \notag \\
  &=
  \sum_{s=1}^S
  \left[
  \frac{ (H_s - \rho_s)^2 } { \rho_s }
  +
  \sum_{k = 1}^{K - 1}
  \frac{ H_s^2 } { \rho_s }
  \left|{\tilde F}_k^{(s)} \right|^2
  \right]
  ,
  \label{eq:F2st}
\end{align}
where $\nu$ and $k$ are the indices of
inter- and intra-umbrella fluctuation modes,
and the primed sum excludes
the term of $\nu = k = 0$;
on the next line,
$H_s$ is the normalized inter-umbrella histogram,
and
$\tilde F_k^{(s)}$
is the histogram fluctuation of the $k$th mode in umbrella $s$,
which is the average of $R_k^{(s)}\bigl( z(t) \bigr)$
for the data points in umbrella $s$.
%If the eigenmodes are made of a sequence of orthogonal polynomials
%as in Sec. \ref{sec:bandpass_poly},
%then
%$\tilde f_k(t) = R_k\bigl( z(t) \bigr)$,
%\begin{align*}
%\tilde f_k(t)
%=
%\int \phi_k(z) \, \frac{ \delta\bigl(z(t) - z\bigr) } { \rho(z) } \, dz
%=
%R_k\bigl( z(t) \bigr)
%,
%\end{align*}
%and
%\begin{equation}
%  \tilde F_k(T)
%  =
%  \frac 1 T \sum_{t = 1}^T R_k\bigl( z(t) \bigr)
%  .
%  \label{eq:Fk_def}
%\end{equation}




\note{The table of symbols is listed in Table \ref{tab:symbols}.
  \begin{table*}
  \footnotesize
  \centering
  \rowcolors{1}{white}{LightGray}
  \setlength{\tabcolsep}{4pt} % column separation
  \caption{\label{tab:symbols}
    Table of symbols.}
  \begin{tabular}{l | p{12cm} }
    Symbol          &   Description \\
    \hline
    $\mathbf{A}$    &   Transition matrix. \\
    $\alpha$        &   Updating magnitude. \\
    $b$             &   Cutoff of the window function of homogeneous updating schemes. \\
    $C_M$           &   Integral of the mass function, $M(\bar q)$.  \\
    $\delta$        &   Dirac's or discrete $\delta$-function. \\
    $c$             &   Constant. \\
    $\Err, \Err_R, \Err_A$          &   Error, residual error, asymptotic error. \\
    $\ln f$         &   Updating factor in the WL algorithm.  \\
    $\phi_{ki}$     &   Eigenvectors of the updating matrix. \\
    $g$             &   $1$ for a periodic variable, or for $2$ a non-periodic one. \\
    $G_i$           &   Integral of the autocorrelation function of histogram fluctuation at bin $i$. \\
    $\Gamma_k$      &   Integral of the autocorrelation function of mode $k$. \\
    $h_i(t)$        &   Instantaneous histogram.  \\
    $H_i$           &   Average normalized histogram.  \\
    $i, j$          &   Bin indices. \\
    $\lambda_k$     &   The $k$th eigenvalue of the updating matrix. \\
    $k, l$          &   Mode indices. \\
    $K$             &   Cutoff wave number of bandpass updating schemes.  \\
    $M(q)$          &   Mass function.   \\
    $m(q)$          &   Mass distribution,
                        $m(q) \equiv M(q)/\int_0^{ q(T) } M(q') \, dq'$.  \\
    $\mu_i$         &   Unadjusted window function of a homogeneous updating scheme. \\
    $n$             &   Number of bins. \\
    $\nu_k$         &   $\nu_k \equiv \lambda_k / \lambda$. \\
    $\rho_i$        &   Flat sampling distribution. \\
    $\pi_i$         &   (Instantaneous) distribution. \\
    $q(t)$          &   $\int_0^t \alpha(t') \, dt'$.  \\
    $\bar q(t)$     &   $q(T) - q(t)$.  \\
    $t, \tau$       &   Time. \\
    $T$             &   Simulation length. \\
    $\theta_k(q')$       &   $\theta_k(q') \equiv e^{\lambda_k \, [q - q(T)]}$. \\
    $u_i(t)$        &   Original bias potential. \\
    $v_i(t)$        &   Shifted bias potential. \\
    $\mathbf w$     &   Updating matrix. \\
    $v_{*i}(t)$     &   Bin-average-deducted bias potential, Eq. \eqref{eq:x_def}. \\
    $\xi_{*i}(t)$   &   Bin-average-deducted The noise of histogram. \\
    ${\tilde v}_k$  &   Mode of the shifted bias potential. \\
    ${\tilde u}_k$  &   Mode of the original bias potential. \\
    $z$             &   Reaction coordinate. \\
    $\xi_i(t)$    &   The noise part of the instantaneous histogram, $h_i(t)$.
  \end{tabular}
  \end{table*}
}

\bibliography{simul}
\end{document}
