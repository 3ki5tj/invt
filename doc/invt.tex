\documentclass[reprint]{revtex4-1}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}



\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{64,0,42}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}
\newcommand{\answer}[1]{{\color{DarkBlue}\footnotesize \textsc{Answer.} #1}}
\newcommand{\summary}[1]{{\color{DarkPurple}\footnotesize \textsc{Summary.} #1}}


\begin{document}



\title{Optimal updating factor in Wang-Landau and metadynamics simulations}



\begin{abstract}
  The Wang-Landau (WL) algorithm and metadynamics
  are two closely-related techniques in free energy simulations.
  %
  They allow one to sample a flat distribution
  along a quantity of interest, $z$,
  by adaptively constructing a bias potential,
  which is the negative potential of mean force (PMF).
  %
  The two algorithms differ by
  the manner of updating the bias potential:
  %
  in the WL case, each simulation step triggers
  an update of the bias potential to only
  the bin containing the current $z$,
  while in the metadynamics case,
  the bias potential of several neighboring bins
  are affected as well.
  %
  The asymptotic error of the resulting PMF
  is determined by the updating magnitude,
  and the rate of the reduction of the magnitude
  over the simulation time.
  %
  In the WL case,
  a simple and effective formula for the magnitude
  is given by the inverse of the number of simulation steps.
  %
  In this study, we shall reexamine the optimality of this prescription,
  and attempt to extend it to the metadynamics case.
  %
  Further, we compared the two algorithms, and found that
  although metadynamics produces a smoother PMF profile,
  it is less effective in reducing errors with the shorter wavelengths.
  %
  Thus, it relies more heavily on the assumption of
  the smoothness of the PMF profile,
  especially with a wide updating window.
\end{abstract}

\maketitle



\section{Introduction}



Free energy calculation\cite{frenkel} is a central theme
in computational physics and chemistry.
%
Given a system,
the problem is to compute a distribution, $p^*(z)$,
along a quantity of interest $z$, and to see how
the distribution changes with the external conditions,
such as temperatures and pressures.
%
The negative logarithm $-\log p^*(z)$,
or the potential of mean force (PMF),
defines a free energy.
%
A straightforward approach of computing the PMF
is to run multiple independent simulations
under different conditions,
and then piece together the information
gathered there.
%
This solution is often unsatisfactory
for a complex and glassy system,
because the system can be trapped
in a local free energy minimum
for a long time.



A better solution is to artificially alter
the target distribution,
such that we can, in a single simulation, sample
a flat distribution\cite{mezei1987, berg1992, lee1993,
wang2001, wang2001pre, laio2002}
along $z$ over a wide range.
%
This alleviates the above problem of local trapping
as the system is now forced to travel along the $z$ direction,
which often represents a slow reaction coordinate.
%
It, however, requires one to efficiently construct a
bias potential for an often unknown system.
%
Note that the bias potential is directly related to the PMF
as the former must precisely offset the latter
to achieve a flat distribution.



The Wang-Landau (WL) algorithm\cite{wang2001, wang2001pre}
and metadynamics\cite{laio2002}
are two widely-used techniques for this purpose,
in which a bias potential is actively build-up
based on the simulation history.
%
In essence, these techniques
encourage the system to leave a region that has been visited
by elevating the bias potential there.
%
To be effective, the updates to bias potential
are made at every few steps,
while a side effect is that the underlying dynamics
is no longer a equilibrium sampling
required by statistical mechanics.
%
Thus, one has to reduce the magnitude of updating
over the course of simulation.



Naturally, the manner of reducing
the updating magnitude\cite{
belardinelli2007, belardinelli2007jcp, belardinelli2008,
morozov2007, zhou2008, morozov2009,
komura2012, caparica2012, caparica2014}
determines the precision of the final bias potential,
hence that of PMF.
%
For the WL algorithm, it is found
that asymptotically the optimal magnitude of updating $\ln f$
should be given by the
inverse time\cite{
belardinelli2007, belardinelli2007jcp, belardinelli2008,
morozov2007, zhou2008},
where ``time'' is defined as
the number of simulation steps
divided by the number of bins.


In this study,
we shall give a proof of the inverse-time formula,
and try to generalize it to
updating schemes involving neighboring bins,
such as those used in metadynamics.
%
As a byproduct, we shall show that
the single-bin scheme used by the WL algorithm
is generally one of the best
updating schemes in terms of convergence.
%
The article is organized as follows.
%
We present the analytical results in Sec. \ref{sec:theory},
numerically verify some key aspects
in Sec. \ref{sec:results},
and conclude the article
in Sec. \ref{sec:conclusion}.




\section{\label{sec:theory}
Theory}


In this section,
we first review the basics and fix notations
in Sec. \ref{sec:background}.
%
We then prove the optimality
of the inverse-time formula
for the single-bin WL algorithm
in Sec. \ref{sec:single-bin}.
%
Next, we discuss the generalized
multiple-bin schemes
in Secs. \ref{sec:multiple-bin}
and \ref{sec:band-matrix}.
%



\subsection{\label{sec:background}
Background}



\subsubsection{Flat-distribution sampling}



Given a system,
consider the problem of computing
the distribution, $p^*_i$
along a discrete quantity $i$.
%
%The discreteness of $i$ is natural
%in discrete models.
%
For example, $i$ can be the energy $E$
in lattice spin models; or the temperature index
in a simulated tempering simulation.
%
For the quantity of interest, $z$, is continuous,
which is typical in molecular systems,
we can discretize $z$
such that the integer $i$ represents
the index of a small interval, or a bin, $(z, z + dz)$.
%
In both cases,
the distribution is normalized as
$\sum_{i = 1}^n p^*_i = 1$.



For a large system,
the distribution $p^*_i$ tends to
be localized around some $i_0$,
%
and to find out the global property,
it is often desirable to carry out
a biased sampling that targets
a wider distribution $p_i$.
%
%Here, we refer to simulations that target
%a flat or nearly flat distribution
%as entropic or multicanonical sampling.



To do so, we need to introduce a bias potential $V_i$
to modify the target distribution to
%
\begin{equation}
  \pi_i \propto p^*_i \, e^{-V_i}.
  \label{eq:pi_p_phi1}
\end{equation}
%
Upon normalization, $\sum_{i = 1}^n \pi_i = 1$,
we get
%
\begin{equation}
  \pi_i =
  \frac{ p^*_i \, e^{-V_i} }
  { \sum_{j = 1}^n p^*_j \, e^{-V_j} }.
  \label{eq:pi_p_phi}
\end{equation}
%
Particularly,
to achieve a flat distribution $\pi_i$,
the bias potential $V_i$
must coincide with $\log p^*_i$,
up to an additive constant.



Generally, we may wish to sample
a nearly flat distribution\cite{dayal2004, trebst2004}, $p_i$,
by adjusting the bias potential $V_i$
on the fly.
%
For example,
we can monitor the histogram accumulated over
a period of time and use it for $\pi_i$ in
Eq. \eqref{eq:pi_p_phi1},
and then update
$V_i$ according to
$$
V_i^{\mathrm{new}}
=
V_i^{\mathrm{old}}
-
\log \frac{ p_i } { \pi_i }.
$$
%
%From Eq. \eqref{eq:pi_p_phi1},
%it is clear that to achieve the target distribution
%$\pi_i = p_i$ requires
%%
%\begin{equation}
%\log p^*_i
%=V_i + \ln p_i + c
%\label{eq:bias_pmf}
%\end{equation}
%%
%%
%In the end of the simulation,
%the PMF, $-\log p^*_i$,
%can be deduced from Eq. \eqref{eq:bias_pmf}.
%
The above inversion process,
adopted by early multicanonical or
entropic sampling\cite{berg1992, lee1993},
can be inconvenient in practice,
for we need to make the period
as long as a multiple of
the autocorrelation time,
which is difficult to estimate for an unknown system.


For later convenience, we introduce
%
\begin{equation}
  v_i \equiv V_i - \ln p^*_i + \ln p_i.
  \label{eq:v_def}
\end{equation}
%
Since the last two terms of Eq. \eqref{eq:v_def}
are constant during the course of simulation,
updates to $V_i$ are equivalent to those to $v_i$.
%
In terms of $v_i$, Eq. \eqref{eq:pi_p_phi}
becomes
%
\begin{equation}
  \pi_i
  =
  \frac{                p_i \, e^{-v_i} }
       { \sum_{j = 1}^n p_j \, e^{-v_j} }
  \propto
  p_i \, e^{-v_i}.
  \label{eq:pi_p_v}
\end{equation}
%
According to Eq. \eqref{eq:pi_p_phi1},
$v_i$ should approach a constant of $i$
upon convergence (i.e., when $\pi_i \approx p_i$).




\subsubsection{Updating schemes}



In the WL algorithm\cite{wang2001, wang2001pre},
$v_i$ are updated
in each MC step $t$,
and $v_i(t)$ as a function of $t$
is updated as
%
\begin{equation}
  v_i(t+1)
  =
  v_i(t)
  +
  \delta_{i, \, i(t)}
  \frac{ \alpha(t) } { p_i }.
  \label{eq:wl_update}
\end{equation}
%
where $i(t)$ is the bin at step $t$,
and $\alpha(t)$ is the updating magnitude.
%
We refer to the above scheme as the single-bin scheme,
for it applies only to the bias potential
at the current bin, $i(t)$,
%
By contrast, a multiple-bin scheme
means the general case in which
several neighboring bins are updated.

%The values of $v_j(t)$ at $j \ne i(t)$
%are kept unchanged.



One can show that with a constant $\alpha(t) = \alpha > 0$,
the distribution collected from
the trajectory is identical to $p_i$.
%
However, the continuous update
makes $v_i(t)$ a fluctuating quantity,
and Eq. \eqref{eq:pi_p_v} no longer holds
at all times.
%%
%The source of the deviation is two-fold.
%%
%First, since $v_i(t)$ is updated continuously,
%there is a random noise that is proportional
%to $\sqrt \alpha$.
%%
%Second, there is a systematic error
%that comes from the updating dynamics itself,
%as it breaks the Markovian nature
%that underlies Eq. \eqref{eq:pi_p_v}.
%%
%
Thus, one has to decrease $\alpha$ over time
to reduce the error of $v_i$,
in order to get the unbiased distribution
$\ln p_i^*$ via Eq. \eqref{eq:v_def}.





In the original WL scheme,
the updating magnitude $\alpha$ (or $\ln f$
in the original paper) is kept as a constant
for a period of time,
which is referred to as a stage below.
%
The histogram collected in the stage is monitored.
%
Once the histogram is sufficiently flat,
we are allowed to enter a new stage
with a reduced $\alpha$
(usually half as large as
that in the previous stage).
%
This scheme works well for early stages.
%
However, in later stages, it tends to reduce $\alpha$
too quickly, making the asymptotic error
saturate.



\subsubsection{$1/t$ formula}



A more effective way
of updating the $\alpha(t)$
is to follow the formula
%
\begin{equation}
  \alpha(t) = \frac{1}{t},
  \label{eq:alpha_invt}
\end{equation}
%
where $t$ is the number of steps
from the beginning of the MC simulation,
which shall be referred to as the ``time'' below.
%
This surprisingly simple formula has attracted
several studies, notably the one by Zhou.
%
There are, however, some reservations about
the constant of proportionality:
the optimal formula for $\alpha(t)$
might be $C/t$ with a constant $C$
different from $1.0$.



%Here we shall derive the optimal $\alpha(t)$
%For the WL algorithm, Eq. \eqref{eq:wl_update}.
%%
%Further, we shall generalize the result
%To a class of multiple-bin schemes,
%In which an update affects
%Not only the current bin $i(t)$,
%But also a few neighboring ones.
%%(such schemes are designed
%%to maintain a smooth bias potential).
%%
%We shall show that
%The constant of proportionality, $1$,
%In Eq. \eqref{eq:alpha_invt}
%Is optimal for the single-bin scheme,
%Eq. \eqref{eq:wl_update}
%(as in the WL algorithm),
%But not necessarily so
%For a multiple-bin scheme.
%%
%Below, we discuss the single-bin scheme
%In Section \ref{sec:single-bin},
%And the multiple-bin case
%In Section \ref{sec:multiple-bin}.



\subsection{\label{sec:single-bin}
Single-bin scheme}



In this section,
we shall derive the optimal $\alpha(t)$
for the single-bin scheme,
Eq. \eqref{eq:wl_update}.
%
To do so,
we shall first express the error of $v(t)$
as a functional of $\alpha(t)$,
and then find the optimal value
by functional variation.



\subsubsection{Differential equation}



We first approximate Eq. \eqref{eq:wl_update}
by a differential equation
%
\begin{equation}
  \dot v_i(t)
  =
  h_i(t) \frac{ \alpha(t) } { p_i },
  \label{eq:vt_diffeq}
\end{equation}
%
where
$\dot v_i(t) \equiv dv_i(t)/dt$,
%
and $h_i(t) = \delta_{i, i(t)}$
is the instantaneous histogram,
which is equal to $1.0$
for the current bin $i(t)$
or zero otherwise.



Next, we split $h_i(t)$ into a deterministic part
and a noise part,
%
\begin{equation}
  h_i(t) = \langle h_i(t) \rangle + \zeta_i(t).
  \label{eq:h_split}
\end{equation}
%
Here, the deterministic part can be related
to the ``ensemble average'' of $h_i$.
%
The ensemble consists of many similar simulation copies
that have experienced the same schedule $\alpha(t)$
and have reached the same $v_i(t)$
at time $t$.
%
The initial states and the stochastic forces
during the process may, however, be different.



For sufficiently small $\alpha(t)$,
the $v_i$'s remain roughly the same for a short period.
%
Then,
the sampling process is approximately Markovian, % of a finite order,
and we may assume Eq. \eqref{eq:pi_p_v}
for the deterministic part
%
\begin{equation}
  \langle h_i(t) \rangle
  \approx
  \pi_i
  =
  \frac{ p_i \, e^{-v_i} }
  { \sum_{j = 1}^n p_j \, e^{-v_j} }.
  \label{eq:h_ave}
\end{equation}
%
%
%
For the noise part, we have
$\langle \zeta_i(t) \rangle = 0$,
%
%The noise is not necessarily white,
and the correlation functions
depend only on the time difference
%
\begin{equation}
  \langle \zeta_i(t) \, \zeta_j(t') \rangle
  =
  \sigma_{ij}(t - t'),
  \label{eq:zeta_zeta_correlation}
\end{equation}
%
where $\sigma_{ij}(t)$ is an even function of $t$
that vanishes at large $t$.
%
More explicitly,
if the transition matrix of a period $\tau$
is $T^\tau_{ij}$,
then
$
  \langle \zeta_i(t) \, \zeta_j(t') \rangle
  =
  \langle h_i(t) \, h_j(t') \rangle
  -
  \langle h_i(t) \rangle \, \langle h_j(t') \rangle
  =
  T^{t - t'}_{ij} \pi_j - \pi_i \, \pi_j.
$




\subsubsection{Linear approximation}



To proceed, we first observe that
the average $\bar v = \sum_{i = 1}^n p_i \, v_i$
increases steadily over time:
%
\begin{equation}
\frac{ d \bar v } { d t }
=
\sum_{i = 1}^n p_i \dot v_i
=
\alpha(t) \sum_{i = 1}^n h_i(t) = \alpha(t).
\label{eq:dvbardt}
\end{equation}
%
However, the difference between $v_i$ and $\bar v$,
%
\begin{equation}
  x_i \equiv v_i - \bar v = v_i - \sum_{j = 1}^n p_j \, v_j,
  \label{eq:x_def}
\end{equation}
%
is expected to be small in the asymptotic regime,
and we can expand Eq. \eqref{eq:h_ave} as
\begin{align}
\langle h_i(t) \rangle
&\approx
\frac{ p_i \, e^{- x_i} }
{ \sum_{ j = 1}^n p_j \, e^{- x_j} }
\approx
\frac{ p_i ( 1 - x_i ) }
{ \sum_{ j = 1}^n p_j (1 - x_j) }
\notag \\
&\approx
p_i \, \left(
  1 - x_i + \sum_{j=1}^n p_j \, x_j
\right)
=
p_i \, (1 - x_i),
\label{eq:hdet_linearize}
\end{align}
where we have used $\sum_{j=1}^n p_j = 1$,
and
%
\begin{equation}
  \sum_{i = 1}^n p_i \, x_i = 0,
  \label{eq:px_sum}
\end{equation}
which follows directly from Eq. \eqref{eq:x_def}.
%
With Eqs.
\eqref{eq:vt_diffeq},
\eqref{eq:h_split},
\eqref{eq:dvbardt},
and
\eqref{eq:hdet_linearize},
we get a set of decoupled equations
%
\begin{equation}
  \dot x_i(t)
  =
  -\alpha(t) \, \left[ x_i(t) - \frac{ \zeta_i(t) } { p_i } \right].
  \label{eq:dxdt_WL}
\end{equation}



The total error of the bias potential can be written in
terms of $x_i$,
\begin{equation}
\mathcal E
=
\sum_{i = 1}^n p_i \, \langle x_i^2 \rangle
=
\sum_{i = 1}^n p_i \, \left\langle (v_i - \bar v)^2 \right\rangle,
\label{eq:error_sum}
\end{equation}
where
$\bar v = \sum_{i = 1}^n p_i v_i$.
%
Our aim is to find the $\alpha(t)$
that minimizes Eq. \eqref{eq:error_sum}.
%
To do so, we shall first consider
a simpler one-variable problem.



\subsubsection{One-variable problem}



Consider the following equation
of a single variable:
%
\begin{equation}
\dot x(t) = -\alpha(t) \, \lambda \left[ x(t) - \xi(t) \right],
\label{eq:dxdt_alpha}
\end{equation}
%
where $\xi(t)$ is a generalized noise
that is invariant under time translation:
%
\begin{equation}
\left\langle \xi(t) \, \xi(t') \right\rangle
=
\kappa(t - t').
\label{eq:noise_correlation}
\end{equation}
%
Additionally, we require
\begin{equation}
  \lim_{t \rightarrow \pm\infty} \kappa(t) = 0.
  \label{eq:kappat_limit}
\end{equation}
%
For example, for a white noise,
$\kappa(t)$ is proportional to
Dirac's $\delta$-function, $\delta(t)$.
%
We wish to show that the optimal $\alpha(t)$
of minimizing $\langle x^2(t) \rangle$ at long times
is given by
%
\begin{equation}
  \alpha(t) = \frac{1}{\lambda \, (t + t_0)}.
\label{eq:alpha_opt}
\end{equation}



To do so, we first recall
the formal solution of Eq. \eqref{eq:dxdt_alpha}:
%
\begin{equation}
x(t) = x(0) \, e^{-\lambda \, q(t)}
+ \int_0^t \dot u\bigl( q(t') \bigr) \, \xi(t') \, dt',
\label{eq:xt_solution}
\end{equation}
%
where,
%
\begin{equation}
q(t) \equiv \int_0^t \alpha(t') \, dt',
\label{eq:qt_definition}
\end{equation}
%
and
%
\begin{align}
u(z)
&\equiv
e^{-\lambda \, q(t) + \lambda \, z}.
\label{eq:ut_definition}
\end{align}


We shall further demand that
%
\begin{equation}
  \lim_{t \to \infty} q(t) \to \infty.
  \label{eq:qt_limit}
\end{equation}
%
Then, for a large time $t$,
the first term on the right-hand side
of Eq. \eqref{eq:xt_solution} can be neglected, and
%
\begin{align}
\left\langle x^2(t) \right\rangle
%&=
%\int_0^t \int_0^t \dot u(t') \, \dot u(t'')
%    \left\langle \xi(t') \xi(t'') \right\rangle dt'' \, dt'
%\notag
%\\
&=
\int_0^t \int_0^t
  \dot u\bigl( q(t') \bigr) \,
  \dot u\bigl( q(t'') \bigr) \,
  \kappa(t' - t'') \, dt'' \, dt'.
\label{eq:x2t_average}
\end{align}



Variating this expression and
using the Euler-Lagrange equation yields
$$
\begin{aligned}
0
&=
\frac{d}{d\tau} \int_0^t
  \dot u\bigl( q(t') \bigr) \, \kappa(t' - \tau) \, dt'
\\
&= \int_0^t
  \ddot u\bigl( q(t') \bigr) \, \kappa(t' - \tau) \, dt',
\end{aligned}
$$
where we have dropped the boundary terms
by using Eq. \eqref{eq:kappat_limit}.
%
%In fact, we can repeat the process $n$ times, and
%$$
%\int_0^t u^{(n)}(t') \, \kappa(t' - \tau) \, dt' = 0,
%\qquad (n \ge 1)
%$$
%
In order for this to hold for any $\tau$,
we must have
%
\begin{equation}
\ddot u\bigl( q(t') \bigr) = 0,
\qquad
\mathrm{or}
\;\;
\dot u\bigl( q(t') \bigr) = c,
\label{eq:ddu_eq_0}
\end{equation}
%
where $c$ is a constant of $t'$.
%
Using Eq. \eqref{eq:ut_definition}
we get
$$
e^{-\lambda \, q(t) + \lambda \, q(t')}
=
c \, (t' + t_0),
$$
where $t_0$ is a constant.
%
Taking the logarithm, and differentiating this with respect to $t'$
yields Eq. \eqref{eq:alpha_opt}.



\subsubsection{Optimal $\alpha(t)$ for the single-bin scheme}



Going back to the problem of
minimizing Eq. \eqref{eq:error_sum}
for the single-bin scheme,
we find from Eq. \eqref{eq:dxdt_WL}
that with $\lambda = 1$,
Eq. \eqref{eq:alpha_opt}
will optimize each term $\langle x_i^2 \rangle$,
and hence the sum.
%
Thus, Eq. \eqref{eq:alpha_invt} is optimal
up to a constant $t_0$. %defining the origin of $t$.



\subsection{\label{sec:multiple-bin}
Multiple-bin scheme}



We now consider the multiple-bin scheme.
%
In this case,
upon a visit to bin $j$,
we update not only the bias potential there,
but also those for a few neighboring bins $i$.
%
The update magnitudes are given by a matrix $\mathbf w$
with elements $w_{ij}$.
%
Then, Eq. \eqref{eq:vt_diffeq} is generalized to
\begin{equation}
  \dot v_i(t) =
  \sum_{j=1}^n \alpha(t) \, h_j(t) \frac{ w_{ij} } { p_j }.
  \label{eq:vt_diffeq_mbin}
\end{equation}


The procedure of optimization is largely
the same as the single-bin case,
whereas the matrix nature brings about
some technical difficulties that require
certain restrictions and approximations.



\subsubsection{Updating matrix}



The matrix $\mathbf w$ is not arbitrary.
%
A necessary condition to sample desired distribution
$\mathbf p = (p_1, \dots, p_n)$
is that in Eq. \eqref{eq:vt_diffeq_mbin}
when $h_j(t)$ coincides with $p_j$,
the rate of change $\dot v_i(t)$
should be independent of $i$.
%
This allows $v_i(t)$ to approach a constant
asymptotically.
%
Thus, by a proper scaling of $\alpha(t)$,
we may write this condition as
%
\begin{equation}
  \sum_{j = 1}^n w_{ij} = 1.
  \label{eq:w_sumj}
\end{equation}
%
This means $(1, \dots, 1)^T$
is a right eigenvector of $\mathbf w$
with eigenvalue $1$.
%
Thus, the transpose $\mathbf w^T$
resembles a transition matrix,
although the matrix elements can be negative.



To simplify the following discussion,
we shall further impose the
detailed balance condition:
%
\begin{equation}
  p_i \, w_{ij} = p_j \, w_{ji}.
  \label{eq:w_detailedbalance}
\end{equation}
%
It follows that
\begin{equation}
  \sum_{i = 1}^n p_i \, w_{ij}
  =
  \sum_{i = 1}^n p_j \, w_{ji}
  = p_j,
  \label{eq:w_balance}
\end{equation}
%
i.e., $\mathbf p$ is a left eigenvector of
$\mathbf w$ with eigenvalue $1$.



Further, by Eq. \eqref{eq:w_detailedbalance},
we can define a symmetric matrix $\hat{\mathbf w}$
as $\hat w_{ij} = \sqrt{p_i/p_j} \, w_{ij}$
that can be diagonalized
with a set of orthonormal eigenvectors:
%
$\sum_{i = 1}^n \varphi_{ki} \, \hat w_{ij} = \lambda_k \, \varphi_{kj}$.
%
Thus,
in terms of diagonalizing $\mathbf w$, we have
\begin{equation}
  \sum_{i = 1}^n \phi_{ki} \, w_{ij} = \lambda_k \, \phi_{kj},
  \label{eq:eig_w}
\end{equation}
where
$\phi_{ij} = \sqrt{p_j} \, \varphi_{ij}$ satisfies
the orthonormal conditions:
%
\begin{align}
\sum_{k = 1}^n \phi_{ki} \, \phi_{kj}
&= \delta_{ij} \, p_i,
\label{eq:eig_orthonormal_cols}
\\
\sum_{k = 1}^n \frac{ \phi_{ik} \, \phi_{jk} }{ p_k }
&= \delta_{ij}.
\label{eq:eig_orthonormal_rows}
\end{align}



%It is worth pointing out
Note that, in a stable updating scheme,
all eigenvalues $\lambda_k$ must be nonnegative,
for otherwise there is a fluctuating mode
that can increase indefinitely.



\subsubsection{Linearization and decoupling}



We can now simplify Eq. \eqref{eq:vt_diffeq_mbin}.
%
By using Eq. \eqref{eq:w_balance},
we can show that
Eqs. \eqref{eq:dvbardt} and \eqref{eq:px_sum}
remain true.
%
%Thus, for Eq. \eqref{eq:x_def}, we have
%
$$
\begin{aligned}
\dot x_i
&= \alpha(t) \sum_{j=1}^n w_{ij}
\left( \frac{ h_j } { p_j }  - 1 \right)
\\
&=
-\alpha(t) \sum_{j = 1}^n
w_{ij} \left[ x_j(t) - \frac{\zeta_j (t)}{p_j} \right],
\end{aligned}
$$
where
we have expanded the right-hand side
in linear terms of $x_j$.


Now by diagonalizing the matrix $\mathbf w$
using matrix defined in Eqs. \eqref{eq:eig_w},
\eqref{eq:eig_orthonormal_cols},
and
\eqref{eq:eig_orthonormal_rows},
we get a set of decoupled equations
%
\begin{equation}
\dot y_i(t)
=
-\alpha(t) \, \lambda_i
[y_i(t) - \eta_i(t)],
\label{eq:yt_diffeq}
\end{equation}
%
where
\begin{align}
  y_i &= \sum_{j=1}^n \phi_{ij} \, x_j,
  \label{eq:y_def}
  \\
  \eta_i &= \sum_{j=1}^n \phi_{ij} \frac{ \zeta_j}{ p_j}.
  \label{eq:eta_def}
\end{align}



\subsubsection{Minimization of the error function}



The error function can be obtained
from Eq. \eqref{eq:eig_orthonormal_cols}
as
\begin{align}
  \mathcal E
  &\equiv
  \sum_{i = 1}^n p_i \, x_i^2
  =
  \sum_{i, j, k=1}^n \phi_{ki} \, \phi_{kj} \, x_i \, x_j
  =
  \sum_{k = 1}^n y_k^2.
  \label{eq:y2_sum}
\end{align}
%
We note that there is one mode, say
$y_1 = \sum_{j=1}^n p_j \, x_j$
(corresponding to $\lambda_1 = 1$),
vanishes identically by Eq. \eqref{eq:px_sum}.
%
Thus, we can start the sum in Eq. \eqref{eq:y2_sum}
from $k = 2$.



In analogous to Eq. \eqref{eq:x2t_average},
we can rewrite Eq. \eqref{eq:y2_sum} as
%
\begin{align}
  \mathcal E
  %&= \sum_{i = 2}^n \langle y_i^2 \rangle
  %\notag
  %\\
  &=
  \int_0^t \int_0^t
  \sum_{i = 2}^n
  \dot u_i\bigl( q(t') \bigr) \,
  \dot u_i\bigl( q(t'') \bigr) \,
  \kappa_i(t' - t'') \, dt' \, dt'',
  \label{eq:error_mbin}
\end{align}
%
where
\begin{align*}
  u_i(z)
  &\equiv
  e^{\lambda_i \, [q(t') - z]},
  \\
  \kappa_i(t - t')
  &\equiv
  \left\langle
    \eta_i(t) \, \eta_i(t')
  \right\rangle.
\end{align*}
%
\note{The extremal condition of $\mathcal E$ is given by
%
\begin{align}
\sum_{i=2}^n
\lambda_i \, u_i\bigl( q(\tau) \bigr)
\int_0^t
\ddot u_i\bigl( q(t') \bigr) \, \kappa_i(t' - \tau) \, dt' = 0.
\label{eq:optimal_mbin}
\end{align}
%
If all $\lambda_i$, hence all $u_i$, are the same,
the above condition can be satisfied
by Eq. \eqref{eq:ddu_eq_0},
which is the single-bin case.
%
Unfortunately, the above expression
appears to be useless in a general setting.
}



\subsubsection{White-noise approximation}



In the asymptotic regime,
we can approximate $\kappa_i(t)$ as
%
\begin{equation}
  \kappa_i(t) \approx \Gamma_i \, \delta(t)
  \label{eq:kappa_delta}
\end{equation}
%
where, $\Gamma_i = \int_{-\infty}^\infty \kappa_i(t) \, dt$.
%
Then, Eq. \eqref{eq:error_mbin} is simplified as
%
\begin{align}
  \mathcal E
  &=
  \int_0^t
  \sum_{i = 2}^n
  \Gamma_i \, \dot u_i^2\bigl( q(t') \bigr) \, dt'.
  \label{eq:error_mbin1}
\end{align}
%
The next order correction to Eq. \eqref{eq:kappa_delta}
would lead to a correction to $\mathcal E$
that is about $\alpha_{\max}^2$ as large.
%
\note{This is equivalent to saying that the Fourier transform
  $$
  \tilde \kappa_i(\omega) = \Gamma_i
  $$
  is a roughly constant.
  %
  For the next order correction we would have
  $$
  \tilde \kappa_i(\omega) = \Gamma_i + \Gamma^{(2)}_i \omega^2 + \dots,
  $$
  which leads to a correction to $\mathcal E$
  $$
  \Gamma^{(2)}_i
  \int_0^t \ddot u_i^2\bigl( q(t') \bigr) \, dt'
  \propto
  \alpha^2 \, \mathcal E.
  $$
  This correction is of order $\alpha^2$
  compared to the main contribution in Eq. \eqref{eq:error_mbin1}.
  %
  Thus, we can assume Eq. \eqref{eq:kappa_delta},
  once $\alpha(t)$ has dropped below a certain level.
  \\
}
\note{
  The extremal property, Eq. \eqref{eq:optimal_mbin},
  now becomes
  %
  \begin{align}
    \sum_{i=1}^n
      \Gamma_i \, \lambda_i \,
      u_i\bigl( q(\tau) \bigr) \,
      \ddot u_i\bigl( q(\tau) \bigr) = 0.
    \label{eq:optimal_mbin1}
  \end{align}
  %
  If we interpret $u_i$ as the position,
  and $\Gamma_i \, \lambda_i$ as the mass,
  then the right-hand side give the virial.
  %
  Thus, the extremal condition demands
  the total virial to be zero.
  $$\,$$
}
%
For example,
if we assume perfect sampling,
%
\begin{equation}
  \langle \zeta_i(t) \, \zeta_j(t') \rangle
  =
  p_i \, (1 - p_i) \, \delta_{ij} \, \delta(t - t').
  \label{eq:zz_perfect}
\end{equation}
%
Thus, from Eqs. \eqref{eq:eig_orthonormal_rows} and \eqref{eq:eta_def},
we get
%
\begin{equation}
  \kappa_i(t - t')
  =
  \sum_{j,k = 1}^n
  \frac{ \phi_{ij} } { p_j }
  \frac{ \phi_{ik} } { p_k }
  \langle \zeta_j(t) \, \zeta_k(t') \rangle
  %
  \approx \delta(t - t').
  \label{eq:kappa_perfect}
\end{equation}
%
where we have assumed the large $n$ limit
such that $p_i \ll 1$
in the last expression.



%
%We can study two limiting cases.
%
For $\tau \ll t$,
the dominant contribution comes from
the term(s) with the smallest eigenvalue $\lambda_i = \lambda_n$,
and
$$
\ddot u_n(\tau) = 0,
\quad
\dot u_n(\tau) = c,
\quad
\mathrm{and}\;
u_n(\tau) = c \, (\tau + t_0),
$$
which means
$$
\alpha(\tau) \approx \frac{1}{\lambda_n \, (\tau + t_0)}.
$$



\subsubsection{\label{sec:optWL}
Optimality of the single-bin scheme}



We now show
that the single-bin scheme
is an optimal scheme.
%
Consider a class of $\mathbf w$ matrices
with the same set of eigenvectors,
hence same $\Gamma_i$'s,
but with different eigenvalues, $\lambda_i$'s.
%
Using the Cauchy-Schwarz inequality, we have,
for any set of $\beta_i \ge 0$
%
\begin{align}
&
\left( \int_0^t
  dt' \sum_{i = 2}^n \Gamma_i \,
                     \dot u_i^2\bigl( q(t') \bigr)
\right)
\left(
\int_0^t
  dt' \sum_{i = 2}^n \Gamma_i \, c_i^2
\right)
\notag
\\
&
\ge
\left(
\int_0^t
  dt' \sum_{i = 2}^n \Gamma_i \, c_i \,
                     \dot u_i\bigl( q(t') \bigr)
\right)^2
\notag
\\
&
=
\left(
  \sum_{i = 2}^n \Gamma_k \, c_i
    \left[
      1 - e^{ -\lambda_i q(t) }
    \right]
\right)^2.
\notag
\end{align}
%
\note{A short proof of the above version
  of the Cauchy-Schwarz inequality
  is the following.
  %
  First, we have
$$
\int_0^t
  dt' \sum_{i = 2}^n \Gamma_i \,
    \left( \dot u_i \, X - c_i \right)^2
\ge 0,
$$
  for any $X$.
  %
  Thus, the right-hand side,
  as a quadratic polynomial of $X$,
  must have a non-positive discriminant.
  %
  This condition gives the desired inequality.
  \\%
}
%
Note that the last expression depends on
$q(t')$ only through the end point value,
which is fixed in the variation process.
%
Thus, the inequality sets a lower bound
of the error $\mathcal E$
expressed in Eq. \eqref{eq:error_mbin1},
%
The equality is achieved
if $\dot u_i\left( q(t') \right) = c_i$
for all $i = 2, \dots, n$ at any $t'$,
up to a multiplicative factor.
%
Solving this set of equations yields
$$
\alpha(t') = \frac{              1              }
                  { \lambda_i \, (t' + t_{i0} ) },
\quad
\mathrm{with\;\;}
t_{i0} = \frac{             t            }
              { e^{ \lambda_i q(t) } - 1 }.
$$
Such a solution is possible only if
\begin{equation}
\lambda_2 = \dots = \lambda_n.
\end{equation}
%
\note{
  We add some steps for solution.
  Integrating $\dot u_i = c_i$ yields
  \begin{equation}
  u_i(t') = c_i \left(t' + t_{i0} \right).
  \label{eq:ui_solution}
  \end{equation}
  Given that $u_i(t) = 1$ and $u_i(0) = e^{-\lambda_i \, q(t)}$,
  we have
  $$
  c_i = \frac{ 1 }{ t + t_{i0} },
  \quad
  \mathrm{and\;}
  \frac{ t_{i0} } { t + t_{i0} }
  =
  e^{ -\lambda_k \, q(t) }.
  $$
  Taking the logarithm of Eq. \eqref{eq:ui_solution} yields
  $$
  -\lambda_i \, q(t) + \lambda_i \, q(t')
  = \log c_i + \log\left( t' + t_{i0} \right).
  $$
  Differentiating this with respect to $t'$ yields
  $$
  \lambda_i \, \alpha(t') = \frac{ 1 } { t' + t_{i0} }.
  $$
}

Now an updating matrix $\mathbf w$ satisfying this condition
is essentially a multiple of the identity matrix
(since $\lambda_1$ corresponds to the mode
of a uniform shift of all $v_i$,
we can freely set it to $\lambda_2$
without changing the nature of the updating scheme).
%
Thus, in terms of convergence,
the single-bin scheme
(as adopted in the WL algorithm)
is generally the most efficient
updating scheme.




\subsubsection{Optimal coefficient of proportionality}



If we wish to find,
for a given updating matrix $\mathbf w$,
a near-optimal $\alpha(t)$,
%
then a good candidate is
%
\begin{equation}
\alpha(\tau) = \frac{1}{\lambda \, (\tau + t_0) },
\label{eq:alpha_trial}
\end{equation}
%
and to find the optimal value of $\lambda$.
%
Using Eq. \eqref{eq:alpha_trial}
in Eq. \eqref{eq:error_mbin1} yields
%
\begin{align}
\mathcal E
&=
\frac{1}{t+t_0}
\sum_{i=2}^n \frac{ \lambda_i^2 } { \lambda \, (2 \lambda_i - \lambda) }
\left[
  1 - \left(
    1 + \frac{t}{t_0}
  \right)^{1 - 2 \, \lambda_i/\lambda}
\right]
\notag
\\
&\stackrel{      t \gg t_0      }
          { =\joinrel=\joinrel= }
%
\sum_{i = 2}^n
\begin{dcases}
  \frac{ 1 }
       { t }
  \frac{ \lambda_i^2 }
       { \lambda \, (2 \, \lambda_i - \lambda) }
  &
  \mathrm{if \;} \lambda < 2 \, \lambda_i,
  \\%[1em]
  %
  %
  \frac{   1    }
       { 4 \, t }
  \ln \frac{ t + t_0 }
           {  t_0    }
  &
  \mathrm{if \;} \lambda = 2 \, \lambda_i,
  \\%[1em]
  %
  %
  \frac{ t_0^{ 2 \, \lambda_i / \lambda  - 1} }
       { t^{ 2 \, \lambda_i / \lambda } }
  \frac{ \lambda_i^2 }
       { \lambda \, (\lambda - 2 \, \lambda_i) }
  &
  \mathrm{if \;} \lambda > 2 \, \lambda_i.
\end{dcases}
\label{eq:y2_sum_perfect_invt}
\end{align}
%
\note{In the above expression,
$t + t_0$ are replaced by $t$.
%
Adding back $t_0$, we get
$$
\begin{aligned}
  \mathcal E
  \stackrel{         t \to \infty         }
           { =\joinrel=\joinrel=\joinrel= }
  \sum_{i = 2}^n
  \begin{dcases}
    \frac{ 1 }{ t + t_0 }
    \frac{ \lambda_i^2 } { \lambda \, (2 \, \lambda_i - \lambda) }
    &
    \mathrm{if \;} \lambda < 2 \, \lambda_i,
    \\%[1em]
    %
    %
    \frac{1}{4 \, (t + t_0) } \ln\frac{t + t_0}{t_0}
    &
    \mathrm{if \;} \lambda = 2 \, \lambda_i,
    \\
    %
    %
    \frac{ t_0^{ 2 \, \lambda_i / \lambda  - 1} }
         { (t + t_0)^{ 2 \, \lambda_i / \lambda } }
    \frac{ \lambda_i^2 }
         { \lambda \, (\lambda - 2 \, \lambda_i) }
    &
    \mathrm{if \;} \lambda > 2 \, \lambda_i.
  \end{dcases}
\end{aligned}
$$
}
%
The last two cases, $\lambda \ge 2\lambda_i$,
are asymptotically suboptimal
as they decay slower than $1/t$.
%
Thus, the optimal $\lambda$
satisfies $\lambda < 2 \min \lambda_i$,
and in this case,
we can seek that
the optimal value from
the smallest real root of
%
\begin{equation}
\sum_{i = 2}^n
\frac{ \lambda_i - \lambda }
{ \left(2 - \lambda/ \lambda_i \right)^2 }
= 0.
\label{eq:optimal_lambda_approx}
\end{equation}



Under the approximation, we have
$$
\frac{ \lambda_i^2 }{ \lambda \, (2 \lambda_i - \lambda) } \ge 1,
$$
with the equality achieved only at $\lambda = \lambda_i$,
we see the from Eq. \eqref{eq:y2_sum_perfect_invt}
that the minimal error is achieved with
$\lambda_2 = \dots = \lambda_n = \lambda$,
agreeing with the general conclusion
in Sec. \ref{sec:optWL}.




\subsection{\label{sec:band-matrix}
Uniform band matrix}



As an example, consider the following matrix $\mathbf w$
%
\begin{equation}
  w_{ij} = \mu_{|i-j|} + \mu_{i+j-1} + \mu_{2n+1-i-j},
  \label{eq:w_band}
\end{equation}
%
where $\mu_0, \dots, \mu_b \, (b < n)$
are a set of numbers satisfying
%
\begin{equation}
\mu_0 + 2 \, \mu_1 + \cdots + 2 \, \mu_b = 1,
\label{eq:m_normalization}
\end{equation}
%
and $\mu_l = 0$ for $l > b$.


This matrix, being symmetric,
satisfies Eqs. \eqref{eq:w_sumj}-\eqref{eq:w_balance},
for the flat distribution $p_i \equiv 1/n$.
%
It has $n$ orthonormal eigenvectors,
$\pmb\varphi^{(1)}, \dots, \pmb\varphi^{(n)}$,
given by
%
\begin{equation}
\varphi^{(k)}_i
= \sqrt{
    \frac{ 2 - \delta_{k, 1} }
         {       n           }
       }
  \cos \frac{ \left(i - \frac12\right) (k - 1) \, \pi}{n}.
\label{eq:wband_eigenvector}
\end{equation}
%
with eigenvalues
\begin{equation}
  \lambda_k = \mu_0 + \sum_{l = 1}^b 2 \, \mu_l \cos \frac{(k - 1)  \, l \pi}{n}.
\label{eq:wband_eigenvalue}
\end{equation}
%
\note{We derive Eqs.
  \eqref{eq:wband_eigenvector} and \eqref{eq:wband_eigenvalue}
  below.

  To simplify the calculation,
  let us first consider the unnormalized eigenvectors,
  $$
  \theta^{(k)}_i
  =
  \cos \frac{ \left(i - \frac12\right) (k - 1) \, \pi}{n},
  $$
  which satisfies the following reflection symmetry,
  \begin{equation}
    \theta^{(k)}_i = \theta^{(k)}_{1-i} = \theta^{(k)}_{2n+1-i}.
    \label{eq:wband_reflection_symmetry}
  \end{equation}
  %
  We shall also define $\mu_l = 0$, for $l \le 0$, then
$$
\begin{aligned}
&\sum_{i = 1}^n
  \theta^{(k)}_i
  \, w_{ij}
=
- \theta^{(k)}_j \, \mu_0
\\
&
+\sum_{i=1}^n
  \theta^{(k)}_i
  \left(
    \mu_{i - j} + \mu_{j - i} + \mu_{i + j - 1} + \mu_{2n+1-i-j}
  \right)
.
\end{aligned}
$$

Now
$$
\begin{aligned}
  &
  \sum_{i=1}^n
  \theta^{(k)}_i \,
  \left(
    \mu_{i - j} + \mu_{2n+1-i-j}
  \right)
  \\
  &=
  \sum_{l=1-j}^{n-j} \theta^{(k)}_{j+l} \, \mu_l
  +
  \sum_{l=n-j+1}^{2n-j} \theta^{(k)}_{2n+1-j-l} \, \mu_l
  \\
  &=
  \sum_{l=1-j}^{2n-j} \theta^{(k)}_{j+l} \, \mu_l
  =
  \sum_{l=0}^{b} \mu_l \, \cos\frac{(j+l-\frac12)(k-1)\pi}{n},
\end{aligned}
$$
where we have used Eq. \eqref{eq:wband_reflection_symmetry}.
%
Similarly,
$$
\begin{aligned}
  \sum_{i=1}^n
  \theta^{(k)}_i
  \left(
    \mu_{j - i} + \mu_{i+j-1}
  \right)
  =
  \sum_{l=0}^{b} \mu_l \, \cos\frac{(j-l-\frac12)(k-1)\pi}{n}.
\end{aligned}
$$
%
So
$$
\begin{aligned}
\sum_{i = 1}^n \theta^{(k)}_i \, w_{ij}
&=
- \theta^{(k)}_j \, \mu_0
+\sum_{l=0}^{b} \mu_l \, 2 \, \cos\frac{(k-1)l\pi}{n} \, \cos\frac{(j-\frac12)(k-1)\pi}{n}
\\
&= \theta^{(k)}_j \, \lambda_k,
\end{aligned}
$$
with $\lambda_k$ given by Eq. \eqref{eq:wband_eigenvalue}.

Next, let us compute the normalization factor.
%
If $k = 1$, $\theta^{(1)}_i = 1$, and
$
\sum_{i = 1}^n \left( \theta^{(1)}_i \right)^2 = n.
$
For $k \ge 2$,
$$
\begin{aligned}
  \sum_{i = 1}^n \cos^2 \left[\left(i - \frac1 2 \right) a\right]
  &=
  \frac n 2
  +
  \frac 1 2
  \sum_{i = 1}^n \cos\left[(2 i - 1)\, a \right]
  =
  \frac n 2,
\end{aligned}
$$
where $a = \frac{k-1}{n} \pi$.
Thus, the normalization factor $\sqrt{(2 - \delta_{k1})/n}$
encompasses both cases.

We can also show that columnwise orthogonality.
%
For this we need a lemma, for $a = q \, \pi/n$,
with $q$ a positive integer.
$$
\begin{aligned}
\sum_{k = 1}^n \cos[(k - 1) \, a]
&=
1 + \cos a + \dots + \cos[(n - 1) \, a]
\\
&=
\frac{
      \sin\frac a 2
    + \sin \left[ \left( n - \frac 1 2 \right) a \right]
    }
    {
      2 \, \sin \frac a 2
    }
\\
&=
\frac{ 1 - (-1)^q } { 2 }
= \operatorname{odd}(q),
\end{aligned}
$$
%
where, we have used
$\sin \left[ \left( n - \frac 1 2 \right) a \right]
= \sin \left( q \, \pi - \frac a 2 \right)
= -(-)^q\sin\frac a 2.$
%
and $\operatorname{odd}(q)$
yields $1$ for odd $q$ or $0$ otherwise.
%
Adding the case of $q = 0$, where the sum yields $n$,
we get
$$
\sum_{k = 1}^n \cos\left[(k - 1) \, \frac { q \, \pi } { n }  \right]
= n \delta_{q, 0} + \operatorname{odd}(q).
$$


Now, for the orthogonality,
$$
\begin{aligned}
  \sum_{k = 1}^n
  \theta^{(k)}_i \, \theta^{(k)}_j
  &=
  \frac 1 2
  \sum_{k = 1}^n
  \left[
    \cos \tfrac{ (i - j) (k - 1) \, \pi }
               {         n              }
    +
    \cos \tfrac{ (i + j - 1) (k - 1) \, \pi }
               {             n              }
  \right]
  \\
  &=
  \frac 1 2
  \left[
    n \, \delta_{i, j}
    +
    \operatorname{odd}(i - j)
    +
    \operatorname{odd}(i + j - 1)
  \right]
  \\
  &=
  \frac n 2 \, \delta_{i, j}
  + \frac 1 2.
\end{aligned}
$$
where we have used the fact
that $i - j$ shares the parity with $i + j$
in the last step.
%
So
$$
  \sum_{k = 1}^n
  \varphi^{(k)}_i \, \varphi^{(k)}_j
  =
  \frac 2 n
  \sum_{k = 1}^n
  \theta^{(k)}_i \, \theta^{(k)}_j
  -
  \frac 1 n
  =
  \delta_{i, j}.
$$
}
%
We discuss two special cases below.



\subsubsection{Tridiagonal matrix}



Particularly, if $\mu_2 = \dots = \mu_b = 0$,
the matrix $\mathbf w$ is tridiagonal,
%
\begin{equation}
\arraycolsep=3.6pt\def\arraystretch{1.4}
\mathbf w
=
\left(
  \begin{array}{cccccccc}
    1 - \mu_1   & \mu_1 & 0 & \dots & 0 \\
    \mu_1 & 1 - 2 \, \mu_1  & \mu_1 & \dots & 0 \\
    \vdots & &  & & \vdots \\
    0 & \dots & \mu_1 & 1 - 2 \, \mu_1  & \mu_1 \\
    0 & \dots & 0 & \mu_1 & 1 - \mu_1
  \end{array}
\right).
\label{eq:wtridiag}
\end{equation}
%
We have
\begin{equation}
  \lambda_k = 1 - 4 \, \mu_1 \sin^2 \frac{(k - 1) \, \pi}{2n}.
\label{eq:wtridiag_eigenvalue}
\end{equation}
%
Thus, for this updating matrix to be stable,
we need
$\min \lambda_k = \lambda_n > 0$,
and
$$
\mu_1 < \frac{1}{4 \, \cos^2\frac{\pi}{2n} }.
$$



We can find an analytical expression
of the optimal value of $\lambda$
in the limit of large $n$.
%
In this case,
we convert Eq.
\eqref{eq:y2_sum_perfect_invt}
%and
%\eqref{eq:optimal_lambda_approx}
to an integral,
$$
\begin{aligned}
\mathcal E
&=
\frac{2 \, n}{\pi \, t}
\int_0^{\pi/2}
\frac{ \left(1 - 4 \, \mu_1 \, \cos^2 p \right)^2 }
{ \lambda \, \left(2 - 8 \, \mu_1 \, \cos^2 p - \lambda \right) }
\, dp
\notag \\
&=
\frac{n}{4 \, t}
\left(
  \frac{2 - 4 \, \mu_1 + \lambda}{ \lambda }
  +
  \frac{ \lambda }
  { \sqrt{ (2 - \lambda) (2 - 8 \, \mu_1 - \lambda) } }
\right).
\end{aligned}
$$
\note{The integral is evaluated by contour integration.
  %
$$
\begin{aligned}
\mathcal E
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\int_0^{2 \, \pi}
\frac{ \left(1 - 4 \, \mu_1 \, \cos^2 p \right)^2 }
{ 2 - 8 \, \mu_1 \, \cos^2 p - \lambda }
\, dp
\\
&=
\frac{n}{\lambda \, t}
\frac{1}{2 \pi i}
\oint
\frac{ \left[1 - \mu_1 \, \left(z+\frac{1}{z}\right)^2 \right]^2 }
{ 2 - 2 \, \mu_1 \, \left(z + \frac{1}{z}\right)^2 - \lambda }
\, \frac{dz}{z},
\end{aligned}
$$
where the contour is the unit circle
in the complex plane of $z$.
%
This integral has five poles, one at the origin,
which produces a residue $\frac{2 - 4 \, \mu_1 - \lambda}{4}$
(from series expansion of small $z$),
and the other four at
$$
z + \frac{1}{z} = \pm\sqrt\frac{2-\lambda}{2 \, \mu_1},
$$
with two of them inside the unit circle:
$$
z_\pm = \pm \frac{\sqrt{2-\lambda} -\sqrt{2 - 8 \, \mu_1 - \lambda}}
{\sqrt{8 \mu_1}}.
$$
Thus,
$$
\begin{aligned}
  \mathcal E
&=
\frac{n}{\lambda \, t}
\left(
 \frac{2 - 4 \, \mu_1 - \lambda}{4}
 +
 \sum_{z = z_{\pm} }
 \frac{ \left(z^2 - \mu_1 (1 + z^2)^2 \right)^2 }
 { 2 z^4 (2 - 4 \mu_1 - \lambda - 4 \mu_1 z^2) }
\right)
\\
&=
\frac{n}{\lambda \, t}
\left(
  \frac{2 - 4 \, \mu_1 - \lambda}{4}
 +
 \frac{ \left(1 - \mu_1 \left(z + \frac{1}{z} \right)^2 \right)^2 }
 { 2 - 4 \mu_1 - \lambda - 4 \mu_1 z_{\pm}^2 }
\right)
\\
&=
\frac{n}{\lambda \, t}
\left(
  \frac{2 - 4 \, \mu_1 - \lambda}{4}
 +
 \frac{ (\lambda/2)^2 }
 { \sqrt{(2-\lambda) (2 - 8 \mu_1 -\lambda)} }
\right).
\end{aligned}
$$
}
%
Minimizing the function yields
%
\begin{equation}
\lambda = \frac{1 - 4 \, \mu_1} { 1 - 2 \, \mu_1 },
\label{eq:lambda_tridiag}
\end{equation}
%
and the error in the optimal is given by
%
\begin{equation}
\mathcal E
=
\frac{n}{t}
\left(
  1+ \frac{2 \, \mu_1^2}{1-4 \, \mu_1}
\right).
\label{eq:error_tridiag}
\end{equation}
%
%From Eq. \eqref{eq:error_tridiag}, it is clear that
Clearly, the error
increases rapidly with magnitude of $\mu_1$,
no matter its sign,
and optimal value is given by $\mu_1 = 0$,
which is the single-bin scheme.



\subsubsection{Gaussian matrix}



Another common updating scheme
in metadynamics is a Gaussian wave.
%
Below we show that this scheme
is stable, although
it might be less effective
in reducing local noises.



We start by defining
a continuous limit
for $b = n - 1 \gg 1$:
$x = l \, \Delta x$,
$\Delta x = \pi/n$,
and
$\mu_l = \mu(x) \, \Delta x$.
%
Then
Eq. \eqref{eq:wband_eigenvalue}
can be approximated by an integral:
%
\begin{equation}
\lambda_k
=
2 \int_0^\pi
  \mu(x) \, \cos \bigl( (k-1) \, x \bigr) \, dx,
\label{eq:lambda_int}
\end{equation}
%
with the normalization
%
\begin{equation}
1 = 2 \int_0^\pi \mu(x) \, dx.
\label{eq:mx_normalization}
\end{equation}



For the Gaussian wave,
$\mu(x) \propto \exp(-\frac{1}{2} x^2/\sigma^2)$.
%
If $\Delta x \ll \sigma \ll 1$,
we can extended
the upper limit of the integrals
in Eqs. \eqref{eq:lambda_int}
and \eqref{eq:mx_normalization}
to infinity, and
%
\begin{equation}
\mu(x)
=
\frac{            1            }
     { \sqrt{ 2 \pi \sigma^2 } }
%
\exp\left(
      -\frac{       x^2     }
            { 2 \, \sigma^2 }
    \right),
\end{equation}
%
%
The eigenvalues are given by
%
\begin{equation}
\lambda_k
=
\exp\left(
      -\frac{ (k - 1)^2 \, \sigma^2 }
            {           2           }
    \right).
\label{eq:lambda_Gaussian}
\end{equation}
%
The eigenvalues are all positive,
suggesting stability of the updating scheme.


However,
as $\sigma/\Delta x \gg 1$,
the smallest eigenvalue
%
$$
\lambda_n
\approx
\exp\left(
      -\frac{ n^2 \, \sigma^2 }
            {        2        }
    \right)
=
\exp\left[
      -\frac{ \pi^2 }{ 2 }
      \left(
        \frac{  \sigma }
             { \Delta x }
      \right)^2
    \right]
.
$$
%
is exponentially small.
%
This means that
the optimal value of $\lambda$
given by Eq. \eqref{eq:optimal_lambda_approx}
would be too small in practice,
for it requires $\lambda < 2 \, \lambda_n$.
%
In other words,
with a reasonable $\lambda$,
the error of the last few fluctuation modes
will always decay suboptimally,
accordingly to Eq. \eqref{eq:y2_sum_perfect_invt}.
%
However, since these modes
represent short wavelength fluctuations,
and these modes may be in
the system under consideration.
%
Thus,
we can truncate the sum of the error function
in Eq. \eqref{eq:y2_sum_perfect_invt}
at some $k_{\max}$,
which corresponds to a minimal length scale
$l_{\min} = \Delta x \, (n /k_{\max}) = \pi/k_{\max}$
greater than $\sigma$.
%
Then,
$$
\lambda_{ k_{\max} }
\approx
\exp\left(
      -\frac{ k_{\max}^2 \, \sigma^2 }
            {           2            }
    \right)
=
\exp\left(
      -\frac{  \pi^2 \, \sigma^2    }
            {    2 \, l_{\min}^2  }
    \right).
$$




\section{\label{sec:results}
Numerical results}


\subsection{Optimal $\alpha(t)$}


To determine the optimal schedule,
we shall test a model system
in the following fashion.
%
Initially $\alpha$ is set to some $\alpha_0$
for a sufficiently long time.
%
Then we reset the simulation time $t = 0$,
and switch to
$$
\alpha(t) = \frac{ 1 } { \lambda \, (t + t_0) },
$$
where $t_0 = 1/(\lambda \, \alpha_0)$
to make $\alpha(t)$ continuous.


1. Single-bin scheme.
Show the error versus $\lambda$.

Change upon using local sampling scheme.


2. Multiple-bin scheme.
Error vs. $\lambda$.

3. Optimality of the Single-bin scheme.
Error vs. $\mu_1$.


\section{\label{sec:conclusion}
Conclusions and Discussions}


We verified the optimality of the inverse-time formula.

We have shown that
without a priori information,
the single-bin scheme is asymptotically
the best among all updating scheme
in term of convergence.


\bibliography{simul}
\end{document}
