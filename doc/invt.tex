\documentclass[reprint, superscriptaddress, floatfix]{revtex4-1}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{hyperref}
\usepackage{setspace}

\hypersetup{
  colorlinks,
  linkcolor={red!30!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{64,0,42}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}
\newcommand{\answer}[1]{{\color{DarkBlue}\footnotesize \textsc{Answer.} #1}}
\newcommand{\summary}[1]{{\color{DarkPurple}\footnotesize \textsc{Summary.} #1}}


\newcommand{\Err}{E}
\newcommand{\ii}{\mathrm{i}}
%bin average
\newcommand{\bav}[1]{#1_\mathrm{av}}


\begin{document}



\title{Optimal updating magnitude in adaptive flat-distribution-sampling simulations}

\author{Cheng Zhang}
\author{Justin A. Drake}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}
\author{Jianpeng Ma}
\affiliation{
Department of Biochemistry and Molecular Biology,
Baylor College of Medicine, Houston, Texas 77030, USA}
\affiliation{
Department of Bioengineering,
Rice University, Houston, Texas 77005, USA}
\author{B. Montgomery Pettitt}
\email{mpettitt@utmb.edu}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}



\begin{abstract}
  We present a method of computing the optimal schedule
  of the updating magnitude
  for a class of flat-distribution sampling methods
  for free energy calculation including
  the Wang-Landau (WL) algorithm and metadynamics.
  %
  These methods rely on adaptive construction of
  a bias potential that offsets
  the potential of mean force by histogram-based adaptive updates.
  %
  The updating magnitude should decrease over time
  to reduce the error in the bias potential,
  and one may speed up the convergence by choosing an optimal schedule
  of the updating magnitude.
  %
  %Here, we will show that the optimal schedule
  %can be derived
  %from a mechanical analogy, in which
  %the schedule corresponds to
  %the velocity of a free particle with a position-dependent mass
  %and the final error serves as the action.
  %%
  %Therefore, the optimal schedule follows from
  %the equation of motion of the particle.
  %
  We will show that
  the optimal schedule depends on the updating scheme.
  %
  While the asymptotically optimal schedule for
  the single-bin updating scheme (commonly used in WL)
  is given by an inverse-time formula,
  that for the Gaussian updating scheme (commonly used in metadynamics)
  is often more complex.
  %and the initial updating magnitude is ideally half of
  %the previous equilibrium value.
  %
  Further,
  we show that the single-bin updating scheme
  belongs to a class of bandpass updating schemes
  that are optimal for asymptotic convergence.
  %
  These bandpass updating schemes aim at
  partially flattening a few selected histogram modes
  and their optimal schedule
  is also given by the inverse-time prescription.
  %
  Constructed from orthogonal polynomials,
  bandpass updating schemes serve as generalizations
  of the Langfeld-Lucini-Rago algorithm
  and can be used for parameter tuning for the Gaussian ensemble method
  and umbrella sampling.
  %
\end{abstract}

\maketitle



\section{Introduction}



Free energy calculation\cite{frenkel, newman} is a central theme
in computational physics and chemistry
that can provide insight into an array of phenomena not easily studied
with traditional experiments.
%
Given a system,
often the task is to compute
a distribution, $p(z)$,
along a collective variable, $z$, by sampling via either Monte Carlo\cite{
  frenkel, newman, landau_binder} (MC)
or molecular dynamics\cite{frenkel, karplus2002} (MD) simulations.
%
The distribution can be related to
the dimensionless free energy, or potential of mean force (PMF),
along the collective variable, $z$,
as $-\ln p(z)$.
%
However, reliable estimation of the PMF is often difficult
when the collective variable is energetically restricted to local regions
of the complicated, multimodal free energy surface.
%
Thus,
to overcome these energetic barriers in sampling,
%and to capture the global shape of the free energy surface
an effective strategy is to introduce a bias potential that
offsets the PMF
such that the distribution along $z$ is
flat\cite{mezei1987, berg1992, *lee1993,
wang2001, wang2001pre,
huber1994,
*laio2002, *laio2008, *barducci2011, *sutto2012}.
%
\note{The flat-distribution sampling
allows one to more accurately capture the
global shape of the free energy surface
by improving the sampling around the rarely visited barrier region
and accelerating transitions among free energy basins.}



Many efficient flat-distribution-sampling (FDS) techniques
based on adaptive construction of the bias potential
have been introduced,
including the Wang-Landau (WL) algorithm\cite{
  wang2001, wang2001pre}
and metadynamics\cite{huber1994,
  *laio2002, *laio2008, *barducci2011, *sutto2012, micheletti2004}
among others\cite{kim2006, *kim2007, kim2010, junghans2014,
  langfeld2012, pellegrini2014,
  maragliano2006, *abrams2008,
  zheng2010}.
%
These techniques regularly update the bias potential
to discourage future visits to previously sampled configurations
by incrementally elevating the bias potential.
%
A key difference lies
in the updating window function,
which specifies
a neighborhood around the current value of
$z$ on the free energy surface
where the updating should occur
as well as the relative updating magnitude.
%
Often, the window function
takes the form of a discrete
$\delta$-function (a boxcar function one histogram bin wide)
in WL,
or that of a Gaussian
in metadynamics.\cite{junghans2014}
%
We will refer to the two updating schemes
as the single-bin
and Gaussian updating schemes, respectively.
%
While the former is popular for MC simulations\cite{wang2001,
  wang2001pre, kim2006, *kim2007},
the latter is more popular for MD simulations
as it provides a smoother bias potential
that can be readily differentiated for force calculation.\cite{huber1994,
  *laio2002, *laio2008, *barducci2011, *sutto2012, junghans2014}
%
In both cases,
regular updates to the bias potential
disrupts the underlying equilibrium
sampling\cite{zhou2005, morozov2007, zhou2008},
and for convergence of the estimated PMF
one needs to gradually decrease
the updating magnitude of the bias.



The schedule of reducing
the updating magnitude
can critically affect
the precision of the final bias potential,
and hence that of the PMF\cite{laio2005, bussi2006, poulain2006,
belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016,
liang2007, min2007,
morozov2007, zhou2008,
komura2012, *caparica2012, *caparica2014,
barducci2008, dickson2011, dama2014}.
%
While a slow reduction of the updating magnitude
leads to large variation of the bias potential,
a rapid reduction can converge the bias potential
to a wrong value.
%
For the single-bin case, it is known that
the optimal updating schedule is given by the
inverse time\cite{
belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016,
liang2007,
morozov2007, zhou2008}
or
the inverse number of updating steps.


In this study,
we first present a method of computing
the optimal schedule
for an adaptive FDS method
of a general updating scheme,
including the single-bin and Gaussian versions.
%
We will map the optimization problem to a mechanical one,
in which the schedule plays the role of the velocity of
a free particle with a position-dependent mass,
and the final error becomes the action.
%
This mapping allows the optimal schedule
to be derived from the equation of motion
that minimizes the action.
%
The resulting optimal schedule
depends on the updating window function
through the mass of the particle.
%
While the optimal schedule in the single-bin case
is given by the known inverse-time formula,
that for Gaussian case is more complex
and sensitive to the simulation length
and the width of the Gaussian window.

We will then compare
the efficiency of error reduction
of different updating schemes.
%
We find that
the single-bin updating scheme
is optimal for long simulations
without a priori assumption of the smoothness of the PMF.
%
More generally,
it can be generalized to
a class of bandpass updating schemes
that are equally optimal.
%
The bandpass updating schemes
can avoid local noise
by partially flattening a few long-range distribution modes.
%
The bandpass updating schemes
are easier to use than the Gaussian updating scheme
in that the optimal schedule of the former
is simply given by the inverse-time formula,
saving the effort of going through the general procedure described above.

Finally, we present an application of
the bandpass updating schemes
by constructing it from
a sequence of orthogonal polynomials.
%
The bias potential is a linear combination
of orthogonal polynomials
and the coefficients are adjusted on-the-fly
based on the observed statistical moments.
%
This application can be regarded
as a generalization of the Langfeld-Lucini-Rago (LLR)
algorithm\cite{langfeld2012, pellegrini2014}.
%
A similar construction with
a quadratic bias potential\cite{neuhaus2006, *neuhaus2007, zhu2012}
allows one to sample an array of Gaussian distributions
with desired means and variances.

The article is organized as follows.
%
We present the analytical results in Sec. \ref{sec:theory},
provide numerical examples
in Sec. \ref{sec:results},
and conclude the article
in Sec. \ref{sec:conclusion}.




\section{\label{sec:theory}
Theory}



We develop the theory
in the following order.
%
In Sec. \ref{sec:background},
we briefly review the basics of FDS
and some known aspects of the optimal schedule.
%
Then, in Sec. \ref{sec:single-bin},
we derive the method of
computing the optimal schedule
in the simple case of the
single-bin updating scheme
used by the WL algorithm,
by proving the optimality
of the known inverse-time formula.
%
Next, we extend the method to the general case
of a multiple-bin updating scheme
that encompasses the Gaussian scheme used in metadynamics
in Sec. \ref{sec:multiple-bin}.
%
We then compare different updating schemes
and show that the single-bin scheme
and a class of generalized bandpass schemes
are optimal for convergence
in the long time limit
in Sec. \ref{sec:cmpschemes}.
%
Finally,
we use the bandpass updating schemes
to build a class of adaptive FDS algorithms
with quadratic bias potentials.



\subsection{\label{sec:background}
Flat-distribution sampling}



%\subsubsection{\label{sec:FDS}
%Bias potential}



Consider the problem of computing
the distribution, $p_i$,
along a discrete quantity, $i$,
for a given system.
%
%For example, $i$ can be the energy, $E$,
%in a lattice spin model or the temperature index
%in a simulated tempering simulation\cite{
%marinari1992, *lyubartsev1992}.
%
For a continuous quantity, $z$,
%which is often the case in a (classical) molecular system,
we can use the equivalent integer $i$ to represent
the index of a small interval, or a bin,
$(z, z + \delta z)$.
%
%The distribution is normalized as
%$\sum_{i = 1}^n p_i = 1$.


Sampling a multimodal distribution with deep basin(s)
is often challenging in regular MC and MD simulations.
%
To promote transitions among the basins,
we can apply a bias potential to alter
the intrinsic distribution to a smoother one, $\rho_i$.
%
We will broadly define
a flat-distribution-sampling (FDS) simulation
as a biased simulation
targeting a flat $\rho_i$ (i.e. $\rho_i \equiv 1/n$ for $n$ bins)
or a nearly flat one\cite{
dayal2004, *trebst2004, barducci2008, singh2011}.
%
Since the equilibrium distribution
under a bias potential, $u_i$, is given by
%
\begin{equation}
  \pi_i
  %=
  %C_i \, p_i \, e^{-u_i}
  =
  \frac{                p_i \, e^{-u_i} }
       { \sum_{j = 1}^n p_j \, e^{-u_j} }
  \propto
  p_i \, e^{-u_i}
  ,
  \notag
  %\label{eq:pi_p_V}
\end{equation}
%
we wish to find a bias potential that allows
the resulting $\pi_i$ to coincide
with the desired $\rho_i$.
%
Upon convergence, the bias potential would satisfy
%
\begin{equation}
  u_i \to \ln \frac{p_i}{\rho_i} + \mathrm{const.}
  ,
  \label{eq:Vi_target}
\end{equation}
%
and the PMF, $-\ln p_i$, can be readily
deduced from the bias potential. % $u_i$.



Many methods\cite{mezei1987, berg1992, *lee1993,
wang2001, wang2001pre, huber1994,
*laio2002, *laio2008, *barducci2011, *sutto2012}
have been developed to find the desired bias potential.
%
In an earlier class of equilibrium FDS methods\cite{
  mezei1987, berg1992, *lee1993, marinari1992, *lyubartsev1992},
the bias potential, $u_i$, is
estimated a priori and fixed
during simulation,
%
and a correction to the bias potential
is derived from the normalized histogram, $H_i$,
accumulated from the simulation as
%
\begin{equation}
  \hat u_i
  =
  u_i
  +
  \ln \frac{ H_i }
           { \rho_i }.
  \label{eq:vcorr_equil}
\end{equation}
%
\note{This follows from
  \begin{align*}
    H_i \approx \pi_i
    &\propto p_i \, e^{-u_i},
    \\
    \rho_i
    &\propto p_i \, e^{-\hat u_i}.
  \end{align*}
}
However, this formula often requires a long
accumulation period for a precise histogram,
and thus disallows
continuous improvement of the bias potential
%(hence that of the PMF)
as simulation lengthens.

Below we will focus on a more recent class of adaptive FDS methods,
including the WL algorithm\cite{wang2001, wang2001pre}
and metadynamics\cite{huber1994, laio2002, *laio2008, *barducci2011, *sutto2012},
which address the above shortcoming
by continuously and incrementally updating the bias potential.
%
%\subsubsection{Updating schemes}
%
In the WL algorithm\cite{wang2001, wang2001pre},
the bias potential, $u_i(t)$, is updated
at each step $t$ as
%
\begin{equation}
  u_i(t+1)
  =
  u_i(t)
  +
  \delta_{i, \, i(t)}
  \frac{ \alpha(t) } { \rho_{i(t)} }
  ,
\label{eq:wl_update}
\end{equation}
%
where $i(t)$ is the bin at step $t$,
$\delta_{i, \, i(t)}$ is the Kronecker delta function,
which is $1$ when $i = i(t)$ or $0$ otherwise,
and $\alpha(t)$ is the updating magnitude.
%
We refer to this scheme as
the \emph{single-bin (updating) scheme},
for it applies only to the bias potential
at the current bin, $i(t)$.
%
In a more general
\emph{multiple-bin (updating) scheme},
%including the Gaussian updating scheme used in metadynamics,
the updating also involves neighboring bins:
%
\begin{equation}
  u_i(t+1)
  =
  u_i(t)
  +
  w_{i, \, i(t)}
  \frac{ \alpha(t) }
       { \rho_{i(t)} }
  .
  \label{eq:mbin_update}
\end{equation}
%
A common example is the Gaussian updating scheme used in metadynamics,
with $w_{ij}$ being roughly a Gaussian centered on bin $j$.
%
Note that if the updating occurs
only every few sampling steps,
then $t$ means the number of updating steps.

%\subsubsection{Updating magnitude and the inverse-time formula}



While adaptive FDS simulations
can often sample the desired distribution,
frequent updates to the bias potential
disrupt the underlying equilibrium sampling
and introduce artificial errors that need to be
reduced by gradually decreasing the updating magnitude\cite{
  belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016,
  zhou2005, morozov2007, zhou2008,
  laio2005, bussi2006, poulain2006, liang2007,
  crespo2010, *atchade2011, *fort2015}.


In the original WL algorithm\cite{
wang2001, wang2001pre},
the updating magnitude, $\alpha(t)$,
(or $\ln f$ therein)
is controlled in stages.
%
In each stage, $\alpha(t)$
is kept constant,
and the histogram along $i$
is collected and monitored.
%
Once the histogram is sufficiently flat,
we reset the histogram and start a new stage
with $\alpha(t)$ reduced by a factor of $1/2$\cite{
wang2001, wang2001pre}.
%
While this scheme works well for early stages,
it tends to reduce the magnitude
too quickly in late stages, making the asymptotic error
saturate\cite{
belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016}.


%For the single-bin scheme
%used in WL algorithm,
A more effective way
of controlling the schedule, $\alpha(t)$,
in late stages
is to follow the formula
%
\begin{equation}
  \alpha(t) = \frac{1}{t},
  \label{eq:alpha_invt}
\end{equation}
%
where $t$,
referred to as the ``time'' below,
is the number of updating steps
from the beginning of the simulation\cite{
belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016,
morozov2007, zhou2008,
komura2012, *caparica2012, *caparica2014}.
%
This is a general result\cite{
  robbins1951, pellegrini2014},
and it has an intuitive explanation\cite{
  marsili2006, barducci2008}
that interprets the bias potential
as a runtime average computed from all data points collected so far,
which absorbs the contribution of the newest data point
by its due weight,
the inverse of the current sample size, $t$
(cf. Appendix \ref{sec:equilerr}).




\subsection{\label{sec:single-bin}
Single-bin updating scheme}



In this section,
we will derive the optimal $\alpha(t)$
for the single-bin scheme,
Eq. \eqref{eq:wl_update},
as a preparation
for the more general multiple-bin scheme.
%
We will first express the error of
the bias potential, $u_i(t)$,
as a functional of $\alpha(t)$,
and then minimize it by variation.
%



\subsubsection{Error}



We define the net error in the bias potential
by deducting two contributions.
%
First, to deduct the target value given by
Eq. \eqref{eq:Vi_target},
we introduce a shifted bias potential
%
\begin{equation}
  v_i(t)
  \equiv
  u_i(t)
  -
  \ln \frac { p_i }
            { \rho_i }
  ,
  \label{eq:v_def}
\end{equation}
%
which should tend to a constant of $i$
upon convergence.
Note that the updates to $v_i(t)$ are
equivalent to the updates to $u_i(t)$,
since the shift
remains a constant during simulation.
%%
%In terms of $v_i$'s, Eq. \eqref{eq:pi_p_V}
%becomes
%%
%\begin{equation}
%  \pi_i
%  =
%  \frac{                \rho_i \, e^{-v_i} }
%       { \sum_{j = 1}^n \rho_j \, e^{-v_j} }
%  \propto
%  \rho_i \, e^{-v_i}.
%  \label{eq:pi_p_v}
%\end{equation}
%%
%According to Eq. \eqref{eq:pi_p_V},


Second, the bias potential following Eq. \eqref{eq:wl_update}
generally grows over time.
%
Since a uniform increase of $u_i(t)$, or, equivalently, that of $v_i(t)$,
does not affect the resulting distribution, $\pi_i(t)$,
the real deviation of the bias potential
depends only on the difference between $v_i(t)$
and a baseline value, $\bav{v}(t)$,
for the average growth\cite{
dama2014}.
%
We define the bias potential deviation from the
bin average, $\bav{v}(t) \equiv \sum_{i=1}^n \rho_i \, v_i(t)$,
as
%
\begin{equation}
  v_{*i}(t) \equiv v_i(t) - \bav{v}(t)
  .
\label{eq:x_def}
\end{equation}
%
\note{Alternatively,
  we may define $v_{*i}(t)$ such that
  $\rho_i \, e^{-v_{*i}(t)}$ is a normalized distribution,
  i.e.
  $$
  v_{*i}(t) \to -\ln\left(
    \frac{ (p_i/\rho_i) \, e^{ -u_i(t) } }
    { \sum_{j=1}^n p_j \, e^{ -u_j(t) } }
  \right).
  $$
  We can readily show that the two definitions are equivalent
  for small $v_{*i}$.}%
%
Below we will use the notations, $x_{*i}$ and $\bav{x}$,
for a general variable, $x_i$.
%
The total error is then given by
%
\begin{equation}
  \Err(t)
  =
  \sum_{i = 1}^n \rho_i \,
  \left\langle v_{*i}^2(t) \right\rangle
  .
\label{eq:error_def}
\end{equation}




\subsubsection{\label{sec:sbin_diffeq}
Differential equation}



To proceed, we will
approximate the finite difference Eq. \eqref{eq:wl_update}
by a differential equation\footnote{In
going to the continuous-time setup,
we find it more convenient to shift the origin of time by $-1$,
e.g. the sum $\sum_{i=1}^T$ is mapped to the integral $\int_0^T dt$.}
%
\begin{equation}
  \dot u_i(t)
  \equiv
  \frac{ d u_i(t) } { dt }
  \approx
  \alpha(t) \, \frac{ h_i(t) } { \rho_i }
  \equiv
  \alpha(t) \, f_i(t)
  ,
  \label{eq:ut_diffeq}
\end{equation}
%
with
%
$h_i(t) = \delta_{i, i(t)}$
%\begin{equation}
%  h_i(t) = \delta_{i, i(t)}
%  ,
%  \label{eq:h_def}
%\end{equation}
%
being the instantaneous histogram,
which is equal to $1$
for the bin $i(t)$ at time $t$
or zero otherwise,
and we have defined
$f_i(t) \equiv h_i(t) /\rho_i$.
%
Clearly,
%
\begin{equation}
  \bav{f}(t)
  = \sum_{i=1}^n \rho_i \cdot \frac{ h_i(t) } { \rho_i }
  = \sum_{i=1}^n \delta_{i, i(t)}
  = 1
  ,
  \label{eq:fav1}
\end{equation}
%
The shifted bias potential, $v_i(t)$,
follows the same differential equation
as the shift is a constant.
%
Deducting the bin average [cf. Eq. \eqref{eq:x_def}],
we get
%
\begin{equation}
  \dot v_{*i}(t)
  \approx
  \alpha(t) \, f_{*i}(t)
  .
  \label{eq:vt_diffeq}
\end{equation}


We can split the histogram into
a deterministic averaged part, $\langle f_i(t) \rangle$,
and a random fluctuating part, $\zeta_i(t)$,
%
\begin{equation}
  f_i(t) =
  \langle f_i(t) \rangle
  +
  \zeta_i(t)
  .
  \notag
  %\label{eq:h_split}
\end{equation}
%
The deterministic part can be related
to an average in an ensemble consisting of
many copies of similar simulations
sharing the same schedule $\alpha(t)$
and the same bias potential at time $t$.
%
The initial states and the stochastic forces
during the process may differ, however.
%
For sufficiently small $\alpha(t)$,
the bias potential remains roughly the same for a short period,
and we may assume a quasi-equilibrium sampling process
such that
%with the deterministic part specified by Eq. \eqref{eq:pi_p_V}:
%
\begin{align}
  \langle f_i(t) \rangle
  %&=
  %\frac{ \langle h_i(t) } { \rho_i } \rangle
  &\approx
  \frac{ \pi_i(t) } { \rho_i }
  =
  \frac{                          e^{-v_{*i}(t)} }
       { \sum_{j = 1}^n \rho_j \, e^{-v_{*j}(t)} }
  %\notag
  %\\
  %&
  \approx
  1 - v_{*i}(t)
  ,
  \notag
  %\label{eq:h_ave}
\end{align}
%
where we have assumed the smallness
of $v_{*i}(t)$ in the linear expansion.
%
\note{
The second step follows from
$$
\frac{ \langle h_i(t) \rangle }
     { \rho_i }
\approx
\frac{                       1 - v_{*i}  }
     { \sum_{ r = 1 }^n \rho_j (1 - v_{*j}) }
=
\frac{                       1 - v_{*i}  }
     { 1 - \sum_{ j = 1 }^n \rho_j \, v_{*j} }
.
$$
}%
%
Deducting the bin average and using Eq. \eqref{eq:fav1}, we get
$\langle f_{*i}(t) \rangle \equiv \langle f_i(t) \rangle - 1 \approx - v_{*i}(t)$,
and
%
\begin{align}
  f_{*i}(t)
  &\approx
  - v_{*i}(t)
  +
  \zeta_i(t)
  .
  \label{eq:sh_ave}
\end{align}


We will approximate the histogram fluctuation part, $\zeta_i(t)$,
hence its deviation from the bin average, $\zeta_{*i}(t)$,
as white noise such that
\begin{equation}
  \langle \zeta_{*i}(t) \, \zeta_{*i}(t') \rangle
  = G_i \, \delta(t - t')
  ,
  \notag
  %\label{eq:G_def}
\end{equation}
for some $G_i$,
and $\delta(t)$ is the Dirac delta function.
%
We expect the approximation to hold for simulation
much longer than the autocorrelation time.

From Eqs.
\eqref{eq:vt_diffeq} and \eqref{eq:sh_ave},
we get % a set of decoupled equations
%
\begin{equation}
  \dot v_{*i}(t)
  %=
  %\alpha(t) \, \left[ \langle f_{*i}(t) \rangle + \zeta_{*i}(t) \right]
  \approx
  -\alpha(t) \, \left[ v_{*i}(t) - \zeta_{*i}(t) \right]
  .
  %\notag
  \label{eq:dxdt_singlebin}
\end{equation}
%
Without the random part, $\zeta_{*i}(t)$,
this equation would drive
the deviation of the bias potential, $v_{*i}(t)$,
to $0$ at long times,
implying the convergence.
%
The formal solution of Eq. \eqref{eq:dxdt_singlebin} is
%
\begin{equation}
  v_{*i}(T)
  =
  v_{*i}(0) \, e^{-q(T)}
  +
  \int_0^T
    \dot{\vartheta}\bigl( q(t) \bigr) \, \zeta_{*i}(t) \, dt,
  \label{eq:xt_solution}
\end{equation}
%
where
%
$q(t) \equiv \int_0^t \alpha(t') \, dt'$,
%
and
%
\begin{align}
  \vartheta(q')
  &\equiv
  e^{q' - q(T)}.
  \label{eq:theta_def}
\end{align}



\subsubsection{Optimal schedule}



From Eqs. \eqref{eq:error_def} and \eqref{eq:xt_solution},
we can write the total error at the end of period $T$
as a sum of a residual error, $E_R$ (from the decay of the initial error)
and an asymptotic error, $E_A$:
%
\begin{align}
  \Err(T)
  =
  \Err_R(T) + \Err_A(T)
  ,
  \label{eq:error_tot}
\end{align}
%
where
\begin{align}
  \Err_R(T)
  &= \Err(0) \, e^{-2 \, q(T)}
  ,
  \label{eq:ER_sbin}
  \\
  \Err_A(T)
  &=
  \sum_{i=1}^n \rho_i
    \int_0^T\!\!\int_0^T
    \langle
      \zeta_{*i}(t) \zeta_{*i}(t')
    \rangle
    \dot \vartheta\bigl( q(t) \bigr)
    \dot \vartheta\bigl( q(t') \bigr)
    \, dt dt'
  \notag \\
  &= \Gamma \, \int_0^T \dot \vartheta^2\bigl( q(t) \bigr) \, dt
  ,
  \label{eq:EA_sbin}
\end{align}
with
%$\Gamma \equiv \sum_{i=1}^n \rho_i \, G_i$.
$\Gamma \equiv \sum_{i=1}^n \rho_i \, G_i \approx \sum_{i=1}^n \rho_i \int_{-\infty}^{\infty} \langle \zeta_{*i}(t) \zeta_{*i}(0) \rangle \, dt$.
%\begin{equation}
%  \Gamma \equiv \sum_{i=1}^n \rho_i \, G_i
%  =
%  \sum_{i=1}^n \rho_i \int_0^T
%    \langle \xi_{*i}(t) \xi_{*i}(0) \rangle \, dt
%  .
%  \notag%\label{eq:Gamma_from_G}
%\end{equation}

The error depends implicitly on the curve, $\alpha(t)$,
or, equivalently, the curve, $q(t)$,
via $v_{*i}(T)$.
%
%If $\alpha(t)$ is a constant, $a_0$,
%so that $q(t) = a_0 \, t$,
%then after a long period,
%the error approach an equilibrium value
%%
%\begin{equation}
%  \Err_\mathrm{equil.}
%  = \Err_A = \Gamma \int_0^\infty e^{-2\,q'} \, a_0 \, d q'
%  = \frac{1}{2} a_0 \Gamma
%  .
%  \label{eq:error_equil_sbin}
%\end{equation}
%
Our aim is to find the $q(t)$
that minimizes this error.
%
It is convenient to
vary the curve, $q(t)$ ($0 < t < T$),
under a fixed value of $q(T)$,
since it allows us to focus on
the asymptotic error, $E_A(T)$.
%
The contribution of the residual error
will be considered later
by adjusting the value of $q(T)$.
%
Note that the asymptotic error
has the form of an action of a free particle
at $\vartheta\bigl( q(t) \bigr)$,
with the endpoints fixed at
$\vartheta\bigl( q(0) \bigr)  = e^{- q(T)}$
and
$\vartheta\bigl( q(T) \bigr) = 1$.
%
Thus, the velocity should be a constant
%Since the residual error
%is fixed during the process,
%the asymptotic error,
%resembling the action of a free particle,
%demands a constant velocity
%
\begin{equation}
  \frac{d}{dt} \vartheta\bigl( q(t) \bigr) = c
  .
\label{eq:dthetadt_const}
\end{equation}
%
Then, by using Eq. \eqref{eq:theta_def}, we have
%
\begin{equation}
  e^{ q(t) - q(T) }
  =
  c \, (t + t_0)
  =
  \frac{ t + t_0 }
       { T + t_0 }
  ,
\label{eq:expqt}
\end{equation}
%
where $t_0$ and $c$ are two constants,
and the latter has been determined
from the $t = T$ case
in the second step.
%
Taking the logarithm,
and differentiating both sides
with respect to $t$,
we get the inverse-time schedule
%
\begin{equation}
  \alpha(t) = \frac{ 1 }{ t + t_0 }
  ,
\label{eq:alpha_invt1}
\end{equation}
%
and Eq. \eqref{eq:alpha_invt}
is the special case of $t_0 = 0$.
%

We now determine the optimal value of $q(T)$,
which is fixed in the above.
%
This is equivalent to choosing a value of $t_0$
\big[because $q(T) = \ln\bigl[1 + (T/t_0)\bigr]$,
from the $t = 0$ case of Eq. \eqref{eq:expqt}\big].
%
Using Eq. \eqref{eq:alpha_invt1} in
Eqs. \eqref{eq:ER_sbin} and \eqref{eq:EA_sbin}
yields
\begin{align}
  \Err_R(T)
  &= \frac{ \Err(0) \, t_0^2 } { (T + t_0)^2 }
  ,
  \notag
  %\label{eq:ER_sbin1}
  \\
  \Err_A(T)
  &= \frac{ \Gamma \, T } { (T + t_0)^2 }
  ,
  \label{eq:EA_sbin1}
\end{align}
%
and the total has a minimum of
\begin{equation}
  \Err(T)
  =
  \frac{ \Gamma } { T + t_0 }
  ,
  \label{eq:Emin_sbin}
\end{equation}
%
reachable at $t_0 = \Gamma /\Err(0)$.
%
%If we assume that at $t = 0$,
%the system has just completed a long simulation
%under a constant updating magnitude, $a_0$,
%then the initial error can be computed from Eq. \eqref{eq:EA_sbin}
%as
%\begin{equation}
%  \Err(0)
%  = \Gamma \int_0^\infty e^{-2\,q'} \, a_0 \, d q'
%  = \frac{1}{2} a_0 \Gamma
%  ,
%  \label{eq:error_equil_sbin}
%\end{equation}
%and
%%
%\begin{equation}
%  t_0 = \frac{ 2 } { a_0 } = \frac{1}{ \alpha(t = 0) }
%  .
%  \label{eq:t0_sbin}
%\end{equation}
%
%The minimum error can be related to
%the expected histogram fluctuation\cite{zhou2005, zhou2008}
%(cf. Appendix \ref{sec:hfluc}).



\subsection{\label{sec:multiple-bin}
Multiple-bin updating scheme}



We now turn to a general multiple-bin updating scheme.
%
By definition, Eq. \eqref{eq:mbin_update},
a visit to bin $j$ in this case results in updates of the bias potential
not only at bin $j$, but also at a few neighboring bins $i$'s.
%
Similar to Eq. \eqref{eq:ut_diffeq},
we can approximate the updating scheme
as a differential equation,
%
\begin{equation}
  \dot u_i(t)
  \approx
  \alpha(t) \,
  \sum_{j=1}^n w_{ij} \, f_j(t)
  =
  \alpha(t) \,
  \sum_{j=1}^n w_{ij} \, \frac{ h_j(t) } { \rho_j }
  .
  \label{eq:ut_diffeq_mbin}
\end{equation}
%
The optimization procedure is similar to
the single-bin case.
%
The key simplification comes from the
projection of the bias potential to the $n$ eigenmodes
of the updating matrix, $\mathbf w$,
formed by the relative magnitudes, $w_{ij}$'s.



\subsubsection{\label{sec:updating-matrix}
Updating matrix and spectral decomposition}



We will first review the spectral decomposition
of the updating matrix, $\mathbf w$,
which helps establish the convergence criterion
and the decomposition of error into
independent eigenmodes.
%
The matrix, $\mathbf w$, is subject to several conditions.
%
First, we have a fixed-point condition\cite{bussi2006, dama2014}.
%
To sample the desired distribution,
$\pmb\rho = (\rho_1, \dots, \rho_n)$,
the growth rate of the bias potential,
${\dot u}_i(t)$
in Eq. \eqref{eq:ut_diffeq_mbin},
should be a constant of $i$
if $h_j(t)$ were the same as $\rho_j$,
i.e.
$\sum_{j=1}^n w_{ij}$ should be a constant
to allow the bias potential to grow uniformly
in the asymptotic regime.
%
If the rate of growth is positive,
we can, by a proper overall scaling of $\mathbf w$,
write this condition as
%
\begin{equation}
  \sum_{j = 1}^n w_{ij} = 1
  .
\label{eq:w_sumj}
\end{equation}
%
In other words, $(1, \dots, 1)^T$
is a right eigenvector of $\mathbf w$
with eigenvalue $1$.
%
Thus, the transpose $\mathbf w^T$
resembles a transition matrix,
except that some elements can be negative
in our cases.
%
Below we will %take advantage of the analogy and
borrow techniques used
in studying transition matrices\cite{vankampen},
but limit ourselves to properties that are unaffected
by the admission of negative elements into $\mathbf w$.


%To simplify the ensuing discussion,
Second, we limit ourselves to matrices % $\mathbf w$'s
that satisfy
the detailed balance condition for $\pmb\rho$,
%
\begin{equation}
  \rho_i \, w_{ij} = \rho_j \, w_{ji}
  .
  \label{eq:w_detailedbalance}
\end{equation}
%
This requires the scaled updating matrix,
$\sqrt{ \rho_i/\rho_j } \, w_{ij}$,
to be symmetric,
and the symmetry allows the diagonalization
of $\mathbf w$ with a set of
eigenvectors, $\phi_{ki}$,
%
\begin{equation}
  \sum_{i = 1}^n \phi_{ki} \, w_{ij}
  =
  \lambda_k \, \phi_{kj}
  ,
\label{eq:eig_w}
\end{equation}
%
satisfying the orthonormal conditions\cite{vankampen}:
%
\begin{align}
  \sum_{k = 0}^{n - 1}
    \phi_{ki} \, \phi_{kj}
  &=
  \delta_{ij} \, \rho_i,
  \label{eq:eig_orthonormal_cols}
  \\
  \sum_{i = 1}^n
    \frac{ \phi_{ki} \, \phi_{li} }
         { \rho_i }
  &=
  \delta_{kl}
  ,
  \label{eq:eig_orthonormal_rows}
\end{align}
%
where we have started the index of the eigenmodes at $0$
instead of $1$ for later convenience.

The above symmetry implies the existence of
a left eigenvector, $\pmb \rho$,
corresponding to the uniform right eigenvector
associated with Eq. \eqref{eq:w_sumj},
with eigenvalue $1$:
%
\begin{equation}
  \sum_{i = 1}^n \rho_i \, w_{ij}
  =
  \sum_{i = 1}^n \rho_j \, w_{ji}
  =
  \rho_j
  .
  \notag
  %\label{eq:w_balance}
\end{equation}
%
We will label this eigenvector by $k = 0$,
and
%
\begin{equation}
  \phi_{0i} = \rho_i,
\label{eq:eigenmode0}
\end{equation}
%
with $\lambda_0 = 1$.
%
Clearly, Eq. \eqref{eq:eigenmode0}
satisfies the normalization condition
given by Eq. \eqref{eq:eig_orthonormal_rows}
for $k = l = 0$,
while the orthogonality demands
%
\begin{equation}
  \sum_{ i = 1 }^n \phi_{ki}
  =
  \delta_{k0}
  .
\label{eq:ortho0}
\end{equation}

We can further define
a generalized Fourier transform, $\mathcal{F}$,
for variable, $x_i$,
%
\begin{equation}
  {\tilde x}_k
  \equiv \mathcal{F}[x_i]_k
  \equiv \sum_{i = 1}^n \phi_{ki} \, x_i
  ,
  %\notag
  \label{eq:gft_def}
\end{equation}
%
and the inverse transform is
%$x_i = \sum_{k = 0}^{n-1} \phi_{ki} \, \tilde{x}_k / \rho_i$.
%
\begin{equation}
  x_i = \frac{1}{\rho_i} \sum_{k = 0}^{n-1} \phi_{ki} \, \tilde{x}_k
  .
  %\notag
  \label{eq:gft_inv}
\end{equation}
%
Then,
we can rewrite Eq. \eqref{eq:ortho0} as
$\mathcal F[1]_k = \delta_{k0}$,
%
and Eq. \eqref{eq:eigenmode0} means that
the bin average
%of any quantity, $x_i$, is the first mode of the Fourier transform:
$\bav{x}= \mathcal F[x_i]_0 = \tilde x_0$,
%
and
\begin{equation}
  \tilde x_{*k}
  = \tilde x_k - \bav{x} \, \delta_{k0}
  =
  \begin{dcases}
    0           & k = 0
    ,
    \\
    \tilde x_k  & k > 0
    .
  \end{dcases}
  \label{eq:xtstar}
\end{equation}
%
Thus, the only effect
of the bin-average deduction on $\tilde x_k$ is to
annihilate the $k=0$ mode.
%
From Eqs. \eqref{eq:gft_def} and \eqref{eq:gft_inv},
we have Parseval's theorem,
\begin{equation}
  \sum_{k=1}^{n-1} \tilde x_k \, \tilde y_k
  =
  \sum_{k=0}^{n-1} \tilde x_{*k} \, \tilde y_{*k}
  =
  \sum_{i=1}^n \rho_i \, x_{*i} \, y_{*i}
  .
  \label{eq:parseval}
\end{equation}
\note{Proof:
\begin{align*}
  \sum_{k=0}^{n-1} \tilde x_{*k} \, \tilde y_{*k}
  &=
  \sum_{k=0}^{n-1} \tilde x_{*k} \sum_{i=1}^n \phi_{ki} \, y_{*i}
  \\
  &=
  \sum_{i=1}^n \left( \sum_{k=0}^{n-1} \tilde x_{*k} \, \phi_{ki} \right) \, y_{*i}
  =
  \sum_{i=1}^n \rho_i \, x_{*i} \, y_{*i}.
\end{align*}
}

We can now use Eq. \eqref{eq:eig_w} to diagonalize
the multiple-bin updating scheme, Eq. \eqref{eq:mbin_update},
as
%
\begin{equation}
  {\tilde v}_k(t + 1) =
  {\tilde v}_k(t) + \alpha(t) \, \lambda_k \,
  {\tilde f}_k(t)
  ,
  \label{eq:vkupdate}
\end{equation}
%
or in continuous time, we have, for $k > 0$,
%
\begin{equation}
  \dot{\tilde v}_k(t)
  =
  \alpha(t) \, \lambda_k \, {\tilde f}_k(t)
  \approx
  -\alpha(t) \, \lambda_k \,
  \bigl[ {\tilde v}_k(t) - {\tilde \zeta}_k(t) \bigr]
  ,
  \label{eq:vt_diffeq_mbin}
\end{equation}
%
where
we have used Eq. \eqref{eq:xtstar} and
the transformed version of Eq. \eqref{eq:sh_ave}
in the second step.

Equation \eqref{eq:vt_diffeq_mbin} shows that
in the absence of the fluctuating term, $\tilde \zeta_k(t)$,
the deviation of the bias potential would tend to zero
at a rate of $\alpha(t) \, \lambda_k$.
%
Thus, the updating scheme is convergent
only if no eigenvalue, $\lambda_k$, is negative
for $k > 0$.
%
%With a set of heterogeneous eigenvalues,
%the errors of the eigenmodes are reduced at different rates.
%Thus, the optimization problem of the final error
%becomes more complicated and it often
%involves a compromise among eigenmodes.
%
The formal solution is given by
\begin{equation}
  \tilde v_k(T)
  =
  \tilde v_k(0) \, e^{-\lambda_k \, q(T)}
  +
  \int_0^T
    \dot{\vartheta}_k\bigl( q(t) \bigr) \, \tilde\zeta_k(t) \, dt
  ,
  \label{eq:xt_solution}
\end{equation}
where
\begin{equation}
  \vartheta_k(q') \equiv e^{\lambda_k \, [q' - q(T)]}
  .
  \label{eq:uk_def}
\end{equation}





\subsubsection{Error}



By Eq. \eqref{eq:parseval},
we can rewrite the error defined in Eq. \eqref{eq:error_def} as
a sum of the errors in individual modes\footnote{If
the error weighting distribution differs from $\rho_i$
in Eq. \eqref{eq:error_def},
the error would contain quadratic cross terms among different eigenmodes.
%
While the solution for the optimal schedule retains the same form,
the mass function defined in Eq. \eqref{eq:mass_func}
will include the corresponding cross terms.
}
%$\Err(T) = \sum_{k = 1}^{n - 1} \left\langle {\tilde v}_k^2(T) \right\rangle$.
%
\begin{align}
  \Err(T)
  =
  \sum_{k = 1}^{n - 1}
    \left\langle
      {\tilde v}_k^2(T)
    \right\rangle
  .
  \notag
\end{align}
%
As in Eq. \eqref{eq:error_tot},
we can decompose the error as the sum
of the residual error and the asymptotic error, with
\begin{align}
  \Err_R(T)
  &=
  \sum_{k = 1}^{n-1}
    \left\langle
      {\tilde v}_k^2(0)
    \right\rangle \,
    e^{ - 2 \, \lambda_k  \, q(T) }
  ,
  \label{eq:error_res}
  \\
  \Err_A(T)
  &=
  \sum_{k=1}^{n-1}
  \int_0^T\!\!\int_0^T
  \langle
    \tilde\zeta_k(t)
    \tilde\zeta_k(t')
  \rangle
  \vartheta_k\bigl(q(t)\bigr)
  \vartheta_k\bigl(q(t')\bigr)
  \, dtdt'
  \notag \\
  &=
  \int_0^T
  \sum_{k = 1}^{n-1}
  \Gamma_k \, \dot \vartheta_k^2\bigl( q(t) \bigr) \, dt
  ,
  \label{eq:error_asym}
\end{align}
%
where
%
%$\Gamma_k$ is computed from
%the autocorrelation function
%of the histogram fluctuation,
%$\bigl\langle {\tilde \zeta}_k(0)
%\, {\tilde \zeta}_k(t) \bigr\rangle$,
%as:
%
\begin{equation}
  \Gamma_k
  \approx
  \int_{-\infty}^\infty
  \bigl\langle
    \tilde\zeta_k(t) \tilde\zeta_k(0)
  \bigr\rangle
  \, dt
  \to
  \sum_{t = -\infty}^\infty
  \bigl\langle
    \tilde\zeta_k(t) \tilde\zeta_k(0)
  \bigr\rangle
  ,
  \label{eq:Gamma_sum}
\end{equation}
%
with the second expression being
the equivalent in the discrete-time setting.
%
Equation \eqref{eq:Gamma_sum} represents
a first approximation for modeling
a general fluctuation, ${\tilde \zeta}_k(t)$,
as an equivalent white noise.
In this way,
the underlying sampling process
affects the error only through the
few numbers, $\Gamma_k$'s.
%
Note that by Eq. \eqref{eq:parseval}, we have
$\sum_{k=1}^{n-1} \bigl\langle {\tilde \zeta}_k(t) \, {\tilde \zeta}_k(0) \bigr\rangle
= \sum_{i=1}^n \rho_i \bigl\langle \zeta_{*i}(t) \, \zeta_{*i}(0) \bigr\rangle,$
and after summing $t$ from $-\infty$ to $\infty$,
\begin{equation}
  \sum_{k=1}^{n-1} \Gamma_k = \sum_{i=1}^n \rho_i \, G_i = \Gamma
  .
  \label{eq:Gammak_sum}
\end{equation}
This shows the equivalence of
Eq. \eqref{eq:EA_sbin1} and \eqref{eq:error_asym}
in the case of the single-bin updating scheme, with $\lambda_k \equiv 1$.
%In Appendix \ref{sec:Gamma_measure},
%we discuss a way of estimating $\Gamma_k$
%from an FDS simulation under a constant magnitude.
%



\subsubsection{\label{sec:optschedule}
Optimal schedule}



As in the single-bin case,
by varying the schedule, $\alpha(t)$, under a fixed value of
$q(T) = \int_0^T \alpha(t) \, dt$,
we can focus on the asymptotic error,
$\Err_A(T)$.
%
The value of $q(T)$ will be adjusted later
to minimize the total error in Sec. \ref{sec:optinitalpha}.
%
We can rewrite Eq. \eqref{eq:error_asym} much like an action
of a particle whose position is given by $q(t)$:
%
\begin{equation}
  \Err_A(T)
  =
  \int_0^T
    {\mathcal L} \bigl[ q(t)\bigr]
    \, dt
  ,
  \notag
  %\label{eq:error_asym_Lagrangian}
\end{equation}
%
where the Lagrangian is
%
\begin{align}
  {\mathcal L} \bigl[ q(t) \bigr]
  &=
  \sum_{ k = 1 }^{n - 1}
    \Gamma_k \, {\dot \vartheta}_k^2\bigl[ q(t) \bigr]
  %\notag
  %\\
  %&=
  =
  %\sum_{ k = 1 }^{n - 1}
  %  \Gamma_k \, \lambda_k^2 \, \vartheta_k^2 \bigl[ q(t) \bigr]
  %\; \dot q^2( t )
  %\notag
  %\\
  %&=
  M^2\bigl(q(T) - q(t) \bigr)
  \; \dot q^2( t )
  .
\notag
\end{align}
%
where we have
used Eq. \eqref{eq:uk_def} and
defined the (square-root) mass function as
%
\begin{equation}
  M(\bar q)
  \equiv
  \sqrt{
    \textstyle\sum_{ k = 1 }^{n - 1}
    \Gamma_k \, \lambda_k^2 \, e^{-2 \, \lambda_k \, \bar q}
  }
  .
  %\notag
  \label{eq:mass_func}
\end{equation}
%
In this mechanical analogy,
the schedule, $\dot q(t) = \alpha(t)$,
corresponds to the velocity of the particle,
%
and the above Lagrangian, containing only the kinetic energy,
describes a free particle
with a position-dependent mass.
%
Here, the Hamiltonian coincides with the Lagrangian:
%
\begin{equation}
  \mathcal H
  =
  \frac{ \partial \mathcal L }
       { \partial \dot q     }
  \, \dot q
  -
  \mathcal L
  =
  2 \, \mathcal L
  - \mathcal L
  =
  \mathcal L
  .
  \notag
  %\label{eq:error_asym_Hamiltonian}
\end{equation}
%
Since the Lagrangian
does not explicitly depend on time, $t$,
the Hamiltonian is conserved,\cite{goldstein, *landau_mechanics, *arnold}
which means that the asymptotic error grows
linearly with time at a rate of $\mathcal L$.
We may set
%
\begin{equation}
  \sqrt{ \mathcal H }
  =
  \sqrt{ \mathcal L }
  =
  M\bigl( q(T) - q(t) \bigr)
  \;
  \dot q(t)
  =
  \frac{C_M}{T}
  ,
  \label{eq:Lagrangian_const}
\end{equation}
%
for some positive $C_M$.
%
Equation \eqref{eq:Lagrangian_const} serves
as a generalization of Eq. \eqref{eq:dthetadt_const}.
%
By integrating this equation of motion, we get
%
\begin{equation}
  \int_{ q(T) - q(t) }^{ q(T) }
    M(\bar q)
    \;
    d \bar q
  =
  C_M \, \frac t T
  ,
  \label{eq:q_opt}
\end{equation}
%
and $C_M$ can be determined from
the $t = T$ case as
%
\begin{equation}
  C_M =
  \int_{ 0 }^{ q(T) }
    M( \bar q )
    \;
    d \bar q
  .
  \label{eq:mint}
\end{equation}
%
Thus, Eq. \eqref{eq:Lagrangian_const}
gives an implicit equation for $q(t)$,
and hence the optimal schedule,
$\alpha(t) = \dot q(t)$.
%
In Appendix \ref{sec:schedule_geometry},
we give a geometric characterization of this solution.
%
The minimal asymptotic error is
%
\begin{equation}
  \Err_A(T)
  =
  \mathcal L \, T
  =
  \frac { C_M^2 } { T }
  =
  \frac 1 T
  \left(
    \int_0^{ q(T) } M(\bar q) \, d \bar q
  \right)^2
  .
\label{eq:error_asym2}
\end{equation}



%\subsubsection{\label{sec:eqlerr}
%Initial error
%%from a simulation under a constant updating magnitude
%}



\subsubsection{\label{sec:optinitalpha}
  Optimal initial updating magnitude
}


We now determine the optimal value of $q(T)$
that minimizes the total error defined in Eq. \eqref{eq:error_tot}.
%
To model the initial error for Eq. \eqref{eq:error_res},
we will assume that at $t = 0$
the system has undergone a preliminary FDS simulation
under a constant updating magnitude, $a_0$,
for $T_0$ steps.
%
If the initial bias potential
before the preliminary run is zero,
one can show that
\begin{equation}
  \left\langle
    {\tilde v}_k^2(0)
  \right\rangle
  = \frac 1 2 \, a_0 \, \Gamma_k \, \lambda_k
  + \epsilon_k
  ,
  \label{eq:xt2_eql1}
\end{equation}
%
where
$\epsilon_k \equiv \tilde u_k^2 \, e^{-2\, \lambda_k \, a_0 \, T_0}$
and if no eigenvalue is zero,
then we can ignore the $\epsilon_k$ term
in the case of $T_0 \to \infty$,
%
\begin{equation}
  \left\langle
    {\tilde v}_k^2(0)
  \right\rangle
  = \frac 1 2 \, a_0 \, \Gamma_k \, \lambda_k
  .
  \label{eq:xt2_eql}
\end{equation}
%
\note{
Consider a long period, $T_0$, under a fixed schedule,
$\alpha(t) = a_0$,
the residual error becomes negligible, and
the $k$th component of the asymptotic error
is given by
%
\begin{align*}
  \left\langle
    {\tilde v}_k^2(T_0)
  \right\rangle
  =
  \int_0^{T_0}
    \Gamma_k \, (\lambda_k \, a_0)^2 \,
      e^{ 2 \, \lambda_k \, a_0 \, (t - T_0) }
    \, dt
  \stackrel{ T_0 \to \infty }
  { =\joinrel=\joinrel=\joinrel= }
  \frac 1 2 \, \Gamma_k \, \lambda_k \, a_0
  .
\notag
%\label{eq:error_eql}
\end{align*}
}
%



%
From Eqs. \eqref{eq:error_res}, \eqref{eq:xt2_eql1},
and \eqref{eq:error_asym2},
we find that
the optimal $q(T)$ satisfies
%
\begin{align}
  \mathcal Y\bigl( q(T) \bigr)
  &\equiv
  \frac{ M\bigl( q(T) \bigr) } { T }
    \int_0^{q(T)} M(q') \, d q'
  -\frac{a_0}{2} \, M^2\bigl( q(T) \bigr)
  \notag \\
  &
  -\sum_{k = 1}^{n-1}
  \lambda_k \, \epsilon_k \, e^{-2\lambda_k \, q(T)}
  =0
  .
\label{eq:opt_qT}
\end{align}
%
%\begin{align}
%  m\bigl( q(T) \bigr)
%  +
%  \frac{2}{a_0 \, C_M \, M\bigl( q(T) \bigr)}
%  \sum_{k = 1}^{n-1}
%  \lambda_k \, \tilde u_k^2 \,
%  e^{-2 \, \lambda_k \, [a_0 \, T_0 + q(T)] }
%  \\=
%  \frac{1} { T \, (a_0 / 2) }
%  .
%\label{eq:opt_qT}
%\end{align}
%
\note{Let $q_T \equiv q(T)$,
$$
\begin{aligned}
  \frac{
    \partial \Err_R(T)
  }
  {
    \partial q_T
  }
  &=
  -a_0 \, M^2(q_T)
  -2
  \sum_{k=1}^{n-1} \lambda_k \,
  \tilde u_k^2 e^{-2 \, \lambda_k \, (a_0 \, T_0 + q_T)}
  ,
  \\
  \frac{
    \partial \Err_A(T)
  }
  {
    \partial q_T
  }
  &=
  \frac 2 T \,
  M(q_T) \,
  \int_0^{ q_T } M(q) \, dq
  =
  \frac{ 2 \, C_M } { T } \, M(q_T)
  .
\end{aligned}
$$
Then solve $\partial (\Err_R + \Err_A) / \partial Q_c = 0$.
}
%
We can rewrite the condition in terms of
the initial updating magnitude by Eq. \eqref{eq:Lagrangian_const} %\eqref{eq:mQ_invTa},
\begin{align}
  \alpha(0)
  %=
  %\frac{1}{T \, m\bigl( q(T) \bigr) }
  =
  \frac{ a_0 } { 2 }
  +
  \sum_{k = 1}^{n-1}
  \frac{
    \lambda_k \, \epsilon_k \, e^{-2\lambda_k \, q(T)}
  }{M^2\bigl( q(T) \bigr)}
  .
  \label{eq:opt_alpha0}
\end{align}
%
If the initial bias, $\epsilon_k$, is negligible,
which is often true if no eigenvalue is close to $0$,
then
%
\begin{equation}
  \alpha( t = 0 )
  \approx
  \frac{ a_0 }
       { 2 }
  ,
\label{eq:half_alpha0}
\end{equation}
%
i.e. the optimal initial updating magnitude
is half of the previous equilibrium value\footnote{This factor, $1/2$,
happens to coincide with the
recommended reduction factor of the updating magnitude
at stage transitions
in the original WL algorithm\cite{
wang2001, wang2001pre}.
}
%[cf. Eq. \eqref{eq:t0_sbin}].



\subsubsection{\label{sec:procedure}
Procedure of computing the optimal schedule and error
}


Based on the above results,
we give a procedure of computing
the optimal schedule and the error
for a given adaptive FDS simulation.
%
Basically, we need to extract
$\lambda_k$'s, $\Gamma_k$'s, and $\epsilon_k$'s
from the given system and updating scheme (input),
and then apply them to Eq. \eqref{eq:q_opt}
for the schedule,
as shown in Fig. \ref{fig:vardep}.
%
While the updating scheme alone
determines $\lambda_k$'s,
$\Gamma_k$'s depend also on the system
and the underlying sampling process.
%
We have

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/vardep.pdf}
  }
  \caption{
    \label{fig:vardep}
    Computation of the optimal schedule and the error.
  }
\end{figure}




\begin{enumerate}

\item
Compute the eigenvalues, $\lambda_k$'s
from the updating matrix, $\mathbf w$.
%
Note that
no eigenvalue can be negative in a convergent updating scheme.

\item \label{step:prerun}
Run a preliminary adaptive FDS simulation
under a constant updating magnitude, $a_0$.
%
\note{The updating magnitude should be sufficiently small
  such that the resulting error is no greater than
  multiple of $1.0$.}

\item \label{step:Gamma}
Estimate $\Gamma_k$'s from Eq. \eqref{eq:varXt}
(cf. Appendix \ref{sec:Gamma_measure})
as well as the initial bias $\epsilon_k$.\footnote{The
  value of $\tilde u_k$ can be estimated
  from the histogram-corrected bias potential
  by Eq. \eqref{eq:uhatav},
  although it leads to overestimated
  $\epsilon_k$'s for large-$k$ modes.
}

\item \label{step:qT}
Compute the optimal value of $q(T)$ by solving Eq. \eqref{eq:opt_qT},
using, e.g. the Newton-Raphson method\cite{press3rd}\footnote{In
  our implementation, the equation was solved for $\ln q(T)$
  to avoid negative values of $q(T)$,
  and we imposed a lower bound,
  $\bigl|\mathcal Y\bigl(q(T)\bigr)/q(T)\bigr|$, for
  $\mathcal Y'\bigl(q(T)\bigr)$
  in computing its change.
  The initial value was set to $q(T) = \ln(1+T\,a_0/2)$.
  We also used the bisection method as a fallback.}
%
%Since $m(\bar q)$ is a decreasing function of $\bar q$,
%we can start from an interval $[Q_L, Q_R]$
%that satisfies $m(Q_L) > 2/(T\,a_0) > m(Q_R)$
%(e.g. we can choose $Q_L = 0$,
%  and repeatedly double $Q_R$ until
%  the latter inequality is satisfied).
%%
%We can then iteratively refine the interval
%by substituting $Q_M = (Q_L + Q_R)/2$
%for either $Q_L$ and $Q_R$,
%depending on the sign of $m(Q_M) - 2/(T \, a_0)$,
%until the width of the interval is less than
%the desired precision,
%%
%and set $q(T)$ to $Q_M$.

\item \label{step:alpha}
Compute the optimal schedule by
numerically integrating Eqs. \eqref{eq:q_opt} and \eqref{eq:mint}.
%
To handle Gaussian updating schemes more effectively,
we have chosen the $\mathcal N+1$ grid points
($\mathcal N=20000$ here)
over $[0, q(T)]$ to be given by
$q_{[i]} = \bigl(q(T) + 1\bigr) - \bigl(q(T)+1\bigr)^{1-i/\mathcal N}$,
where $i = 0, \dots, \mathcal N$.\footnote{For
  a mass distribution, $m(\bar q)$, that is
  inversely proportional to $\bar q$,
  this grid would make the integral,
  or the difference, $t_{[i+1]} - t_{[i]}$,
  roughly a constant.}
%
The curve $q(t)$ is then integrated
using the trapezoidal rule\cite{press3rd},
resulting in a list of values, $\bigl(q_{[i]}, t_{[i]} \bigr)$.
%
We then compute the updating magnitude by numerical differentiation:
$\alpha_i = (1 - e^{-\lambda_1 \, \hat \alpha_i})/\lambda_1$\footnote{This
  discretization follows from the runtime average interpretation
  discussed in Appendix \ref{sec:equilerr}.}
and
$\hat \alpha_i \equiv (q_{[i+1]} - q_{[i-1]}) / (t_{[i+1]} - t_{[i-1]})$,
with
$q_{[ -1]} \equiv 0$,
$q_{[\mathcal N+1]} \equiv q(T)$,
$t_{[ -1]} \equiv 0$, and
$t_{[\mathcal N+1]} \equiv T$.
%
The resulting values of $(t_{[i]}, \alpha_{[i]})$
can be linearly interpolated
to get the updating magnitude, $\alpha(t)$, at any simulation time, $t$.


\item
Compute the total error $\Err(T)$ from
Eqs.
\eqref{eq:error_tot},
\eqref{eq:error_res},
\eqref{eq:xt2_eql1},
and
\eqref{eq:error_asym2}.

\end{enumerate}

%The above procedure is not always necessary.
%%
%As we will show below, for a class of bandpass updating schemes,
%  the asymptotically optimal schedule
%  is simply given by the inverse-time schedule, Eq. \eqref{eq:alpha_invt1}.
%
%For these updating schemes,
%  the preliminary run with a constant updating magnitude
%  can be replaced by the WL-style stagewise reduction
%  until the updating magnitude falls under
%  the value given the inverse-time prescription\cite{
%    belardinelli2007, *belardinelli2007jcp, *belardinelli2008, *belardinelli2016}.



\subsubsection{\label{sec:Gaussian_scheme}
Gaussian and homogeneous updating schemes}



A commonly-used multiple-bin updating scheme in metadynamics
is the Gaussian updating scheme,
whose updating matrix can be characterized by
a fixed Gaussian centered on the bin of visit
(with some boundary adjustment).
%to satisfy the constraints discussed in Sec. \ref{sec:updating-matrix},
%as shown in Fig. \ref{fig:mat}(a).
%
The eigenvalues of this scheme are given by\cite{bussi2006}
\begin{align}
  \lambda_k
  =
  \exp\left[
        -\frac 1 2
        \left(
         \frac{ 2 \, \pi \, k \, \sigma_z }
              { g \, \Delta z  }
        \right)^2
      \right]
  ,
%\notag
\label{eq:lambda_Gaussian}
\end{align}
where $\sigma_z$ and $\Delta z = z_{\max} - z_{\min}$
are the width of the Gaussian and the span
in the original collective variable, $z$,
respectively.

More generally, we can define a class of
translationally-invariant or \emph{homogeneous} updating schemes
by a rigid window function with a floating center.
%
As shown in Appendix \ref{sec:homo},
the homogeneous updating schemes share
the same set of orthonormal eigenvectors, $\phi_{ki}$,
which are cosine and sine functions,
and differ only by the eigenvalues.

Note that we can reconstruct the updating matrix
from the eigenvalues and eigenvectors
as\cite{bussi2006}
%
\begin{equation}
  w_{ij}
  =
  \frac{1}{\rho_i} \sum_{k=0}^{n - 1}
  \lambda_k \, \phi_{ki} \, \phi_{kj}
  .
  \label{eq:w_from_phi}
\end{equation}
%
Thus, it is possible to improve the updating scheme
by selecting a spectrum of eigenvalues
optimal for asymptotic convergence.
We will pursue this strategy below.




\subsection{\label{sec:cmpschemes}
Comparison of updating schemes}


Now that we can compute the optimal schedule
for a given updating scheme,
we will turn to the problem of finding
optimal updating schemes
given that each updating scheme
adopts the optimal schedule.


\subsubsection{\label{sec:optWL}
Asymptotic optimality of the single-bin scheme}



We will first show
that the single-bin scheme is asymptotically optimal.
%
Consider a class of updating matrices
sharing the same set of eigenvectors,
hence the same $\Gamma_k$'s,
but with different positive eigenvalues,
$\lambda_k$'s.
%
Using the Cauchy-Schwarz inequality, we have,
for any set of nonnegative numbers, $c_k$'s\footnote{This
inequality follows from the nonnegativity of
the quadratic polynomial,
$$
\int_0^T
  dt \sum_{k = 1}^{n-1} \Gamma_k \,
    \left( {\dot \vartheta}_k \, \chi - c_k \right)^2
  \equiv
  A_2 \, \chi^2 + A_1 \, \chi + A_0
  ,
$$
which implies a nonpositive discriminant,
$A_1^2 - 4 \, A_0 \, A_2$.}
%
%
\begin{align}
&
\left(
  \int_0^T dt
    \sum_{k = 1}^{n-1}
      \Gamma_k \, {\dot \vartheta}_k^2\bigl( q(t) \bigr)
\right)
%
\left(
  \int_0^T dt
    \sum_{k = 1}^{n-1}
      \Gamma_k \, c_k^2
\right)
%
\notag
\\
&
\qquad \qquad
\ge
\left(
  \int_0^T dt
    \sum_{k = 1}^{n-1}
      \Gamma_k \, c_k \, {\dot \vartheta}_k \, \bigl( q(t) \bigr)
\right)^2
\notag
\\
&
\qquad \qquad
=
\left(
  \sum_{k = 1}^{n-1} \Gamma_k \, c_k
    \left[
      1 - e^{ -\lambda_k \, q(T) }
    \right]
\right)^2.
\notag
%\label{eq:CSineq}
\end{align}
%
Note that the last expression %of Eq. \eqref{eq:CSineq}
depends on the curve, $q(t)$ ($0 < t < T$),
only through the endpoint value, $q(T)$,
which is fixed in the variational process.
%
Thus, the inequality sets a lower bound
for the asymptotic error $\Err_A(T)$
given in Eq. \eqref{eq:error_asym}.
%
The equality is achieved
if $\dot \vartheta_k\left( q(t) \right) = c_k$
for all $k > 0$ at any $t$,
up to a multiplicative factor.
%
Solving these equations yields
a set of identical eigenvalues,
$\lambda_1 = \cdots = \lambda_{n-1}$
and identical $c_k$'s.

We can show that the above optimal solution for asymptotic convergence
represents the single-bin updating scheme.
%
Since the $k = 0$ eigenmode represents
a uniform shift of the bias potential,
we can set $\lambda_0 = \lambda_1$
without changing the nature of the updating scheme.
%
By Eqs. \eqref{eq:w_from_phi} and
\eqref{eq:eig_orthonormal_cols}, we have
\begin{align*}
  w_{ij}
  = \frac{1}{\rho_i} \sum_{k=0}^{n-1} \lambda_1 \phi_{ki} \phi_{kj}
  = \lambda_1 \, \delta_{ij}
  ,
\end{align*}
and $\lambda_1$ must be $1$ by \eqref{eq:w_sumj}.
%
Thus, $w_{ij} = \delta_{ij}$
is reduced to the updating matrix of the single-bin updating scheme.
%
This demonstrates the asymptotic optimality of
the single-bin updating scheme.
%
%We can further verify that
%$\vartheta_k = (t+t_0)/(T+t_0)$
%in this case,
%and the optimal schedule coincides with
%Eq. \eqref{eq:alpha_invt1}.
%
%The asymptotic error can be written as
%$\Err_A(T) = \sum_{ k = 1 }^{n-1} \Gamma_k \, T / (T + t_0)^2$,
%which is also identical to Eq. \eqref{eq:EA_sbin1}.
%
%\begin{align}
%  \Err_A(T)
%  =
%  \frac{       T     }
%       { (T + t_0)^2 }
%  \sum_{ k = 1 }^{n-1}
%    \Gamma_k
%  ,
%\end{align}
%




\subsubsection{\label{sec:bandpass}
Bandpass updating schemes}


We can generalize
the single-bin scheme to a class of
perfect (brick-wall) \emph{bandpass} updating schemes
that discard certain noise modes
of short wavelengths.
%
If the eigenmodes are sorted in descending order
of the wavelength,
and the PMF is sufficiently smooth
to be representable by the first $K$
eigenmodes,
we may assign the first $K$ eigenmodes with eigenvalue $1$
and the rest of eigenmodes with eigenvalue $0$:
%
\begin{equation}
  \lambda_1 = \cdots = \lambda_{K-1} = 1
  ,
  \mathrm{\; and \;}
  \lambda_K = \cdots = \lambda_{n-1} = 0
  .
  \label{eq:lambda_bandpass}
\end{equation}
%
The updating matrix can then be reconstructed
from the eigenvalues by Eq. \eqref{eq:w_from_phi}.
%(cf. Appendix \ref{sec:homo_bandpass} for the case of homogeneous updating schemes).
%
%Then, the updating matrix is, according to
%Eq. \eqref{eq:w_from_phi},
%%
%$$w_{ij} = \frac{1}{\rho_i} \sum_{k=0}^{K-1} \phi_{ki} \, \phi_{kj}.$$
%
In effect, each updating step of the bandpass updating scheme
is equivalent to a single-bin updating step,
Eq. \eqref{eq:wl_update},
followed by a filtering step
that completely removes the components of the noise modes of $k \ge K$
from the bias potential.
%
%The asymptotic error is then
%$\Err_A(T) = \sum_{ k = 1 }^{ K - 1 } \Gamma_k T/(T + t_0)^2$,
%which is less than that from Eq. \eqref{eq:EA_sbin1}.

Applying this construction to the basis of cosine and sine functions,
we get a homogeneous bandpass updating scheme
that resembles the Gaussian updating scheme,
but with better convergence in the asymptotic limit.
%
In Sec. \ref{sec:lj},
we will further compare the performance of the two updating schemes
in an example.
%
%While both updating scheme are able to deliver
%a smooth and differentiable potential of mean force,
%the bandpass updating scheme is superior in asymptotic convergence,
%because of the set of clear-cut spectrum of eigenvalues.
%Eq. \eqref{eq:lambda_bandpass},
%instead of a Gaussian one, Eq. \eqref{eq:lambda_Gaussian},
%
%\footnote{As an estimate of error,
%  we note that truncating a linear or quadratic function
%  at mode $K$ for a non-periodic variable
%  results in an error roughly proportional to
%  $|\max_i u_i - \min_i u_i|^2/K^3$.}



\subsection{\label{sec:bandpass_poly}
Bandpass updating schemes constructed from orthogonal polynomial bases}


In addition to the above application to
homogeneous updating schemes,
we will consider here bandpass updating schemes
constructed from a sequence of orthogonal polynomials.
%
These updating schemes are more intuitive and easier to implement
as the bias potential in this case is
a low-order polynomial of the collective variable.

\subsubsection{Orthogonal polynomial basis}

It is convenient to
replace the discrete bin index, $i$,
by the continuous variable, $z$;
%
and we will replace $\rho_i$ by $\rho(z) \, \delta z$,
  $\phi_{ki}$ by $\phi_k(z) \, \delta z$,
  and
  $w_{ij}$ by $w(z, z') \, \delta z$, etc.
%
We define the eigenvectors by
$\phi_k(z) = R_k(z) \, \rho(z)$,
where $R_k(z)$ is the $k$th member of an sequence of orthogonal polynomials
that satisfy
\begin{align*}
  \int R_k(z) \, R_{k'}(z) \, \rho(z) \, d z = \delta_{k, k'}
  .
\end{align*}
%
We can readily verify that the eigenvectors satisfy
Eqs. \eqref{eq:eig_orthonormal_cols}, \eqref{eq:eig_orthonormal_rows},
and \eqref{eq:ortho0}.
%
\note{For $k = 0$, $R_0(z)$ must be a constant,
  and by the orthonormal condition, we have $R_0(z) = 1$.
  Thus, Eq. \eqref{eq:ortho0} is satisfied.}%
%
The eigenvectors allow us to construct the updating matrix
from Eq. \eqref{eq:w_from_phi},
with the eigenvalues given by \eqref{eq:lambda_bandpass}:
\begin{align*}
  w(z, z')
  =
  \sum_{k=1}^{K-1} \frac{ \phi_k(z) \, \phi_k(z') } { \rho(z) }
  = \sum_{k=1}^{K-1} R_k(z) \, R_k(z') \, \rho(z'),
\end{align*}
%
where we have dropped the $k=0$ mode
that corresponds to a uniform shift
of the bias potential.
%
By Eq. \eqref{eq:mbin_update},
the bias potential is updated as
%
$$
u(z, t+1) = u(z, t)
+ \sum_{k=1}^{K-1} R_k(z) R_k\bigl( z(t) \bigr) \, \alpha(t).
$$
%
We can write the bias potential
as a superposition of the orthogonal polynomials
%
\begin{equation}
  u(z, t) = \sum_{k=1}^{K-1} c_k(t) \, R_k(z),
  \label{eq:uz_decomp}
\end{equation}
%
with the coefficients updated as
\begin{equation}
  c_k(t+1) = c_k(t) + \alpha(t) \, R_k\bigl( z(t) \bigr)
  .
  \label{eq:ckupdate}
\end{equation}

For $k > 0$, $R_k(z)$ is a polynomial of degree $k$ with zero expectation.
%
Thus, $R_k\bigl(z(t)\bigr)$ roughly
represents the deviation of the $k$th statistical moment
of the instantaneous distribution
from its expectation.
Since high-order moments are generally less reliable,
this method is best used with a small $K$ for a narrow distribution.
%
However, by constructing a mixture or expanded ensemble,
we can simultaneously sample multiple target distributions
spread over a wider range, as exemplified below.


\subsubsection{LLR algorithm}

For example,
to sample a flat distribution, $\rho(z) = 1/\Delta z$,
over $[z_c - \Delta z/2, z_c + \Delta z/2]$,
we can use the Legendre polynomials\cite{arfken}.
%
If the interval is sufficiently small,
we can limit the basis to a single linear mode,
$R_1(z) = 2 \sqrt{3} \, (z - z_c)/\Delta z$,
with $K = 2$.
%
The bias potential is a linear function of $z$,
and this updating scheme is essentially the LLR algorithm\cite{langfeld2012}
for the single-interval case,
and the inverse-time schedule
was recommended for optimal convergence of $c_1$\cite{pellegrini2014}.

In real application, the algorithm is often applied simultaneously
to an array of adjacent intervals
to cover a wider range of the collective variable, $z$.
%
In this way, an attempt to move out of the current interval
does not have to be rejected,
but can be interpreted as a transition to
another internal and accepted with a proper probability.



\subsubsection{\label{sec:aus}
Umbrella sampling with adaptive Gaussian ensembles}

In analogy to the LLR algorithm,
we will consider the case of sampling a Gaussian distribution
$\rho(z) \propto \exp[-(z-z_c)^2/(2\sigma_z^2)]$,
with a basis constructed from
the Hermite polynomials\cite{arfken}.
%
As discussed above,
the method is effective only to
a local region of a multimodal distribution,
and thus we will assume a small width, $\sigma_z$.
%
Because the Gaussian distribution has no hard boundaries
as in the previous case,
we need to use
in addition to the linear mode,
$R_1(z) = (z - z_c)/\sigma_z$,
a quadratic one,
$R_2(z) = \bigl[(z - z_c)^2 - \sigma_z^2\bigr] /\left(\sqrt 2 \, \sigma_z^2\right)$,
to restrict the sampling region.
%
This results in a quadratic bias
potential\cite{maragliano2006, *abrams2008, zhu2012, neuhaus2006, *neuhaus2007},
as commonly employed in umbrella sampling\cite{torrie1974, *torrie1977}
and the method of Gaussian ensemble\cite{hetherington1987,
*challa1988, *costeniuc2006}.

Upon convergence,
the parameters, $c_1$ and $c_2$,
can be related to the slope and curvature of the PMF
[cf. Eq. \eqref{eq:Vi_target}].
%
By Eq. \eqref{eq:uz_decomp}, we have
$$
u(z) = \frac{c_1}{\sigma_z} (z - z_c)
+ \sqrt 2 \, c_2 \left[ \frac{(z-z_c)^2}{2 \, \sigma_z^2} - \frac{1}{2} \right].
$$
Thus, we can interpret
$c_1/\sigma_z$ as the average local mean force around $z_c$.
%
The coefficient
$\sqrt{2} \, c_2$ represents the relative ``pressure''
to restrain the width of the distribution to
be the same as the target Gaussian,
and it would exceed $1$ in a barrier region
where the PMF is locally concave,
$-d^2\ln p(z)/dz^2 < 0$.

We can extend the sampling range
by constructing a mixture\cite{shirts2017}
or an expanded ensemble
from a superposition of multiple Gaussian umbrellas
centered at different places via
the frameworks of
parallel\cite{swendsen1986,
  *geyer1991, *hukushima1996, *hansmann1997, *sugita1999,
  *earl2005, *zuckerman2011, *rauscher2009,
  neuhaus2006, *neuhaus2007, kim2010}
and/or simulated tempering\cite{marinari1992,
  *lyubartsev1992, li2007,
  park2007, *nguyen2013, *zhang2015st}.
%
Here, we will focus on the latter framework,
in which the transition probability from
the $i$th umbrella to the $j$th is given by
%
\begin{align}
A(i\to j) =
\min\left\{1,
  \frac{Z^{(i)}}{Z^{(j)}}
  e^{
    u^{(i)}\bigl( z(t), t \bigr)
    - u^{(j)}\bigl( z(t), t \bigr)
    }
  \right\}
  ,
  \label{eq:transprob_st}
\end{align}
where $z(t)$ is the current value of $z$,
and $Z^{(i)} \equiv \int e^{-\beta \, \Phi(\mathbf x)-u^{(i)}(z(\mathbf x),t)} \, d\mathbf x$
is the partition function.
%
Here,
$\beta = 1/(k_B T)$ is the inverse temperature,
$\mathbf x$ denotes the internal degrees of freedoms,
and
$\Phi(\mathbf x)$ is the unbiased potential energy.
%
Note that the unknown parameters, $\ln Z^{(i)}$'s,
serve as the bias potential
for inter-umbrella transitions, and thus can be updated
using the single-bin scheme,
Eq. \eqref{eq:wl_update}\cite{li2007}.

Since bandpass updating schemes are straightforward extension
of the single-bin updating scheme,
we can borrow the existing protocol\cite{belardinelli2007,
  *belardinelli2007jcp, *belardinelli2008, *belardinelli2016}
to the optimally control updating magnitude.
%
We can adopt the WL stage-by-stage strategy in early stages,
and switch to the inverse-time schedule later
once the updating magnitude from the former approach
falls under that from the latter.\footnote{We
  slightly modified the transition criterion
  from the WL to the inverse-time schedule.
  Such a switch was only permitted
  upon completion of a WL stage,
  and the time, $t$, was interpreted as the number of updating steps
  in this just completed WL stage.
  If the switch was successful,
  the above $t$ was used as the offset $t_0$
  in the inverse-time regime.}
  %
Since the WL strategy uses the histogram flatness
as a criterion for stage transitions,
we will redefine the flatness
from the fluctuation given by Eq. \eqref{eq:maxFk}
based on the eigenmode decomposition
(this allows a uniform definition for
both the continuous intra-umbrella
and the discrete inter-umbrella components,
cf. Appendix \ref{sec:hfluc}).
%
%In the asymptotic regime, the optimal schedule
%is simply given by the inverse-time schedule, Eq. \eqref{eq:alpha_invt1}.
%%
%In initial stages, when the inverse-time schedule
%is still inapplicable,
%we can instead use the WL style stage-wise strategy
%to control the updating magnitude
%for the bandpass scheme,
%since it is a straightforward extension of the single-bin scheme.
%%
%However, the histogram fluctuation used for stage transitions
%should be computed from only the first $K$ modes,
%where the histogram flattening was performed

We summarize the method as the following procedure.

\begin{enumerate}

\item
  Set up an array of umbrellas covering the desired range,
    e.g. we may set
    $z_c^{(i)} = z_{\min} + i \Delta z$
    with the same width $\sigma_z$.
  %
  For every $i$,
    set $c_1^{(i)}$ to the estimated mean force,
    and set $\ln Z^{(i)} = c_2^{(i)} = 0$.

\item
  Equilibrate the system at umbrella $i = 1$.
  Repeat the following steps until the simulation ends.

\item
  Conduct a regular MC or MD step
  in the current umbrella $i$ under bias potential $u^{(i)}(z, t)$.
  Update $c_1^{(i)}$ and $c_2^{(i)}$ according to Eq. \eqref{eq:ckupdate},
  and compute the fluctuations
  $\tilde F_1^{(i)}$ and $\tilde F_2^{(i)}$
  defined in Eq. \eqref{eq:Fk_def}.

\item
  Attempt a transition to another umbrella
  with the acceptance probability given by Eq. \eqref{eq:transprob_st}.
  Update $\ln Z^{(i)}$ at the current umbrella
  by Eq. \eqref{eq:wl_update}.

\item
  Change the updating magnitude,
  which will be used for both intra-umbrella
  parameters, $c_k^{(i)}$,
  and inter-umbrella parameters, $\ln Z^{(i)}$.
  %


\end{enumerate}

After simulation,
the weighted histogram analysis method
(WHAM)\cite{ferrenberg1988, *ferrenberg1989,
*kumar1992, *roux1995, *bartels1997, *souaille2001,
*kobrak2003, *gallicchio2005, *chodera2007,
*fenwick2008, *kim2011, *shirts2008,
zhu2012, newman, frenkel},
can be used to recover the unbiased distribution.
%




\section{\label{sec:results}
Numerical results}



\subsection{\label{sec:lj}
Comparison of updating schemes}


\subsubsection{System and simulation details}

Our first test system is
a Lennard-Jones (LJ) fluid of $N = 100$ particles,
and we wish to compute the PMF along
the distance, $r$, between two special particles.
%
We will assume reduced units in the calculation.
%
The two special particles, labeled as $1$ and $2$,
are mutually non-interacting
but they interact normally with other particles
via the pair potential $\varphi(r) = 4(r^{-12} - r^{-6})$.
%
The intrinsic distribution of distance, $p(r)$, is
\begin{align*}
  p(r) = \langle \delta(\mathbf r_{12} - \mathbf r) \rangle
  =
  \frac{V}{Z}
  \int e^{-\beta \Phi(\mathbf r^N)}
  d\mathbf r_3 \cdots d\mathbf r_N
  ,
\end{align*}
where
$V$ is the volume,
$\Phi(\mathbf r^{N}) = \sum_{j=3}^N\sum_{i=1}^{j-1} \varphi(r_{ij})$,
and
$Z = \int e^{\beta \Phi(\mathbf r^N)} d\mathbf r_1 \cdots d\mathbf r_N$.
%
This distribution is related the cavity distribution function
as $y(r) = \left(1 - \frac{1}{N}\right) \, V \, p(r)$\cite{hansen}.
%
In biased sampling, we fixed the two special particles on the $x$-axis,
and varied their distance $r$ along the axis.
%
Further, we added a bias potential, $k_B T \, u(r)$,
to sample a flat distribution along $r$.
%
The reference bias potential was pre-calculated from a long simulation.

We used the Metropolis algorithm\cite{frenkel, metropolis1953}
for configuration sampling.
%
Each step consists of
one MC trial for displacing one of the two special particles,
and another four trials for displacing the other particles.
%
In each trial,
a random particle is displaced in each permissible dimension
according to a uniform distribution over $(-\varepsilon, \varepsilon)$.
We used $\varepsilon = 0.1/\varrho$
to ensure reasonable acceptance ratio,
and $\varrho = N/V$ is the reduced number density.


The PMF was calculated from $r = 0$ to half of the box size
(also the cutoff distance of the potential)
on a grid of size $\delta r = 0.01$ in reduced units.
%
Two densities were tried, $\varrho = 0.1$ and $0.8$,
and the numbers of bins were $n = 500$ and $250$, respectively.
%
The reduced temperature was $T = 3$.
%
We tested the single-bin, Gaussian,
and homogeneous bandpass updating schemes.
%
Each simulation started with an ``equilibration'' phase
of $10^7$ steps under a constant updating magnitude,
$a_0 = 10^{-4}$,\footnote{The
  seemingly small updating magnitude
  corresponds to a larger value of $\ln f = n \, a_0$
  in the WL algorithm.
  The $\ln f$ was $0.05$ for $\varrho = 0.1$,
  or $0.025$ for $\varrho = 0.8$.}
followed by a production phase of another $10^7$ steps
with updating magnitude given by
either the inverse-time or optimal schedule.
%
The deviation of the bias potential was calculated from
Eqs. \eqref{eq:v_def} and \eqref{eq:x_def},
and decomposed to eigenmodes by
Eqs. \eqref{eq:gft_def} and \eqref{eq:wband_eigenvector_refl}.
%
The error was computed from Eq. \eqref{eq:error_def}.
%
We repeated each simulation multiple times
and report the average.
%
To make sure the same schedule for multiple runs under the same condition,
the optimal schedule was pre-computed from a set of fixed
$\Gamma_k$'s and $\epsilon_k$'s,
although ideally they could be estimated from the data
collected from the equilibration phase in individual runs.


\subsubsection{Results}

As shown in Table \ref{tab:lj_error},
all updating schemes
with either the inverse-time or optimal schedule
were able to reduced
the error significantly,
and the errors roughly matched the predicted values
calculated using the pre-computed
$\Gamma_k$'s and $\epsilon_k$'s.
%
The Gaussian updating scheme produced less error than
the single-bin scheme in the $\varrho = 0.1$ case,
but not in the $\varrho = 0.8$ case.
%
The effectiveness of the Gaussian updating scheme
in the low-density case
was probably because the sampling along
the collective variable was free from major barriers
and thus the histogram fluctuation largely
came from local noise.
%
%The optimal schedule also appeared to be more effective
%in reducing the error than the inverse-time schedule.
%%
In this case, we could further reduced the error
by using the bandpass updating scheme
(with the mode cutoff $K = 20$).
%
On the other hand, in the high-density case,
the error was dominated by the $k=1$ eigenmode
corresponding to the longest range fluctuation,
as $\Gamma_1 \approx \Gamma$,
and the reduction of local noise
had little effect on the total error.


\begin{table}[h]\footnotesize
  \caption{\label{tab:lj_error}
    Error of the bias potential under
    the single-bin,
    Gaussian (labeled by the width, $\sigma$),
    and bandpass (labeled by the mode cutoff, $K$)
    updating schemes
    on the LJ system.
    %
    The numbers have been averaged over multiple runs,
    and the predicted values are shown in parentheses for comparison;
    $\Err_\mathrm{init.}$
    denotes the error
    at the end of the equilibration phase under a constant magnitude;
    $\Err_\mathrm{final}$
    denotes the error at the end of the production phase under
    either the inverse-time ($1/t$) or optimal schedule (opt.).
    %
    \note{The reference values can be found
    on the second line of \texttt{verr.log}.}%
  }
  \setlength{\tabcolsep}{2pt}
  \renewcommand\arraystretch{1.4}
  \begin{tabular} { l c c c c }
    \hline
    Scheme & Sched. &
    $\Err_\mathrm{init.}$ &
    $\Err_\mathrm{final}$
    \\
    \hline
    \multicolumn{4}{c}{
      $\varrho = 0.1$,
      $500$ bins,
      $\Gamma_1 \approx 111$,
      $\Gamma \approx 1.1\times10^3$.
    } \\
    \hline
    single-bin & $1/t$
    & $5.7(5.7)\times10^{-2}$
    & $9.2(11.4)\times10^{-5}$
    \\
    %\hline
    $\sigma=0.1$ & $1/t$
    & $7.5(8.9)\times10^{-3}$
    & $6.8(8.2)\times10^{-5}$
    \\
    %\hline
    $\sigma=0.1$ & opt.
    & $7.5(8.9)\times10^{-3}$
    & $3.3(3.9)\times10^{-5}$
    \\
    %\hline
    $\sigma=0.2$ & $1/t$
    & $6.6(7.8)\times10^{-3}$
    & $3.9(4.8)\times10^{-5}$
    \\
    %\hline
    $\sigma=0.2$ & opt.
    & $6.7(7.8)\times10^{-3}$
    & $2.4(2.9)\times10^{-5}$
    \\
    %\hline
    $\sigma=0.5$ & $1/t$
    & $5.4(6.6)\times10^{-3}$
    & $3.3(3.8)\times10^{-5}$
    \\
    %\hline
    $\sigma=0.5$ & opt.
    & $5.8(6.6)\times10^{-3}$
    & $2.7(3.0)\times10^{-5}$
    \\
    %\hline
    $K=20$ & $1/t$
    & $8.0(8.9)\times10^{-3}$
    & $1.6(1.8)\times10^{-5}$
    \\
    \hline
    \multicolumn{4}{c}{
      $\varrho = 0.8$,
      $250$ bins,
      $\Gamma_1 \approx 2.0\times10^4$,
      $\Gamma \approx 2.7\times10^4$
    } \\
    \hline
    single-bin & $1/t$
    & $1.4(1.4)$
    & $3.4(2.7)\times10^{-3}$
    \\
    %\hline
    $\sigma=0.1$ & $1/t$
    & $1.3(1.3)$
    & $3.0(2.8)\times10^{-3}$
    \\
    %\hline
    $\sigma=0.1$ & opt.
    & $1.2(1.3)$
    & $3.0(2.8)\times10^{-3}$
    \\
    %\hline
    $\sigma=0.2$ & $1/t$
    & $1.3(1.2)$
    & $3.5(3.2)\times10^{-3}$
    \\
    %\hline
    $\sigma=0.2$ & opt.
    & $1.2(1.2)$
    & $3.1(2.9)\times10^{-3}$
    \\
    %\hline
    $\sigma=0.5$ & $1/t$
    & $1.0(0.9)$
    & $9.9(8.0)\times10^{-3}$
    \\
    %\hline
    $\sigma=0.5$ & opt.
    & $1.0(0.9)$
    & $5.9(5.4)\times10^{-3}$
    \\
    %\hline
    $K=20$ & $1/t$
    & $1.4(1.3)$
    & $3.1(2.7)\times10^{-3}$
    \\
    \hline
  \end{tabular}
\end{table}

For the Gaussian updating scheme,
there was an optimal width for maximum effectiveness.
%
The optimal width was $\sigma = 0.37$ for $\varrho = 0.1$
or $0.07$ for $\varrho = 0.8$,
and it also depended on the simulation length
and other parameters.
%
Although widening the Gaussian can
effectively suppress random error
from local histogram fluctuation,
it also increased the risk of including
more systematic bias in middle-range modes,
which can be difficult to remove
even with the optimal schedule.
%
This is because the rate of error reduction
is proportional to the eigenvalue, $\lambda_k$
[cf. Eq. \eqref{eq:vt_diffeq_mbin}],
which decreases rapidly with the mode index $k$,
cf. Eq. \eqref{eq:lambda_Gaussian}.

To illustrate the mode dependence of the error,
we show in Fig. \ref{fig:lj_xerr}
the error components, $\langle \tilde v_k^2 \rangle$,
against the eigenmode index, $k$,
for the $\varrho = 0.1$ case
with width $\sigma = 0.2$.
%
For modes $k=10$ to $25$,
the final errors from the Gaussian updating scheme
were greater than those from the single-bin updating scheme,
although the initial values were less.
%
For the Gaussian updating scheme,
the inverse-time schedule tended to
reduce the updating magnitude too quickly,
leaving a hump on the error profile
for these middle-range eigenmodes.
%
The optimal schedule improved over the inverse-time schedule
and produced a flatter profile for the final error.
%
As a result, the optimal schedule usually produced
less error than the inverse-time schedule,
as shown in Table \ref{tab:lj_error}.
%
Although the gain of using the optimal schedule
was modest in the reported cases,
it would increase with the simulation length.
%
For example, for the $\varrho = 0.1$ case
with width $\sigma = 0.2$,
if we extend the simulation to $10^8$ steps,
the error for the optimal schedule was
$2.9\times10^{-6}$,
which was much smaller than that
for the inverse-time schedule
$1.8\times10^{-5}$.
\note{The predicted errors were
$3.7\times10^{-6}$ and $1.8\times10^{-5}$, respectively.}

One way to overcome the systematic bias
in the Gaussian updating scheme
is to use the histogram-based correction formula,
Eq. \eqref{eq:uhatav}.
%
As shown in Fig. \ref{fig:lj_xerr},
even for the simulation using the non-optimal $1/t$ schedule,
the correction was able to reproduce
the flatter profile seen in the single-bin case.
%
However, this correction also introduced
some local noise in the large $k$ modes,
and it would be best used as a post-simulation
analysis tool for simulations
with detectable residual bias.
%
For example, for the simulation in the $\varrho = 0.8$ case
with $\sigma = 0.5$ and the inverse-time schedule,
the correction was able to lower
the error from $9.8\times10^{-3}$
(Table \ref{tab:lj_error}) to $3.3\times10^{-3}$.


\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/lj_xerr.pdf}
  }
  \caption{
    \label{fig:lj_xerr}
    Error components from FDS simulations
    using the Gaussian updating scheme
    of width $\sigma = 0.2$
    on the LJ system at density $\varrho = 0.1$.
    %
    The optimal schedule (opt.) was more effective
    in reducing the errors of middle-range eigenmodes
    than the inverse-time schedule ($1/t$).
    The ``corrected'' values were obtained from
    Eq. \eqref{eq:uhatt}.
    The lines are a guide to the eyes.
  }
\end{figure}

In Fig. \ref{fig:lj_alpha},
we show the optimal schedules in a few cases.
%
%Generally, the optimal schedule
%for the $\varrho = 0.8$ case
%was closer to the inverse-time schedule
%than that for the $\varrho = 0.1$ case.
%%
%This was probably because in the former case
%the first mode dominates the total error,
%$\Gamma_1 \approx \Gamma$,
%and the schedule was roughly optimized for this mode,
%resulting in
%$\alpha(t) \approx 1/[\lambda_1 (t + t_0)] \approx 1/(t+t_0)$
%as $\lambda_1 \approx 1$
%[cf. Eq. \eqref{eq:lambda_Gaussian}].
%
Since we have chosen a long equilibration period
for the preliminary run under the constant updating magnitude, $a_0$,
the initial magnitude was around
the ideal value of $a_0/2 = 5\times10^{-5}$
given by Eq. \eqref{eq:half_alpha0}.
%
But if the Gaussian were too wide,
e.g. if $\sigma = 0.6$,
the eigenvalues $\lambda_k$ of some modes
became too small to damp out the initial bias,
resulting in a much larger initial updating magnitude, $\alpha(0)$
[cf. Eq. \eqref{eq:opt_alpha0}].
%
In this case, the predicted schedule
would be unusable because the resulting deviation in the bias potential
was no longer small,
and one may have to either decrease the width
or to elongate the preliminary run.




\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/lj_alpha.pdf}
  }
  \caption{
    \label{fig:lj_alpha}
    Optimal schedules of Gaussian updating schemes
    for the LJ system.
    %
    The starting updating magnitude was usually
    around $a_0/2 = 5\times10^{-5}$
    as given by Eq. \eqref{eq:half_alpha0}.
    But an overly wide window ($\sigma = 0.6$)
    led to a much larger updating magnitude.
  }
\end{figure}




\subsection{\label{sec:potts}
Umbrella sampling with adaptive Gaussian ensembles}


As another proof of principle,
we tested the adaptive umbrella sampling method
introduced in Sec. \ref{sec:aus}
on the two-dimensional $L\times L$ ten-state
Potts model\cite{wu1982, newman, wang2001, wang2001pre}
in the energy space.
%
In this case, $\Phi(\mathbf x) = 0$
and the intrinsic distribution is
the density of states, $g(E)$.
%
We spread the Gaussian umbrellas
of the same width $\sigma_E = L$ over $[-1.8L^2, -0.8L^2]$
with an even spacing of $L$.
%
For simplicity,
only transitions between neighboring umbrellas
were performed.
%
Initially, we set
$c_1 = \tilde \beta_c \, \sigma_E$
for all umbrellas,
with $\tilde \beta_c = 1.4$
being the approximate inverse critical temperature.
%
For configuration sampling in each umbrella,
we used the Wolff algorithm\cite{wolff1989, newman}
(which is more efficient than the Metropolis algorithm)
at $\beta = c_1/\sigma_E$
followed by a Metropolis step with
acceptance probability
$\min\bigl\{1, e^{c_2 [R_2(E) - R_2(E')]} \bigr\}$,
where $E$ and $E'$ are the energies of the old and new states.
%
The acceptance ratio was no less than 92\% in any umbrella.
%
In the initial WL stages,
the initial updating magnitude was $0.01$;
the fluctuation threshold for stage transition was $0.2$;
and the reduction factor was $1/2$.

The results for the $L = 64$ case
after $10^5 L^2$ steps are shown
in Fig. \ref{fig:pt_hist}.
%
The Gaussian histograms were evenly spaced with similar width as intended,
%and the overall energy histogram was roughly flat,
as shown in Fig. \ref{fig:pt_hist}(a).
%
We computed the density of states by WHAM,
and found the critical temperature to be $T_c = 0.70162$,
in agreement with a previous study\cite{wang2001pre}.
%
As discussed in Sec. \ref{sec:aus},
$c_1/\sigma_E$ roughly represents
the local or statistical temperature, $\beta(E) = d\ln g(E)/dE$,
and it demonstrated
the backbending behavior\cite{kim2006, *kim2007, kim2010}
typical in a first-order phase transition,
as shown in Fig. \ref{fig:pt_hist}(b).
%
The coefficient, $\sqrt 2 \, c_2$,
was indeed above $1.0$
in the critical region\cite{kim2010},
as shown in Fig. \ref{fig:pt_hist}(c).


\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/pt_hist.pdf}
  }
  \caption{
    \label{fig:pt_hist}
    Umbrella sampling with adaptive quadratic bias potentials
    on the $64 \times 64$ ten-state Potts model.
    %
    (a) Normalized histograms (solid lines)
    and the reweighted distribution
    at the critical temperature (dashed line).
    %
    (b) The parameter, $c_1/\sigma_E$ (squares), roughly corresponding to
    the statistical temperature, $\beta(E)$,
    (dashed line, estimated from the smoothed density of states)
    demonstrated the backbending behavior.
    %
    (c) The parameter, $\sqrt 2 \, c_2$ (squares),
    exceeded $1.0$
    in the critical region, indicating local convexity
    of the density of states.
  }
\end{figure}



\section{\label{sec:conclusion}
Conclusions and Discussions}



In conclusion,
we have proposed a method of computing
the optimal schedule of the updating magnitude
for a general class of free energy calculation methods,
which we refer to as adaptive flat-distribution-sampling (FDS) methods
that include the Wang-Landau (WL) algorithm and metadynamics.
%
Adaptive FDS methods
incrementally update a bias potential
along a collective variable
to offset the potential of mean force (PMF)
and thereby sample a flat distribution.
%
The optimal schedule delivers the fastest convergence
of the bias potential,
and thus helps minimize the effort
of computing the PMF.
%They differ by the updating schemes,
%which specify how the bias potential is modified
%in each updating step.
%%
%In the single-bin or WL case,
%the update is limited to the current bin;
%while metadynamics,
%the bias potential of a neighborhood of the current one
%is updated with relative weight specified by a Gaussian window.


The optimal schedule depends on the updating window function,
which can be limited to a single-bin (as often in the WL case),
or extended to several neighboring bins
taking a Gaussian shape (as often in the metadynamics case).
%
We can generally characterize
an updating scheme by an updating matrix,
with each column being the normalized
window function for the bin of the column.
%
Our method computes the optimal schedule from
the eigenvalues of the updating matrix,
which represent the relative rates of error reduction,
and the integrals of the autocorrelation functions,
which represent the intrinsic histogram fluctuation
of the eigenmodes.
%
For the single-bin scheme with a set of uniform eigenvalues,
we confirmed that the optimal schedule
is given by the inverse-time formula,
Eq. \eqref{eq:alpha_invt1}.
%
For a general multiple-bin scheme,
including the Gaussian one,
the optimal schedule, having to balance
the different rates of error reduction of the eigenmodes,
is usually not given by a simple closed form,
but is implicitly given by an equation of motion
of a free particle with a position-dependent mass,
Eq. \eqref{eq:q_opt}.
%
In this case,
the optimal schedule and error
can be sensitive to the simulation length
and other parameters.

In particular, the Gaussian updating scheme is often
equipped with a set of rapidly diminishing eigenvalues.
%
While this feature helps suppress
the accumulation of random error and
maintain the smoothness of the bias potential,
it also slows down the reduction
of the initial bias, if any,
in the short- and middle-range eigenmodes.
%
Thus, one needs to carefully choose the width of the Gaussian
and the optimal schedule of decreasing the updating magnitude,
which sometimes needs to be fixed at a maximal acceptable level
for a long time before any reduction can happen.
%
The possible efficiency loss in
error reduction of the long- and middle-range modes
can often be compensated
by the histogram-based correction formula, Eq. \eqref{eq:uhatav},
for post-simulation analysis,
which ideally would remove the remaining bias
and recover a bias potential with precision
similar to that from the single-bin simulation.


In comparing different updating schemes,
we found that
the single-bin updating scheme was optimal
in error reduction
in the long time limit
without a priori assumptions
on the smoothness of the PMF.
%
This scheme can be generalized to
a class of bandpass updating schemes,
which are also asymptotically optimal,
but completely filter the noise modes.
%
The optimal schedule of the bandpass schemes
is always given by an inverse-time formula.
%
If the bandpass updating scheme was constructed
from orthogonal polynomials,
it became a generalization
of the LLR algorithm\cite{langfeld2012, pellegrini2014},
and could be used for parameter adjustment
in umbrella sampling with
quadratic\cite{neuhaus2006, *neuhaus2007, zhu2012}
or similar\cite{kim2010}
bias potentials.



This study has several limitations.
%
First, we have ignored the \emph{systematic}
error\cite{zhou2005, morozov2007, zhou2008}
caused by the updates in adaptive FDS methods.
%
This simplification is justified in the asymptotic regime:
when the updating magnitude is small
and the sampling is a quasi-equilibrium one\cite{
  zhou2005, morozov2007, zhou2008, barducci2008, dama2014},
the random error,
which is roughly proportional to
the square root of the updating magnitude\cite{
  zhou2005, morozov2007, zhou2008, bussi2006},
can easily outweigh
the systematic error,
which is roughly proportional to
the magnitude itself\cite{morozov2007}.
%
%Our method of analysis is most useful for long simulations
%to achieve relatively precise results.
%
Second, by using the white noise approximation,
we have represented the autocorrelation function
of each eigenmode by a single number, $\Gamma_k$.
%
Although this assumption appeared to work well
in our test systems modeling long simulations,
it may break down for relatively short simulations
of glassy systems.
%
Finally, the adaptive umbrella sampling method introduced in Sec. \ref{sec:aus}
approach requires refinement to improve
its general applicability.
%
For example, we find that with slow sampling processes,
the updating scheme might occasionally overly update
the parameter, $c_2$, leading to unreasonable values,
and the parameters used here might not be optimal.
%Our immediate interest is to apply these relations to
%problems containing aqueous mixtures of proteins and nucleic acids.


\section{Acknowledgments}

We thank Dr. Y. Mei, J. Wang,
O. Nassar, Dr. C. Lai, Dr. S. Ou, D. Stuhlsatz,
Dr. C. Myers, and Dr. O. Samoylova
for helpful discussions,
and the two anonymous reviewers for critical reading
of earlier versions of the manuscript.
%
We gratefully acknowledge the Robert A. Welch Foundation (H-0037),
the National Science Foundation (CHE-1152876)
and the National Institutes of Health (GM-037657)
for partial support of this work.
%
JM thanks support from the National Institutes of Health
(R01-GM067801, R01-GM116280),
the Welch Foundation (Q-1512),
and National Science Foundation (Grant PHY-1427654).
%
This research used computing resources of
the National Science Foundation XSEDE grid.
%


\appendix




\section{\label{sec:equilerr}
Runtime average interpretation
of the inverse-time schedule
}



The optimality of the inverse-time schedule
for the single-bin updating scheme
can be understood from the observation that
the schedule allows the bias potential
to coincide with a runtime average
of independent corrections.

Assuming that we can linearize the histogram correction
in Eq. \eqref{eq:vcorr_equil} as
$\ln x \approx x - 1$ % (for $x \approx 1$),
and apply it to the instantaneous histogram, $h_i(t)$,
the independent estimate of bias potential in each step
would be given by
%
\begin{equation}
  \hat u_i(t) \approx u_i(t) + \frac{ h_i(t) } { \rho_i } - 1
  = u_i(t) + f_i(t) - 1
  .
  \label{eq:uhatt}
\end{equation}
%
Below we will use capital letters to denote
time averages, e.g. for variable $x$, we define
$X(t) \equiv \frac{1}{t} \sum_{\tau=1}^t x(\tau)$.
%\begin{equation}
%  X(t) \equiv \frac{1}{t} \sum_{\tau=1}^t x(\tau)
%  .
%  \notag
%  %\label{eq:timeav}
%\end{equation}
%
Time averaging Eq. \eqref{eq:uhatt} yields
\begin{align}
  \hat U_i(t)
  &\approx
  U_i (t)
  + F_i(t) - 1
  \approx
  U_i(t)
  + \ln \frac{ H_i(t) } { \rho_i }
  .
  \label{eq:uhatav}
\end{align}
%
%Equation \eqref{eq:uhatav}
This equation serves as
a generalization of Eq. \eqref{eq:vcorr_equil}
for FDS simulations under a variable bias potential
and we can use it to improve the estimated PMF
after simulation.\footnote{Equation \eqref{eq:uhatav}
  can also be derived
  from the self-healing umbrella sampling method\cite{marsili2006}.
  From Eq. \eqref{eq:uhatt},
  we can construct an independent estimator
  of the unbiased distribution as
  $\hat p_i(t) \approx h_i(t) \, e^{ u_i(t) }$.
  Upon time averaging, we get
  $\rho_i e^{\hat U_i(t)} = \hat P_i(t) \approx H_i \, e^{ U_i(t) }$.}
%
In particular, for the single-bin updating scheme,
Eq. \eqref{eq:wl_update},
we have, up to a constant shift,
%
\begin{align}
  \hat U_i(t)
  &\approx
  \frac{1}{t} \sum_{\tau=1}^t
  \left[
    u_i(\tau) +
    \frac{ u_i(\tau + 1) - u_i(\tau) } { \alpha(t) }
  \right]
  ,
  \notag
  \\
  &\approx
  \Delta u_i
  +
  \sum_{\tau=1}^t
  \left[
    1 + \frac{1}{\alpha(\tau - 1)} - \frac{1}{\alpha(\tau)}
  \right]
  \frac{ u_i(\tau) } {t}
  ,
  \label{eq:Uhat_sum}
\end{align}
where
$\Delta u_i \equiv u_i(t+1) /[t \, \alpha(t)]
- u_i(1)/[t \, \alpha(0)]$.
%$$
%\Delta u_i \equiv \frac{u_i(t+1)}{t \, \alpha(t)}
%- \frac{u_i(1)}{t \, \alpha(0)}.
%$$
%
With the inverse-time schedule, Eq. \eqref{eq:alpha_invt},
each term in the sum vanishes,
and if we further define $\alpha(0) \to \infty$,
then
$$
u_i(t+1) = \hat U_i(t),
$$
i.e. the bias potential in this case coincides with
the runtime average of the correction, $\hat u_i(t)$,
at every step, $t$.
%
On the other hand,
with a constant $\alpha(\tau) = a_0$ for $\tau = 0, 1, \dots$,
Eq. \eqref{eq:Uhat_sum} is approximately a plain average\cite{zhou2005}:
$\hat U_i(t) \approx U_i(t)$,
if we can assume an equilibrium condition
with $u_i(t+1) \approx u_i(1)$.
\note{This would be rather risky for metadynamics,
because we know that the certain noise modes
do not easily go away, although the plain average
formula was also recommended for metadynamics.}

We can also cast other updating relations,
such as Eqs. \eqref{eq:wl_update}
and \eqref{eq:vkupdate},
as runtime averages,
but with dynamic weights.
%
The weighted average
%
\begin{align*}
  X_\omega(t)
  =
  \frac{
    \omega(0) \, x(0) + \cdots + \omega(t) \, x(t)
  }
  {
    \Omega(t)
  }
  ,
\end{align*}
%
where
$\Omega(t) \equiv \omega(0) + \cdots + \omega(t)$
is the accumulated weight,
is equivalent to the recurrence relation
%
\begin{align*}
X_\omega(t) = X_\omega(t-1)
  + \frac{\omega(t)}{\Omega(t)}
  \Delta \, x(t)
,
\end{align*}
where
$\Delta x(t) \equiv x(t) - X_\omega(t-1)$
is the proposed change from the previous average
from the data point at step $t$.
%
The relative weight, $\omega(t)/\Omega(t)$,
can be identified as the updating magnitude,
\begin{equation}
  \alpha^*(t) \equiv
  \frac{ \omega(t) } {\Omega(t)}
  =
  1 - \frac{ \Omega(t-1) } {\Omega(t)}
  .
  \label{eq:alpha_from_Omega}
\end{equation}
%
The inverse-time schedule, Eq. \eqref{eq:alpha_invt1},
follows from the case of unweighted average,
$\omega(t) = 1$ (for $t > 0$),
and updating magnitudes above and below this value
correspond to runtime averages that favor future and past data,
respectively.
%
In the case of Eq. \eqref{eq:vkupdate},
we can identify
the bias potential $\tilde v_k(t)$
as $X(t-1)$,
$\tilde f_k(t)$ as $\Delta x(t)$
by Eq. \eqref{eq:uhatt},
and
the mode-$k$ updating magnitude,
$\alpha(t) \, \lambda_k$,
as $\alpha^*(t)$.
%
This mapping shows that
updating magnitude should not exceed $1.0$.
%
One way to satisfy this condition
in converting the results
from the continuous-time setup is
to use Eq. \eqref{eq:alpha_from_Omega}
with
\begin{align*}
  \frac{ \Omega(t) } { \omega(0) }
  = \prod_{\tau=1}^t \frac{1}{1- \alpha^*(\tau)}
\approx \exp\left(\int_0^t \alpha^*(\tau) \, d\tau \right).
\end{align*}








\section{\label{sec:schedule_geometry}
Characterization of optimal schedules}



We can characterize the optimal schedule
geometrically as follows.
%
First, we define
a normalized mass distribution as
%
\begin{equation}
  m(\bar q)
  =
  \frac{
    M(\bar q)
  }
  {
    \int_0^{ q(T) } M(q') \, d q'
  }
  =
  \frac{
    M(\bar q)
  }
  {
    C_M
  }
  ,
\notag
%\label{eq:mass_distr}
\end{equation}
%
such that
$\int_0^{q(T)} m(\bar q) \, d\bar q = 1$.
%
Then from Eqs. \eqref{eq:Lagrangian_const} and \eqref{eq:q_opt},
we find that for $\bar q(t) \equiv q(T) - q(t)$,
%
\begin{align}
  m\left( \bar q(t) \right)
  &=
  \frac{ 1 }
       { T \, \alpha(t) }
  ,
  \label{eq:mQ_invTa}
  \\
  \int_{\bar q(t)}^{ q(T) }
    m(q') \, d q'
  &=
  \frac t T
  ,
  \label{eq:intmQ_tT}
\end{align}
%
Thus, each point on the schedule,
$\bigl(t, \alpha(t)\bigr)$,
can be mapped to a point,
$\Bigl(\bar q(t), m\bigl(\bar q(t)\bigr)\Bigr)$,
on the mass distribution,
such that the ordinate of the latter curve
is $1/(T\alpha)$,
and the area under the curve in the domain, $[\bar q(t), q(T)]$,
is equal to $t/T$,
as shown in Fig. \ref{fig:massq}(a).

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=\linewidth]{fig/massq.pdf}
  }
  \caption{
    \label{fig:massq}
    (a) Geometric construction of the optimal schedule,
    $\alpha(t)$,
    from the normalized mass distribution,
    $m(\bar q)$.
    %
    (b) Inverse construction of $m(\bar q)$
    from the optimal schedule $\alpha(t)$.
  }
\end{figure}

The schedule, $\alpha(t)$,
also defines a variable transformation from
the mass distribution, $m(\bar q)$.
%much like the Legendre transform.
%
If we define
$\bar q^* \equiv \bar q/q(T)$
and $t^* \equiv t/T$
such that both are limited to the interval $[0, 1]$,
as well as
\begin{align*}
  m^*       &\equiv q(T) \, m(\bar q) = -\frac{ dt^* } { d\bar q^* },
  \\
  \alpha^*  &\equiv \frac{T}{q(T)} \, \alpha(t) = -\frac{ d\bar q^* } { dt^* },
\end{align*}
then it is clear that
both $m^*(\bar q^*)$ and $\alpha^*(t^*)$
can describe the differential relation
between $\bar q^*$ and $t^*$,
but with the natural variables
being $\bar q^*$ and $t^*$, respectively.
%
Further, the transformation is an involution:
if we apply the same transform on $\alpha^*(t^*)$
we would recover $m^*(\bar q^*)$,
as shown in Fig. \ref{fig:massq}(b).
%
A few examples are shown in Table \ref{tab:m_and_a}.
%
For example, in the single-bin scheme,
the mass distribution decays exponentially,
and we would have the inverse-time schedule.
%
On the other hand,
if the mass distribution is inversely proportional to $\bar q$,
which is approximately true for the Gaussian updating scheme
at large $\bar q$,\footnote{If
  the magnitudes of the eigenvalues, $\lambda_k$'s,
  span a wide range, then for a large $\bar q$,
  the sum in Eq. \eqref{eq:mass_func} will be dominated by
  the largest term, where $\lambda_k \approx 1/\bar q$,
  and $M(\bar q) \approx \sqrt{\Gamma_k}/\bar q$.
  }
  %
then we would have an exponentially decaying schedule.

\begin{table}[h]
  \caption{\label{tab:m_and_a}
    Examples of the variable transformation between the
    reduced schedule, $\alpha^*(t^*)$,
    and the reduced mass distribution, $m^*(\bar q^*)$,
    as defined in Eqs. \eqref{eq:mQ_invTa}
    and \eqref{eq:intmQ_tT}.
  }
  \setlength{\tabcolsep}{2pt} % spacing between columns
  \renewcommand\arraystretch{2.0} % spacing between rows
  \begin{tabular} { c c c }
    \hline
    $m^*(\bar q^*)$ &
    $\alpha^*(t^*)$ &
    $\bar q^*(t^*)$
    \\
    \hline
    $(1+ s_\gamma) \, \gamma \, e^{-\gamma \, {\bar q}^*}$ &
    $\dfrac{1}{ \gamma \, (t^* + s_\gamma) }$ &
    $\dfrac{1}{\gamma} \ln\left(\dfrac{1 + s_\gamma}{t^* + s_\gamma} \right)$
    \\
    $\dfrac{1}{\gamma \, ({\bar q}^* + s_\gamma) }$ &
    $(1 + s_\gamma) \, \gamma \, e^{-\gamma \, t^*}$ &
    $(1 + s_\gamma) \, e^{-\gamma \, t^*} - s_\gamma$
    \\
    $\dfrac{c \, a}{\left( {\bar q}^* + q^*_0 \right)^{a+1}}$ &
    $\dfrac{c^{\frac 1 a} / a}{\left( t^* + t^*_0 \right)^{\frac{1}{a}+1}}$ &
    %$\left( \dfrac{c}{t^* + t^*_0} \right)^{1/a} - q^*_0$
    $q^*_0 \left[\left( \dfrac{1+t^*_0}{t^* + t^*_0} \right)^\frac{1}{a} - 1\right]$
    %$\,\begin{aligned}
    %  &a, q^*_0 > 0, \\
    %  &\frac{1}{t^*_0} \equiv \left(1+\frac{1}{q^*_0}\right)^a - 1, \\
    %  &c \equiv {q^*_0}^a(t^*_0 + 1)
    %\end{aligned}$
    \\
    \hline
    \multicolumn{3}{p{8cm}}{
    Here,
    $\gamma > 0$,
    and we have defined
    $s_\gamma \equiv 1/(e^\gamma - 1)$,
    ${t^*_0}^{-1} \equiv \left(1+{q^*_0}^{-1}\right)^a - 1$,
    and
    $c \equiv {q^*_0}^a(1 + t^*_0)$.
    } \\
    \hline
  \end{tabular}
\end{table}




\section{\label{sec:Gamma_measure}
Integrals of the autocorrelation functions of the eigenmodes
}



Here we give a method of measuring
the integrals of the autocorrelation functions, $\Gamma_k$'s,
in an adaptive FDS simulation
under a constant updating magnitude.
%
This method has the advantage of not requiring
explicit computation of
the autocorrelation functions of the eigenmodes.

The idea is to use a modification
of Eq. \eqref{eq:xt2_eql}
that avoids the problem of not
knowing the intrinsic distribution, $p_i$,
hence the potential shift,
$u_i(t) - v_i(t) = \ln(p_i/\rho_i)$.
%
Since the shift,
hence its Fourier transform,
$\tilde u_k(t) - \tilde v_k(t)$,
is a constant of time,
${\tilde u}_{k}$ and ${\tilde v}_{k}$
should share the same variance.
%
But as the ensemble average of ${\tilde v}_{k}$
is zero, we have
$\operatorname{var} {\tilde u}_{k} =
 \operatorname{var} {\tilde v}_{k} =
 \left\langle {\tilde v}_{k}^2 \right\rangle$,
%\begin{equation*}
%  \operatorname{var} {\tilde u}_{k}
%  =
%  \operatorname{var} {\tilde v}_{k}
%  =
%  \left\langle {\tilde v}_{k}^2 \right\rangle
%  .
%\end{equation*}
%
and by using Eq. \eqref{eq:xt2_eql},
we get
%
\begin{equation}
  \operatorname{var} {\tilde u}_k
  =
  \frac{1}{2} \,
  a_0 \, \Gamma_k \, \lambda_k,
\label{eq:varXt}
\end{equation}
%
where the variance can be computed from trajectory averages.
%
In practice, Eq. \eqref{eq:varXt}
would fail if some eigenvalue, $\lambda_k$, is close to zero.
So it is best used for the single-bin updating scheme.

We can alternatively estimate the magnitude of $\Gamma_k$
by the transition matrix method.
%
If the propagation along the collective-variable is Markovian,
then in the long time limit, the evolution can be approximately
described by a transition matrix, $\mathbf A$, with
\begin{align*}
\langle h_i(t) \rangle = \rho_i,
\quad
\langle h_i(t) \, h_j(0) \rangle = \bigl(\mathbf A^t\bigr)_{ij} \, \rho_j
\quad \mbox{(for $t \ge 0$)}
.
\end{align*}
Further if the transition matrix satisfies detailed balance
with a set of eigenvectors, $\psi_{li}$,
that are orthonormal with respect to $\pmb\rho$
[cf. Eqs. \eqref{eq:eig_orthonormal_cols} and
\eqref{eq:eig_orthonormal_rows}]
then we have the decomposition
$A_{ij} = \sum_{l=0}^{n-1} \Lambda_l \, \psi_{li} \, \psi_{lj} / \rho_j$,
with $\Lambda_l$ being the eigenvalue of mode $l$.
%
The autocorrelation function can then be computed as
%
\begin{align*}
\bigl\langle
  \tilde f_k(t) \, \tilde f_k(0)
\bigr\rangle
=
\sum_{l = 0}^{n-1} \Lambda_l^t \,
\left(
  \sum_{i=1}^n \frac{ \phi_{ki} \, \psi_{li} }{ \rho_i }
\right)^2
.
\end{align*}
%
In particular, if the eigenvectors, $\pmb \phi_k$
and $\pmb \psi_k$, coincide,
we would have
$\bigl\langle
  \tilde f_k(t) \, \tilde f_k(0)
\bigr\rangle
=
\Lambda_k^t,$
and
\begin{equation}
  \Gamma_k = 1 + 2 \, \sum_{t=1}^\infty
  \bigl\langle
    \tilde f_k(t) \, \tilde f_k(0)
  \bigr\rangle
  = \frac{1+\Lambda_k}{1-\Lambda_k}
  .
  \notag
  %\label{eq:Gamma_est}
\end{equation}

The above transition matrix method
tends to underestimate
the first few $\Gamma_k$'s.
%
We will nonetheless use it as a model to
study the behavior of $\Gamma_k$
in a local sampling process.
%
If we borrow Eq. \eqref{eq:lambda_Gaussian}
for $\Lambda_k$,
with $\sigma_z$ interpreted as
the average move size along $z$ during sampling,
then
\begin{align}
\Gamma_k = \coth\left[
  \left(
    \frac{ \pi \, k \, \sigma_z^\mathrm{(samp)} } { g \, \Delta z}
  \right)^2
  \right].
  \label{eq:Gamma_local}
\end{align}
For small $k$,
we have $\Gamma_k \propto \Delta z^2/k^2$,\cite{bussi2006}
which means that doubling the sampling range, $\Delta z$,
would make the histogram flattening
at least four times as difficult,
i.e. FDS methods are most effective
for short ranges\cite{wang2001, wang2001pre}.
%
%Using finer bins would, however, not change this value.
%
For a modestly large $k$,
$\Gamma_k$ quickly approaches $1$.
But since $\langle \tilde v_k^2 \rangle \propto \Gamma_k$,
according to Eq. \eqref{eq:xt2_eql},
the error profile for the single-bin updating scheme
would have a roughly flat tail,
as shown in Fig. \ref{fig:lj_xerr}.




\section{\label{sec:homo}
Homogeneous updating schemes
}


Here, we give some mathematical details
on the homogeneous updating schemes,
%
The updating matrix of the updating schemes, $w_{ij}$,
can be characterized by
a fixed window function, $\mu_0, \mu_1, \dots, \mu_b$,
that shifts with the bin $j$.
%
Here $\mu_l$ denotes the relative updating magnitude
(not adjusted by the boundary correction)
on the $l$th bin from the center.
%
The translational invariance requires $\rho_i$ to be flat,
and the updating matrix, $\mathbf w$,
must be symmetric to satisfy detailed balance,
Eq. \eqref{eq:w_detailedbalance}.
%
The updating matrices share the same set of eigenvectors
(which here are sines and cosines of different frequencies),
and thus are completely determined
by the eigenvalues.
%
Further, the multiplication of $\mathbf w$
can be reduced to a convolution with the window function.
%
We will show below that the eigenvalues of the updating matrix
is related to the window function as a cosine transform.
%
Thus, there is a one-to-one correspondence between
the eigenvalues of $\mathbf w$
and the window function.



\subsection{\label{sec:wband_eig}
Eigenvalues and window function}



%While translationally-invariant
%updating schemes
%naturally suit periodic variables\cite{
%dama2014},
%they can also be extended to non-periodic variables
%by imposing the reflective boundary condition\cite{
%bussi2006}.
%%
%For simplicity, we will assume that the target
%distribution is flat, or $\rho_i = 1/n$, below.



For a periodic variable\cite{dama2014},
the updating matrix, $w_{ij}$,
must be a function of $(i-j)\mod n$,
and we write it as
%
\begin{equation}
  w_{ij}
  =
  \mu_{i-j}
  +
  \mu_{i-j+n}
  +
  \mu_{i-j-n}
  ,
  \notag
  %\label{eq:w_band_pbc}
\end{equation}
%
for some numbers $\mu_{-b}, \dots, \mu_b$, ($b \le n/2$)
with $\mu_l = 0$ for $l > b$.
%
These numbers define a window function.
%
To satisfy Eq. \eqref{eq:w_sumj},
we impose the normalization
%
\begin{equation}
  \mu_{-b} + \cdots + \mu_b = 1
  .
\label{eq:mu_normalization}
\end{equation}
%
If $b = n/2$, $\mu_{-b}$ is removed
from the above sum in Eq. \eqref{eq:mu_normalization}
to avoid double counting.
%
For simplicity, we also impose the symmetry
%
\begin{equation}
  \mu_i = \mu_{-i}
  .
\label{eq:mu_symm}
\end{equation}


To find the eigenvectors,
we define for a periodic variable, $\phi_i$,
the out-of-boundary values by
%
\begin{equation}
  \phi_i = \phi_{i \pm n},
\label{eq:phi_pbc}
\end{equation}
%
such that $i \pm n$ lies in between $1$ and $n$.
%
Then, the multiplication of the matrix, $\mathbf w$,
is equivalent to a convolution with the window function:
%
\begin{equation}
  \sum_{ j = 1 }^n
    w_{ij} \, \phi_j
  =
  \sum_{ j = 1 - n }^{ 2 \, n }
    \mu_{i - j} \, \phi_j
  =
  \sum_{ l = -b }^{ b }
    \mu_l \, \phi_{ i - l}
  .
\label{eq:wmul_to_convol}
\end{equation}
%
The characteristic equation,
$\mathbf w \, \pmb\phi = \lambda \, \pmb\phi$,
can then be solved by discrete Fourier transform.
%
We may construct a set of orthonormal eigenvectors,
$\pmb\phi^{(1)}, \dots, \pmb\phi^{(n)}$,
compatible with the periodicity
required by Eq. \eqref{eq:phi_pbc},
%
\begin{equation}
  \phi^{(k)}_i
  =
  \phi_{ki}
  =
  \frac{ \sqrt{ 2 - \delta_{k,0} } } { n }
  %\overset{ \cos } { \sin }
  \,
  {\cos \atop \sin}
  \left(
    \frac{ k \, i \, 2 \, \pi }
         {      n             }
  \right)
  ,
  \notag
\end{equation}
%
where the function takes the cosine form for $k \le n/2$,
or the sine form otherwise.
%
Then one can readily verify that the eigenvalues are given by
\begin{equation}
  \lambda_k
  =
  \mu_0
  +
  2 \,
  \sum_{ l = 1 }^b
  \mu_l
  \cos\left(
  \frac{ k \, l \, 2 \, \pi }
       {      g \, n        }
  \right)
  ,
  \label{eq:wband_eigenvalue}
\end{equation}
%
with $g = 1$.
Note that there is a two-fold degeneracy,
%for the periodic variable,
%or $2$ for a non-periodic one
%There is a two-fold degeneracy,
  $\lambda_{n - k} = \lambda_k$.


For a non-periodic variable,
we use the reflective boundary condition\cite{bussi2006},
and change the form of the updating matrix to
%
%
\begin{equation}
  w_{ij}
  =
  \mu_{ i - j }
  +
  \mu_{ i + j - 1 }
  +
  \mu_{ i + j - 2 n - 1 }
  ,
  \notag
  \label{eq:w_band_refl}
\end{equation}
%
where the last two terms on the right hand side,
representing two reflective mirrors at
$i = 1/2$ and $i = n + 1/2$,
are added to avoid unintended distortion\cite{dickson2011, mcgovern2013}
to the equilibrium distribution\cite{bussi2006}.
%such that Eqs. \eqref{eq:w_sumj}-\eqref{eq:w_balance}
%are satisfied.
%
Again, the window function should satisfy
Eqs. \eqref{eq:mu_normalization}
and \eqref{eq:mu_symm},
but
the cutoff $b$ only needs to be less than $n$.

Note that the two reflective mirrors effectively
define a periodic variable of period $2 \, n$.
Thus, the eigenvalues can be borrowed from
Eq. \eqref{eq:wband_eigenvalue},
with a doubled period,
i.e. $g$ is changed to $2$ in the non-periodic case.

%For a non-periodic variable,
We define the out-of-boundary values as
%
\begin{equation}
  \phi_i
  =
  \begin{dcases}
    \phi_{ 1 - i }           & \mathrm{for \;} i \le 0, \\
    \phi_{ 2 \, n + 1 - i }  & \mathrm{for \;} i > n,
  \end{dcases}
\label{eq:phi_refl}
\end{equation}
%
such that the matrix multiplication is still equivalent to
a convolution as Eq. \eqref{eq:wmul_to_convol}.
%
Since Eq. \eqref{eq:phi_refl}
defines a periodic variable of period $2 \, n$
with a reflective symmetry around $i = 1/2$,
the eigenvectors,
$\pmb\phi^{(1)}, \dots, \pmb\phi^{(n)}$,
should share the same periodicity,
and be even functions about $i = 1/2$:
%
\begin{equation}
  \phi^{(k)}_i
  =
  \phi_{k i}
  =
  \frac{ \sqrt{ 2 - \delta_{k, 0} } }
       {             n              }
  \cos \left[
       \frac{ k \, \left( i - \frac 1 2 \right) \, 2 \, \pi}
            {             2 \, n                           }
       \right]
  .
  %\notag
  \label{eq:wband_eigenvector_refl}
\end{equation}


%\subsection{\label{sec:invert_wband}
%Inversion}

We can recover the window function from the eigenvalues
by inversely transforming Eq. \eqref{eq:wband_eigenvalue},
%
\begin{equation}
  \mu_i
  =
  \frac { 1 } { g \, n }
  \sum_{ k = 0 }^{ g \, n - 1 }
  \lambda_k
  \cos \left(
       \frac{ k \, i \, 2 \, \pi }
            {      g \, n        }
  \right)
  .
\label{eq:mu_from_lambda}
\end{equation}
%
In the non-periodic case,
we have defined
$\lambda_k \equiv \lambda_{2 \, n - k}$
for $k = n + 1, \dots, 2 \, n - 1$,
as well as
%
\begin{align}
  \lambda_n
  =
  (-1)^{ n - 1 }
  \lambda_0
  +
  2 \, \sum_{ k = 1 }^{ n - 1 }
      (-1)^{n - k - 1} \, \lambda_k
  ,
\label{eq:lambdan}
\end{align}
to satisfy the constraint, $\mu_n = 0$.
%





\subsection{Gaussian updating scheme}



We define a Gaussian updating scheme
as an updating scheme with a
Gaussian-shaped window function.
%
This updating scheme is commonly
used in metadynamics.
%
Below we show its convergence
in the continuous limit,
and how to maintain the convergence
on a finite grid.



We recall that
for continuous variable, $z$, over $[0, \Delta z]$
the bin index, $i$, is related to $z$ as
$z = i \, \delta z$,
where
$\delta z = \Delta z/(g \, n)$ is th bin size.

and
$\mu_i = \mu(z) \, \delta z$.
%
For $n \gg 1$,
By assuming the largest bin cutoff, $b \approx g \, n/2$,
we can approximate Eq. \eqref{eq:wband_eigenvalue}
as an integral:
%
\begin{equation}
  \lambda_k
  =
  \int_{-\Delta z/2}^{\Delta z/2}
    \mu(z) \, \cos\left( \frac{ 2 \, \pi \, k \, z } { \Delta z} \right)
    \, d z,
  \notag
  %\label{eq:lambda_int}
\end{equation}
%
with the normalization
$1 = \int_{-\Delta z/2}^{\Delta z/2} \mu(z) \, dz$.

If the width of the Gaussian window, $\sigma_z$,
is very small compared to $\Delta z$,
then we can extend the limits of the integrals
to infinity, and
%
\begin{equation}
  \mu(z)
  \approx
  \frac{            1            }
       { \sqrt{ 2 \pi } \sigma_z }
  %
  \exp\left(
        -\frac{   z^2   }
              { 2 \, \sigma_z^2 }
      \right)
  .
\notag
\end{equation}
%
Then, from Eq. \eqref{eq:wband_eigenvalue},
we get Eq. \eqref{eq:lambda_Gaussian},
which shows that all eigenvalues are positive.
%
In practice, however, some eigenvalues can turn negative
when discretized on a finite grid.
%
We can maintain the convergence of the updating scheme
by redefining the Gaussian scheme
from the same spectrum of eigenvalues, Eq. \eqref{eq:lambda_Gaussian},
and then inversely computing the window function
using Eq. \eqref{eq:mu_from_lambda},
or the updating matrix using Eq. \eqref{eq:w_from_phi}.
%
An example is shown in Fig. \ref{fig:mat}(a).

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/mat.pdf}
  }
  \caption{
    \label{fig:mat}
    Updating matrices, $w_{ij}$, for
    homogeneous updating schemes of
    a non-periodic variable ($n = 500$).
    Here $j$ is the bin of the current collective variable,
    and $i$ is the bin where the bias potential is updated.
    %
    (a) The Gaussian updating scheme of width $\sigma = 20$ bins.
    (b) The bandpass updating scheme of $K = 20$.
    %
    Both updating schemes
    can be characterized by a rigid window
    centered on the current bin, $j$,
    with some boundary adjustments.
  }
\end{figure}




\subsection{\label{sec:homo_bandpass}
Homogeneous bandpass updating schemes}



In a homogeneous bandpass updating scheme,
the eigenvalues are either $0$ or $1$ and
it is convenient to set $\lambda_0 = 1$.
%
We can use Eq. \eqref{eq:mu_from_lambda}
to find the window function.
%
For a periodic variable,
we will take into account the two-fold degeneracy
and modify Eq. \eqref{eq:lambda_bandpass}
by setting $\lambda_{n-K-1}, \dots, \lambda_{n-1}$
also to $1$.
%
Then, $\mu_l = \mathrm{sinr}(K, l, n)$ and $w_{ij} = \mu_{i-j}$,
where
$\mathrm{sinr}(K, l, n)
  =
  \sin
  \frac{ (2 K - 1) \, l \, \pi }
       {              n        }
  / \left( n \, \sin \frac{ l \, \pi } { n } \right)$.
%\begin{equation}
%  \mathrm{sinr}(K, l, n)
%  =
%  \frac{
%    \sin
%    \frac{ (2 K - 1) \, l \, \pi }
%         {              n        }
%  }
%  {
%    n \, \sin \frac{ l \, \pi } { n }
%  }
%  .
%\notag
%%\label{eq:mu_sinc_pbc}
%\end{equation}
%

For a non-periodic variable,
we have $\lambda_n = (-1)^{n-K}$
from Eqs. \eqref{eq:lambda_bandpass} and \eqref{eq:lambdan},
and $\mu_l = (-1)^{n-K+l}/(2 \, n) + \mathrm{sinr}(K, l, 2 \, n)$.
%\begin{equation}
%  \mu_l
%  =
%  (-1)^{n-K+l} / ( 2 \, n )
%    +
%  \mathrm{sinr}(K, l, 2n)
%  .
%  \notag
%  %\label{eq:mu_sinc_refl}
%\end{equation}
%
The updating matrix is given by
$w_{ij} = \mathrm{sinr}(K, i-j, 2 \, n) + \mathrm{sinr}(K, i+j-1, 2 \, n)$.\footnote{$w_{ij}$
  is free from the oscillatory term, $(-1)^{n-K+1}$,
  that appeared in the window function, since
  its contribution to $\mu_{i-j}$ in Eq. \eqref{eq:w_band_refl}
  is canceled by its contribution to either $\mu_{i+j-1}$
  or $\mu_{i+j-2n-1}$,
  whichever exists, of the opposite sign.}
%
The homogeneous bandpass updating scheme
resembles the Gaussian updating scheme
except for some long-range wiggles,
as shown in Fig. \ref{fig:mat}(b).
%



\section{\label{sec:hfluc}
Histogram fluctuation}


We will first review the relation
between the histogram fluctuation and the error
in a long FDS simulation under the single-bin updating scheme.
%
We then give a measure of the overall histogram fluctuation
that can be used in the WL-like criterion of stage transitions
for the bandpass updating scheme based on orthogonal polynomials.
%as a generalization of the WL algorithm for the single-bin scheme.

\subsection{Histogram fluctuation and error}

Let us consider an FDS simulation under a sufficiently accurate bias potential.
The fluctuation of the instantaneous histogram
is then dominated by the random part,
%
\begin{equation}
  f_i(t) \approx \zeta_i(t)
  ,
  \label{eq:xi_zeta}
\end{equation}
and the same holds for the Fourier transform,
$\tilde f_k(t) \approx \tilde \zeta_k(t).$
%
If we define the time average,
%
\begin{align*}
\tilde F_k(T) \equiv \frac 1 T \sum_{t = 1}^T \tilde f_k(t)
=\mathcal F\left[ \frac{ H_i(T) } { \rho_i } \right]_k
,
\end{align*}
%
for the histogram fluctuation of mode $k$,
then the total fluctuation is
%
\begin{align}
  F^2(T)
  &\equiv
  \sum_{k=1}^{n-1} \tilde F_k^2(T)
  =
  \sum_{i=1}^{n} \rho_i \, F_{*i}^2(T)
  =
  \sum_{i=1}^n
  \frac{ [ H_i(T) -\rho_i]^2 }{\rho_i}
  .
  \notag
  %\label{eq:hfluc_def}
\end{align}
%
From Eq. \eqref{eq:xi_zeta}, we have
%$$\bigl\langle \tilde F_k^2(T) \bigr\rangle
%=
%\sum_{t=-\infty}^\infty
%\frac{
%\bigl\langle
%  \tilde f_k(t) \, \tilde f_k(0)
%\bigr\rangle } { T }
%\approx
%\sum_{t=-\infty}^\infty
%\frac{
%\bigl\langle
%  \tilde \zeta_k(t) \, \tilde \zeta_k(0)
%\bigr\rangle } { T }
%=
%\frac{ \Gamma_k } { T },$$
%
\begin{align*}
\bigl\langle \tilde F_k^2(T) \bigr\rangle
&=
\frac{1}{T}
\sum_{t=-\infty}^\infty
\bigl\langle
  \tilde f_k(t) \, \tilde f_k(0)
\bigr\rangle
\\
&\approx
\frac{1}{T}
\sum_{t=-\infty}^\infty
\bigl\langle
  \tilde \zeta_k(t) \, \tilde \zeta_k(0)
\bigr\rangle
=
\frac{ \Gamma_k } { T }
,
\end{align*}
%
and the total histogram fluctuation is
\begin{equation}
  \bigl\langle F^2(T) \bigr\rangle
  =
  \sum_{k=1}^{n-1}
  \bigl\langle \tilde F_k^2(T) \bigr\rangle
  =
  \sum_{k=1}^{n-1}
  \frac{ \Gamma_k } { T }
  =
  \frac{ \Gamma } { T }
  ,
  \label{eq:F2sum}
\end{equation}
where we have used Eq. \eqref{eq:Gammak_sum}
in the last step.
%
We may relate the histogram fluctuation
to the minimal error under the single-bin updating scheme,
Eq. \eqref{eq:Emin_sbin}, as
%we may rephrase the optimality of the inverse-time schedule
%as the inequality,
\begin{equation}
  \Err(T)
  \ge
  \bigl\langle F^2(T+t_0) \bigr\rangle
  .
  \notag
  %\label{eq:fluc}
\end{equation}
%and the histogram fluctuation
%serves as a good estimate of the final error.
%

\subsection{Overall histogram fluctuation}

Equation \eqref{eq:F2sum} provides a way of
defining the overall histogram fluctuation,
hence the histogram flatness,
which can be used in the WL algorithm
as a criterion for transition to new stage with reduced updating magnitude.
%
This definition, however,
diverges in the limit of $n \to \infty$,
when the bin size becomes infinitely small
and collective variable becomes continuous.
%
Thus, we find it more suitable to define the histogram fluctuation
from the mode with the largest $|\tilde F_k(T)|$.
%
The two definitions are often congruent with each other
for a local sampling process with a finite $n$
because the major contribution to $\langle F^2(T) \rangle$
often comes from the few longest-range modes [cf. Eq. \eqref{eq:Gamma_local}].
%
For the bandpass updating scheme,
we need to further restrict the mode index by $k < K$
since the histogram flattening applies
only to the first $K$ modes.
%
Thus, the new definition of overall histogram fluctuation
is given by
\begin{align}
  \max_{1 \le k < K} \left| \, \tilde F_k(T) \, \right|
  .
  \label{eq:maxFk}
\end{align}

If the eigenmodes are made of a sequence of orthogonal polynomials
as in Sec. \ref{sec:bandpass_poly},
we have
\begin{align*}
\tilde f_k(t)
=
\int \phi_k(z) \, \frac{ \delta\bigl(z(t) - z\bigr) } { \rho(z) } \, dz
=
R_k\bigl( z(t) \bigr)
,
\end{align*}
so that
\begin{equation}
  \tilde F_k(T)
  =
  \frac 1 T \sum_{t = 1}^T R_k\bigl( z(t) \bigr)
  =
  \overline{
    R_k\bigl( z(t) \bigr)
  }
  .
  \label{eq:Fk_def}
\end{equation}




\note{The table of symbols is listed in Table \ref{tab:symbols}.
  \begin{table*}
  \footnotesize
  \centering
  \rowcolors{1}{white}{LightGray}
  \setlength{\tabcolsep}{4pt} % column separation
  \caption{\label{tab:symbols}
    Table of symbols.}
  \begin{tabular}{l | p{12cm} }
    Symbol          &   Description \\
    \hline
    $\mathbf{A}$    &   Transition matrix. \\
    $\alpha$        &   Updating magnitude. \\
    $b$             &   Cutoff of the window function of homogeneous updating schemes. \\
    $C_M$           &   Integral of the mass function, $M(\bar q)$.  \\
    $\delta$        &   Dirac's or discrete $\delta$-function. \\
    $c$             &   Constant. \\
    $\Err, \Err_R, \Err_A$          &   Error, residual error, asymptotic error. \\
    $\ln f$         &   Updating factor in the WL algorithm.  \\
    $\phi_{ki}$     &   Eigenvectors of the updating matrix. \\
    $g$             &   $1$ for a periodic variable, or for $2$ a non-periodic one. \\
    $G_i$           &   Integral of the autocorrelation function of histogram fluctuation at bin $i$. \\
    $\Gamma_k$      &   Integral of the autocorrelation function of mode $k$. \\
    $h_i(t)$        &   Instantaneous histogram.  \\
    $H_i$           &   Average normalized histogram.  \\
    $i, j$          &   Bin indices. \\
    $\lambda_k$     &   The $k$th eigenvalue of the updating matrix. \\
    $k, l$          &   Mode indices. \\
    $K$             &   Cutoff wave number of bandpass updating schemes.  \\
    $M(q)$          &   Mass function.   \\
    $m(q)$          &   Mass distribution,
                        $m(q) \equiv M(q)/\int_0^{ q(T) } M(q') \, dq'$.  \\
    $\mu_i$         &   Updating window function of a homogeneous updating scheme. \\
    $n$             &   Number of bins. \\
    $\nu_k$         &   $\nu_k \equiv \lambda_k / \lambda$. \\
    $\rho_i$        &   Flat sampling distribution. \\
    $\pi_i$         &   (Instantaneous) distribution. \\
    $q(t)$          &   $\int_0^t \alpha(t') \, dt'$.  \\
    $\bar q(t)$     &   $q(T) - q(t)$.  \\
    $s, S$          &   Stage index and the total number of stages in the Wang-Landau algorithm. \\
    $\sigma_{ij}(t)$   &   Autocorrelation function of $\zeta_i(t)$. \\
    $t, \tau$       &   Time. \\
    $T$             &   Simulation length. \\
    $\vartheta_k(q')$       &   $\vartheta_k(q') \equiv e^{\lambda_k \, [q - q(T)]}$. \\
    $u_i(t)$        &   Original bias potential. \\
    $v_i(t)$        &   Shifted bias potential. \\
    $\mathbf w$     &   Updating matrix. \\
    $v_{*i}(t)$     &   Bin-average-deducted bias potential, Eq. \eqref{eq:x_def}. \\
    $\zeta_{*i}(t)$ &   The noise of histogram. \\
    ${\tilde v}_k$  &   Mode of the shifted bias potential. \\
    ${\tilde u}_k$  &   Mode of the original bias potential. \\
    $z$             &   Reaction coordinate. \\
    $\zeta_i(t)$    &   The noise part of the instantaneous histogram, $h_i(t)$.
  \end{tabular}
  \end{table*}
}

\bibliography{simul}
\end{document}
